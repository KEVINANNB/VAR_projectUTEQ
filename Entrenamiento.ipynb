{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KEVINANNB/VAR_projectUTEQ/blob/main/Entrenamiento.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64d3933d",
        "outputId": "6eb909ce-844b-4dbc-8f70-045581af6765"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DEFINIR PORCENTAJES DE ENTRENAMIENTO ENTRENAR 80%, VALIDAR 10% Y PRUEVA O TEST 10%**"
      ],
      "metadata": {
        "id": "oOA-kJPu1Ch7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bdef51d"
      },
      "source": [
        "import os\n",
        "import random\n",
        "from shutil import copyfile\n",
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the folder in your Google Drive you want to check\n",
        "# !!! IMPORTANT: Replace this with the actual path to your folder !!!\n",
        "folder_path = '/content/drive/My Drive/YoloV8/grupos/21793-24191' # <--- CHANGE THIS TO YOUR ACTUAL FOLDER PATH\n",
        "\n",
        "# Check if the folder exists\n",
        "if not os.path.exists(folder_path):\n",
        "    print(f\"Error: The folder '{folder_path}' was not found.\")\n",
        "    print(\"Please verify the path is correct.\")\n",
        "elif not os.path.isdir(folder_path):\n",
        "    print(f\"Error: The path '{folder_path}' is not a directory.\")\n",
        "else:\n",
        "    # List all items in the folder\n",
        "    items_in_folder = os.listdir(folder_path)\n",
        "\n",
        "    # Count the number of files (excluding subdirectories if any)\n",
        "    # You might want to refine this if you only want to count specific file types (e.g., .jpg, .txt)\n",
        "    num_files = 0\n",
        "    for item_name in items_in_folder:\n",
        "        item_path = os.path.join(folder_path, item_name)\n",
        "        if os.path.isfile(item_path):\n",
        "            num_files += 1\n",
        "\n",
        "    print(f\"Number of items (files and folders) in '{folder_path}': {len(items_in_folder)}\")\n",
        "    print(f\"Number of files in '{folder_path}': {num_files}\")\n",
        "\n",
        "    # Optional: You can also filter and count specific file types, e.g., .jpg images\n",
        "    # jpg_files = [f for f in items_in_folder if f.lower().endswith(\".jpg\")]\n",
        "    # print(f\"Number of .jpg files in '{folder_path}': {len(jpg_files)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pONLcOb9Ixqc",
        "outputId": "7b94fa42-203c-4bfd-be00-68765d98a841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of items (files and folders) in '/content/drive/My Drive/YoloV8/grupos/21793-24191': 3817\n",
            "Number of files in '/content/drive/My Drive/YoloV8/grupos/21793-24191': 3817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce483baa",
        "outputId": "664c6d50-2938-4d65-9074-c12c93096fd4"
      },
      "source": [
        "import os\n",
        "import random\n",
        "from shutil import copyfile\n",
        "import shutil\n",
        "\n",
        "def img_train_test_split(img_source_dir, train_size, validation_size, train_dir_path, validation_dir_path, test_dir_path):\n",
        "    \"\"\"\n",
        "    Parametros\n",
        "    ----------\n",
        "    img_source_dir : string\n",
        "    Directorio de las imagenes y anotaciones en formato YOLO (.txt)\n",
        "\n",
        "    train_size : float\n",
        "    Porcentaje de la muestra de entrenamiento, ejemplo 0.80 (80%)\n",
        "\n",
        "    validation_size : float\n",
        "    Porcentaje de la muestra de validación, ejemplo 0.15 (15%)\n",
        "\n",
        "    train_dir_path : string\n",
        "    Directorio de destino para las imágenes y anotaciones de entrenamiento.\n",
        "\n",
        "    validation_dir_path : string\n",
        "    Directorio de destino para las imágenes y anotaciones de validación.\n",
        "\n",
        "    test_dir_path : string\n",
        "    Directorio de destino para las imágenes y anotaciones de prueba.\n",
        "\n",
        "    El restante porcentaje quedaria para el numero de imagenes y anotaciones de prueba.\n",
        "    La función asumirá que las carpetas de destino ya existen o las creará si no.\n",
        "    \"\"\"\n",
        "\n",
        "    # Asegurarse de que los directorios de destino existan, crearlos si no\n",
        "    for dir_path in [train_dir_path, validation_dir_path, test_dir_path]:\n",
        "        if not os.path.exists(dir_path):\n",
        "            print(f\"Creating destination directory: {dir_path}\")\n",
        "            os.makedirs(dir_path)\n",
        "        else:\n",
        "            print(f\"Destination directory already exists: {dir_path}\")\n",
        "\n",
        "\n",
        "    subdir_fullpath = img_source_dir\n",
        "\n",
        "    if not os.path.exists(subdir_fullpath) or not os.path.isdir(subdir_fullpath):\n",
        "        print(f\"Source directory not found or is not a directory: {subdir_fullpath}\")\n",
        "        return # Exit the function if the source directory is invalid\n",
        "\n",
        "    # --- Added for debugging ---\n",
        "    print(f\"Contents of source directory '{subdir_fullpath}':\")\n",
        "    source_dir_contents = os.listdir(subdir_fullpath)\n",
        "    # print(source_dir_contents) # Descomentar si quieres ver la lista completa\n",
        "    print(f\"Found {len(source_dir_contents)} items.\")\n",
        "    # -------------------------\n",
        "\n",
        "    if len(source_dir_contents) == 0:\n",
        "        print(subdir_fullpath + ' is empty')\n",
        "        return # Exit the function if the source directory is empty\n",
        "\n",
        "\n",
        "    train_counter = 0\n",
        "    validation_counter = 0\n",
        "    test_counter = 0\n",
        "\n",
        "    # Contar el número de imagenes totales y filtrar solo archivos jpg (case-insensitive)\n",
        "    image_files = [f for f in source_dir_contents if f.lower().endswith(\".jpg\")] # Changed to lower() for case-insensitivity\n",
        "    count_images = len(image_files)\n",
        "\n",
        "    if count_images == 0:\n",
        "        print(\"No .jpg images found in \" + subdir_fullpath)\n",
        "        return # Exit if no images are found\n",
        "\n",
        "    print(f\"Found {count_images} .jpg images in source directory.\")\n",
        "\n",
        "    total_images=count_images\n",
        "    count_images=0 # Reset counter for iteration\n",
        "\n",
        "    #Ordenar de manera aleatoria las imagenes\n",
        "    random.shuffle(image_files)\n",
        "\n",
        "    #Separacion de imagenes y anotaciones\n",
        "    for filename in image_files:\n",
        "        base_filename, ext = os.path.splitext(filename)\n",
        "        annotation_filename = base_filename + '.txt'\n",
        "        annotation_filepath = os.path.join(subdir_fullpath, annotation_filename)\n",
        "        source_image_filepath = os.path.join(subdir_fullpath, filename)\n",
        "\n",
        "        if count_images < int(total_images*train_size):\n",
        "            # Copy image\n",
        "            copyfile(source_image_filepath, os.path.join(train_dir_path, filename))\n",
        "            # Copy annotation if it exists\n",
        "            if os.path.exists(annotation_filepath):\n",
        "                copyfile(annotation_filepath, os.path.join(train_dir_path, annotation_filename))\n",
        "            train_counter += 1\n",
        "        elif count_images < int(total_images*train_size) + int(total_images*validation_size):\n",
        "            # Copy image\n",
        "            copyfile(source_image_filepath, os.path.join(validation_dir_path, filename))\n",
        "            # Copy annotation if it exists\n",
        "            if os.path.exists(annotation_filepath):\n",
        "                copyfile(annotation_filepath, os.path.join(validation_dir_path, annotation_filename))\n",
        "            validation_counter += 1\n",
        "        else:\n",
        "            # Copy image\n",
        "            copyfile(source_image_filepath, os.path.join(test_dir_path, filename))\n",
        "            # Copy annotation if it exists\n",
        "            if os.path.exists(annotation_filepath):\n",
        "                copyfile(annotation_filepath, os.path.join(test_dir_path, annotation_filename))\n",
        "            test_counter += 1\n",
        "        count_images += 1\n",
        "\n",
        "    print(\"Images and annotations copied to training folder: \" + str(train_counter))\n",
        "    print(\"Images and annotations copied to validation folder: \" + str(validation_counter))\n",
        "    print(\"Images and annotations copied to test folder: \" + str(test_counter))\n",
        "\n",
        "# Example usage with direct paths:\n",
        "\n",
        "# Directorio de origen de tus imágenes y anotaciones en Google Drive\n",
        "# ASEGÚRATE DE QUE ESTA RUTA SEA CORRECTA\n",
        "img_source_dir = '/content/drive/My Drive/YoloV8/grupos/21793-24191'\n",
        "\n",
        "# Directorios de destino para entrenamiento, validación y prueba\n",
        "# ASEGÚRATE DE QUE ESTAS RUTAS SEAN CORRECTAS Y APUNTEN A CARPETAS EXISTENTES O DONDE QUIERES CREARLAS\n",
        "train_destination_dir = '/content/drive/My Drive/YoloV8/dataset/train'\n",
        "validation_destination_dir = '/content/drive/My Drive/YoloV8/dataset/valid'\n",
        "test_destination_dir = '/content/drive/My Drive/YoloV8/dataset/test'\n",
        "\n",
        "\n",
        "# Porcentajes para la división\n",
        "train_size = 0.80\n",
        "validation_size = 0.10 # El restante 1 - train_size - validation_size irá a test\n",
        "\n",
        "# Ejecutar la función para dividir el dataset en las carpetas especificadas\n",
        "img_train_test_split(img_source_dir, train_size, validation_size, train_destination_dir, validation_destination_dir, test_destination_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Destination directory already exists: /content/drive/My Drive/YoloV8/dataset/train\n",
            "Destination directory already exists: /content/drive/My Drive/YoloV8/dataset/valid\n",
            "Destination directory already exists: /content/drive/My Drive/YoloV8/dataset/test\n",
            "Contents of source directory '/content/drive/My Drive/YoloV8/grupos/21793-24191':\n",
            "Found 3817 items.\n",
            "Found 1910 .jpg images in source directory.\n",
            "Images and annotations copied to training folder: 1528\n",
            "Images and annotations copied to validation folder: 191\n",
            "Images and annotations copied to test folder: 191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** VER CUANTO HAY EN TRAIN VALID Y TEST**"
      ],
      "metadata": {
        "id": "TBUpMCb009N0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6eb3516",
        "outputId": "f3b3fd07-1aee-4f81-d8d0-c808b797c5dc"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the base path where the dataset was created in Google Drive\n",
        "output_base_path = '/content/drive/My Drive/YoloV8'\n",
        "dataset_path = os.path.join(output_base_path, 'dataset')\n",
        "\n",
        "# Define the paths to the train, valid, and test subdirectories\n",
        "train_path = os.path.join(dataset_path, 'train')\n",
        "valid_path = os.path.join(dataset_path, 'valid')\n",
        "test_path = os.path.join(dataset_path, 'test')\n",
        "\n",
        "print(f\"Number of items in {train_path}:\")\n",
        "if os.path.exists(train_path):\n",
        "    # Count the number of items instead of listing them\n",
        "    num_train_items = len(os.listdir(train_path))\n",
        "    print(num_train_items)\n",
        "else:\n",
        "    print(\"Train directory not found.\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "print(f\"Number of items in {valid_path}:\")\n",
        "if os.path.exists(valid_path):\n",
        "    # Count the number of items instead of listing them\n",
        "    num_valid_items = len(os.listdir(valid_path))\n",
        "    print(num_valid_items)\n",
        "else:\n",
        "    print(\"Validation directory not found.\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "print(f\"Number of items in {test_path}:\")\n",
        "if os.path.exists(test_path):\n",
        "    # Count the number of items instead of listing them\n",
        "    num_test_items = len(os.listdir(test_path))\n",
        "    print(num_test_items)\n",
        "else:\n",
        "    print(\"Test directory not found.\")\n",
        "print(\"-\" * 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of items in /content/drive/My Drive/YoloV8/dataset/train:\n",
            "20207\n",
            "------------------------------\n",
            "Number of items in /content/drive/My Drive/YoloV8/dataset/valid:\n",
            "2521\n",
            "------------------------------\n",
            "Number of items in /content/drive/My Drive/YoloV8/dataset/test:\n",
            "2537\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CREAR YAML**"
      ],
      "metadata": {
        "id": "DvmxUShS04jT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43030f18",
        "outputId": "fbc5e507-026c-4e82-bca9-80de1c278bd3"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the base path where the dataset was created in Google Drive\n",
        "output_base_path = '/content/drive/My Drive/YoloV8'\n",
        "dataset_path = os.path.join(output_base_path, 'dataset')\n",
        "\n",
        "# Define the paths to the train and valid subdirectories relative to the dataset path\n",
        "train_dir_relative = 'train'\n",
        "valid_dir_relative = 'valid'\n",
        "\n",
        "# Define the number of classes\n",
        "# !!! IMPORTANT: Change this to the actual number of classes in your dataset !!!\n",
        "num_classes = 1 # Example: change to 2, 3, etc. based on your dataset\n",
        "\n",
        "# Define the names of your classes in a list, in the correct order\n",
        "# !!! IMPORTANT: Replace 'class1', 'class2', etc. with your actual class names !!!\n",
        "class_names = ['your_class_name'] # Example: ['person', 'car', 'bus']\n",
        "\n",
        "# Define the path for the dataset.yaml file\n",
        "yaml_file_path = os.path.join(output_base_path, 'dataset.yaml')\n",
        "\n",
        "# Create the content for the dataset.yaml file\n",
        "yaml_content = f\"\"\"\n",
        "path: {dataset_path}  # Dataset root directory\n",
        "train: {train_dir_relative}  # Train images (relative to 'path')\n",
        "val: {valid_dir_relative}  # Val images (relative to 'path')\n",
        "test: test # Test images (optional)\n",
        "\n",
        "nc: {num_classes}  # Number of classes\n",
        "names: {class_names}  # Class names\n",
        "\"\"\"\n",
        "\n",
        "# Write the content to the dataset.yaml file in Google Drive\n",
        "with open(yaml_file_path, 'w') as f:\n",
        "    f.write(yaml_content)\n",
        "\n",
        "print(f\"Created dataset.yaml file at: {yaml_file_path}\")\n",
        "print(\"Please edit this file in Google Drive to set the correct number of classes (nc) and class names (names).\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset.yaml file at: /content/drive/My Drive/YoloV8/dataset.yaml\n",
            "Please edit this file in Google Drive to set the correct number of classes (nc) and class names (names).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOLO V8 ENTRENAR 1° VEZ**"
      ],
      "metadata": {
        "id": "9jJ2ZR5O0zFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar la librería ultralytics que contiene YOLOv8\n",
        "%pip install ultralytics -q"
      ],
      "metadata": {
        "id": "3R5UWr0-0IAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db126350",
        "outputId": "0e5c3b70-cc11-4bb0-d5d9-a8731725eacd"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the path to your dataset.yaml file in Google Drive\n",
        "# ASEGÚRATE DE QUE ESTA RUTA SEA CORRECTA\n",
        "yaml_path = '/content/drive/My Drive/YoloV8/dataset.yaml' # <-- ASEGÚRATE DE QUE ESTA RUTA SEA CORRECTA\n",
        "\n",
        "# Define the base YOLOv8 model you want to use\n",
        "# Puedes cambiar 'yolov8s.pt' por 'yolov8m.pt', 'yolov8l.pt', 'yolov8x.pt', etc.\n",
        "model_base = 'yolov8s.pt'\n",
        "\n",
        "# Define training parameters\n",
        "epochs = 150 # Puedes ajustar el número de épocas\n",
        "img_size = 640 # Puedes ajustar el tamaño de la imagen\n",
        "batch_size = 32 # <--- Define el tamaño del batch aquí (ej: 16, 32, 64)\n",
        "patience_value = 0 # <--- Define el valor de paciencia (0 para desactivar Early Stopping)\n",
        "\n",
        "# Define the output directory in Google Drive\n",
        "# Esto creará '/content/drive/My Drive/YoloV8/testing/data' para los resultados del entrenamiento\n",
        "output_project_dir = '/content/drive/My Drive/YoloV8/testing'\n",
        "run_name = 'data' # Nombre de la subcarpeta para esta ejecución de entrenamiento\n",
        "\n",
        "# Construct the training command\n",
        "# Encerrar la ruta con espacios en comillas dobles\n",
        "train_command = f\"yolo train data=\\\"{yaml_path}\\\" model={model_base} epochs={epochs} imgsz={img_size} project=\\\"{output_project_dir}\\\" name=\\\"{run_name}\\\" batch={batch_size} patience={patience_value}\" # Añadimos project, name, batch y patience\n",
        "\n",
        "print(f\"Generated training command: {train_command}\")\n",
        "\n",
        "# Para ejecutar este comando, puedes copiarlo y pegarlo en una celda de terminal de Colab\n",
        "# O puedes usar el magic command %run o ! para ejecutarlo directamente\n",
        "!{train_command}\n",
        "# %run -m ultralytics.yolo.engine.model train {train_command.split('train ')[1]} # Otra forma de ejecutarlo\n",
        "\n",
        "print(\"\\nCopy the command above and run it in a new cell (either code or terminal) to start training.\")\n",
        "print(\"Alternatively, uncomment the line `!{train_command}` above and run this cell.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated training command: yolo train data=\"/content/drive/My Drive/YoloV8/dataset.yaml\" model=yolov8s.pt epochs=150 imgsz=640 project=\"/content/drive/My Drive/YoloV8/testing\" name=\"data\" batch=32 patience=0\n",
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt': 100% ━━━━━━━━━━━━ 21.5MB 172.3MB/s 0.1s\n",
            "Ultralytics 8.3.199 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/My Drive/YoloV8/dataset.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=150, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=data, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=0, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/My Drive/YoloV8/testing, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/My Drive/YoloV8/testing/data, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% ━━━━━━━━━━━━ 755.1KB 13.2MB/s 0.1s\n",
            "Overriding model.yaml nc=80 with nc=3\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
            " 22        [15, 18, 21]  1   2117209  ultralytics.nn.modules.head.Detect           [3, [128, 256, 512]]          \n",
            "Model summary: 129 layers, 11,136,761 parameters, 11,136,745 gradients, 28.7 GFLOPs\n",
            "\n",
            "Transferred 349/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% ━━━━━━━━━━━━ 5.4MB 65.1MB/s 0.1s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.4±0.1 ms, read: 0.4±0.1 MB/s, size: 112.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/My Drive/YoloV8/dataset/train.cache... 10097 images, 13 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 10110/10110 134.2Mit/s 0.0s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.4±0.1 ms, read: 0.5±0.1 MB/s, size: 120.1 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/My Drive/YoloV8/dataset/valid.cache... 1260 images, 1 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1261/1261 10.7Mit/s 0.0s\n",
            "Plotting labels to /content/drive/My Drive/YoloV8/testing/data/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/My Drive/YoloV8/testing/data\u001b[0m\n",
            "Starting training for 150 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      1/150      6.94G      2.241      3.004      1.232         61        640: 100% ━━━━━━━━━━━━ 316/316 0.3it/s 17:03\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 3.8it/s 5.3s\n",
            "                   all       1261       1527      0.603      0.619        0.6      0.253\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      2/150      7.37G       2.01      1.206      1.117         77        640: 100% ━━━━━━━━━━━━ 316/316 6.7it/s 47.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.794      0.658       0.75      0.311\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      3/150      7.39G      2.058      1.207       1.13         70        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.625      0.718      0.667      0.269\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      4/150      7.39G      2.069      1.207      1.133         59        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.688      0.615        0.7      0.282\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      5/150      7.39G      1.995      1.139      1.112         69        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527       0.65      0.743      0.748      0.328\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      6/150      7.39G      1.936      1.082      1.078         79        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.762      0.717      0.771      0.354\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      7/150      7.39G       1.88      1.032      1.062         58        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.6it/s 4.4s\n",
            "                   all       1261       1527      0.761      0.751      0.789      0.355\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      8/150      7.39G      1.856      1.014       1.05         68        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.6it/s 4.4s\n",
            "                   all       1261       1527      0.767        0.7      0.788       0.36\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      9/150      7.39G      1.814     0.9806      1.036         56        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.1s\n",
            "                   all       1261       1527      0.736      0.797      0.826      0.412\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     10/150      7.39G      1.792     0.9641      1.028         75        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.797      0.761      0.856      0.427\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     11/150      7.39G      1.768     0.9534       1.02         75        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.811       0.74      0.833      0.429\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     12/150      7.39G      1.763     0.9385      1.017         69        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.1it/s 3.9s\n",
            "                   all       1261       1527      0.861      0.756      0.863       0.43\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     13/150      7.39G      1.755      0.936      1.016         65        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.0s\n",
            "                   all       1261       1527       0.75      0.749      0.822      0.416\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     14/150      7.39G      1.724     0.9077      1.005         61        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.876      0.783      0.891      0.479\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     15/150      7.39G      1.722     0.9105      1.007         69        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.746      0.741        0.8      0.407\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     16/150      7.39G      1.704     0.9087      1.007         63        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.6it/s 4.4s\n",
            "                   all       1261       1527      0.724      0.824      0.839      0.429\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     17/150      7.39G      1.701     0.8922      1.002         60        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.7it/s 4.2s\n",
            "                   all       1261       1527      0.845       0.88       0.89       0.46\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     18/150      7.39G      1.689     0.8878     0.9945         74        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.1s\n",
            "                   all       1261       1527      0.845        0.8      0.874      0.463\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     19/150      7.39G      1.673     0.8888      0.987         58        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.1s\n",
            "                   all       1261       1527      0.888      0.856      0.905      0.483\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     20/150      7.42G      1.673     0.8876     0.9928         46        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.848      0.891      0.921      0.489\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     21/150      7.42G      1.661     0.8706     0.9877         69        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.1it/s 3.9s\n",
            "                   all       1261       1527      0.793      0.775      0.856      0.437\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     22/150      7.42G      1.662     0.8665      0.986         61        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527       0.83      0.826      0.886      0.486\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     23/150      7.42G      1.654     0.8678     0.9879         60        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.1it/s 3.9s\n",
            "                   all       1261       1527      0.841      0.852      0.891      0.492\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     24/150      7.42G      1.639     0.8619     0.9811         61        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.906      0.804      0.909      0.499\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     25/150      7.42G      1.646       0.85     0.9812         67        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.884      0.779      0.879      0.496\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     26/150      7.42G      1.625     0.8465     0.9758         62        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.0s\n",
            "                   all       1261       1527      0.847       0.84      0.901      0.501\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     27/150      7.42G      1.628      0.844     0.9789         50        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.888      0.878      0.922      0.512\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     28/150      7.42G      1.623     0.8449     0.9742         64        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527       0.95      0.831      0.919       0.51\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     29/150      7.42G      1.622     0.8324     0.9725         61        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.2s\n",
            "                   all       1261       1527      0.871      0.899       0.92      0.535\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     30/150      7.42G       1.63       0.84     0.9727         62        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.944      0.872      0.955      0.528\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     31/150      7.42G       1.61     0.8293     0.9719         49        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.934      0.758      0.906      0.499\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     32/150      7.42G      1.602     0.8305     0.9685         64        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.2s\n",
            "                   all       1261       1527      0.941      0.839      0.931      0.522\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     33/150      7.42G      1.592     0.8267     0.9656         65        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.889      0.887      0.941      0.536\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     34/150      7.42G      1.598     0.8216     0.9684         51        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.917        0.9      0.944      0.543\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     35/150      7.42G      1.595     0.8205     0.9634         64        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.889      0.873      0.932      0.526\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     36/150      7.42G      1.588     0.8136     0.9621         63        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.1s\n",
            "                   all       1261       1527      0.869      0.885       0.94      0.534\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     37/150      7.42G       1.58     0.8047      0.963         64        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.1s\n",
            "                   all       1261       1527      0.897      0.915      0.948      0.557\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     38/150      7.42G      1.582     0.8108     0.9657         70        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.924      0.884       0.94      0.541\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     39/150      7.42G      1.596     0.8192     0.9623         62        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.7it/s 4.2s\n",
            "                   all       1261       1527       0.95      0.894      0.956      0.552\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     40/150      7.42G       1.58     0.8093     0.9585         52        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.909      0.915      0.952      0.556\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     41/150      7.42G      1.578     0.8117     0.9625         50        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.7it/s 4.2s\n",
            "                   all       1261       1527      0.943      0.903      0.962       0.56\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     42/150      7.42G      1.577     0.7992     0.9596         60        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.943      0.899      0.957       0.56\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     43/150      7.42G      1.563     0.7963      0.957         71        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.1it/s 4.0s\n",
            "                   all       1261       1527      0.933      0.896      0.951      0.562\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     44/150      7.42G      1.561     0.7943     0.9557         74        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527       0.92      0.921      0.953       0.56\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     45/150      7.42G      1.557     0.7821     0.9561         81        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.7it/s 4.2s\n",
            "                   all       1261       1527      0.957      0.915      0.963      0.569\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     46/150      7.42G      1.565     0.7927     0.9574         52        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.965      0.918      0.969       0.58\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     47/150      7.42G      1.561     0.7935     0.9553         66        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.965      0.901      0.964      0.575\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     48/150      7.42G       1.54     0.7779     0.9573         66        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.7it/s 4.3s\n",
            "                   all       1261       1527      0.965      0.925      0.971      0.575\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     49/150      7.42G      1.552     0.7834     0.9496         55        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.1s\n",
            "                   all       1261       1527      0.954      0.924      0.965       0.57\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     50/150      7.42G      1.539      0.778     0.9556         52        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.935      0.916      0.962      0.574\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     51/150      7.42G      1.549     0.7769      0.954         70        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.953      0.918      0.964       0.58\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     52/150      7.42G       1.54     0.7756     0.9501         68        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.1it/s 3.9s\n",
            "                   all       1261       1527      0.955       0.93      0.968       0.58\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     53/150      7.42G      1.532     0.7639     0.9512         58        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.961       0.91      0.966      0.583\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     54/150      7.42G      1.527     0.7658     0.9497         70        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.969      0.917       0.97      0.583\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     55/150      7.42G      1.534     0.7625     0.9492         76        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527       0.97      0.921      0.968       0.58\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     56/150      7.42G      1.535      0.766     0.9492         86        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.936      0.941      0.964      0.583\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     57/150      7.42G      1.521     0.7658      0.944         57        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.7it/s 4.3s\n",
            "                   all       1261       1527      0.945      0.935      0.963      0.584\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     58/150      7.42G      1.521     0.7589     0.9435         60        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.2s\n",
            "                   all       1261       1527      0.935      0.933      0.965      0.583\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     59/150      7.42G      1.511     0.7509     0.9452         75        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.2s\n",
            "                   all       1261       1527       0.94      0.921      0.966      0.595\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     60/150      7.42G      1.527     0.7561     0.9499         71        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.951      0.919      0.968       0.59\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     61/150      7.42G       1.52     0.7528     0.9464         55        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.944      0.935      0.972       0.59\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     62/150      7.42G      1.512     0.7448     0.9416         53        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.1it/s 3.9s\n",
            "                   all       1261       1527       0.94      0.937       0.97      0.593\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     63/150      7.42G      1.512     0.7497     0.9397         47        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.963      0.935      0.972      0.594\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     64/150      7.42G      1.503     0.7385     0.9407         80        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.947      0.943      0.971      0.594\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     65/150      7.42G      1.507     0.7434     0.9435         57        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.7it/s 4.2s\n",
            "                   all       1261       1527      0.961      0.945      0.972      0.595\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     66/150      7.42G      1.498     0.7353     0.9344         58        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.2s\n",
            "                   all       1261       1527      0.965      0.945      0.973      0.599\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     67/150      7.42G      1.502     0.7449     0.9397         73        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.949      0.934      0.972      0.594\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     68/150      7.42G      1.496     0.7352     0.9395         65        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.2s\n",
            "                   all       1261       1527      0.966      0.928      0.971      0.593\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     69/150      7.42G      1.483     0.7411     0.9394         55        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.1s\n",
            "                   all       1261       1527      0.964      0.924      0.971      0.595\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     70/150      7.42G      1.497     0.7356     0.9373         77        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.962      0.924      0.969      0.593\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     71/150      7.42G      1.483     0.7272     0.9375         61        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.955       0.95      0.972      0.592\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     72/150      7.42G      1.496       0.73     0.9371         58        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.958      0.934      0.969      0.594\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     73/150      7.42G      1.497     0.7343     0.9405         84        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.1it/s 3.9s\n",
            "                   all       1261       1527      0.967      0.933      0.973      0.595\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     74/150      7.42G      1.488     0.7328     0.9376         75        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.2it/s 3.9s\n",
            "                   all       1261       1527      0.967      0.935      0.972      0.598\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     75/150      7.42G      1.481      0.731     0.9349         54        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.1it/s 4.0s\n",
            "                   all       1261       1527      0.961      0.925      0.969      0.593\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     76/150      7.42G      1.486     0.7274     0.9354         76        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.2s\n",
            "                   all       1261       1527      0.971      0.938      0.972      0.596\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     77/150      7.42G      1.476     0.7246     0.9335         69        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.1s\n",
            "                   all       1261       1527      0.971      0.937      0.972      0.593\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     78/150      7.42G      1.465     0.7156     0.9308         72        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.1s\n",
            "                   all       1261       1527      0.978      0.933      0.973      0.597\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     79/150      7.42G      1.472     0.7168     0.9313         60        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.1s\n",
            "                   all       1261       1527      0.977      0.933      0.973      0.597\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     80/150      7.42G      1.465     0.7149     0.9349         65        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.968      0.933      0.972      0.599\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     81/150      7.42G      1.468     0.7197     0.9325         73        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.969      0.934      0.973      0.599\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     82/150      7.42G      1.463     0.7171     0.9322         62        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.0s\n",
            "                   all       1261       1527      0.975      0.934      0.974        0.6\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     83/150      7.42G      1.457     0.7084     0.9289         74        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.1it/s 3.9s\n",
            "                   all       1261       1527      0.963      0.939      0.976      0.601\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     84/150      7.42G      1.459     0.7135     0.9255         55        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.968      0.944      0.976        0.6\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     85/150      7.42G      1.459     0.7057     0.9347         62        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.2s\n",
            "                   all       1261       1527      0.972      0.945      0.977      0.604\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     86/150      7.42G      1.447     0.7015     0.9292         59        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527       0.97      0.944      0.977      0.606\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     87/150      7.42G      1.452     0.6945     0.9298         68        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.1s\n",
            "                   all       1261       1527      0.968      0.946      0.975      0.604\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     88/150      7.42G      1.453     0.6982     0.9301         58        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.1s\n",
            "                   all       1261       1527      0.963      0.948      0.975      0.604\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     89/150      7.42G      1.437     0.6925     0.9273         53        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.962      0.953      0.975      0.604\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     90/150      7.42G      1.446     0.6961     0.9282         70        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.2it/s 3.9s\n",
            "                   all       1261       1527      0.966      0.951      0.975      0.603\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     91/150      7.42G      1.456     0.6967     0.9254         70        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.2it/s 3.9s\n",
            "                   all       1261       1527      0.965      0.949      0.975      0.601\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     92/150      7.42G      1.437     0.6966     0.9265         66        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.964      0.949      0.974      0.603\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     93/150      7.42G      1.434     0.6875     0.9256         56        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.964      0.952      0.978      0.605\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     94/150      7.42G      1.433     0.6901     0.9228         65        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.1it/s 4.0s\n",
            "                   all       1261       1527      0.965      0.951      0.974      0.606\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     95/150      7.42G      1.431      0.681     0.9235         65        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.2s\n",
            "                   all       1261       1527      0.965      0.949      0.974      0.606\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     96/150      7.42G      1.424     0.6832     0.9222         60        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.1s\n",
            "                   all       1261       1527      0.962      0.945      0.974      0.608\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     97/150      7.42G      1.428     0.6848     0.9239         70        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.961      0.946      0.974      0.607\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     98/150      7.42G      1.419      0.682     0.9204         70        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.1it/s 3.9s\n",
            "                   all       1261       1527      0.959       0.95      0.975      0.608\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K     99/150      7.42G      1.412     0.6764      0.918         54        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.1it/s 3.9s\n",
            "                   all       1261       1527      0.958      0.949      0.974      0.608\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    100/150      7.42G      1.409     0.6807     0.9199         56        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.962      0.949      0.974      0.608\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    101/150      7.42G      1.401     0.6718     0.9207         70        640: 100% ━━━━━━━━━━━━ 316/316 6.8it/s 46.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.963      0.944      0.974      0.608\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    102/150      7.42G      1.422     0.6748     0.9204         61        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.1it/s 4.0s\n",
            "                   all       1261       1527      0.965      0.947      0.974      0.608\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    103/150      7.42G      1.411      0.669     0.9256         60        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.1it/s 3.9s\n",
            "                   all       1261       1527      0.962      0.949      0.974      0.609\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    104/150      7.42G      1.409     0.6673     0.9158         71        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.964       0.95      0.974      0.611\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    105/150      7.42G      1.407       0.67     0.9172         65        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.2s\n",
            "                   all       1261       1527      0.964       0.95      0.974      0.609\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    106/150      7.42G      1.399      0.666     0.9183         55        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.2s\n",
            "                   all       1261       1527      0.962      0.949      0.974      0.612\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    107/150      7.42G      1.396     0.6626     0.9174         45        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.2s\n",
            "                   all       1261       1527      0.963      0.949      0.974      0.612\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    108/150      7.42G      1.411     0.6675     0.9177         62        640: 100% ━━━━━━━━━━━━ 316/316 6.8it/s 46.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.2s\n",
            "                   all       1261       1527      0.963      0.945      0.974      0.612\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    109/150      7.42G      1.402     0.6648      0.917         73        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.964      0.941      0.974      0.613\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    110/150      7.42G       1.39     0.6558     0.9172         69        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.2it/s 3.8s\n",
            "                   all       1261       1527      0.965      0.938      0.974      0.613\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    111/150      7.42G      1.387     0.6496     0.9127         56        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527       0.96      0.938      0.973      0.612\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    112/150      7.42G      1.389     0.6533     0.9132         83        640: 100% ━━━━━━━━━━━━ 316/316 6.8it/s 46.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.1it/s 3.9s\n",
            "                   all       1261       1527      0.958      0.938      0.973      0.612\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    113/150      7.42G      1.391     0.6488     0.9144         72        640: 100% ━━━━━━━━━━━━ 316/316 6.8it/s 46.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.959      0.941      0.973      0.613\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    114/150      7.42G      1.382     0.6454     0.9154         72        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.959      0.943      0.974      0.615\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    115/150      7.42G      1.373     0.6533     0.9134         62        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.2s\n",
            "                   all       1261       1527       0.96      0.945      0.974      0.615\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    116/150      7.42G       1.37     0.6448      0.909         70        640: 100% ━━━━━━━━━━━━ 316/316 6.8it/s 46.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.953      0.945      0.973      0.614\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    117/150      7.42G      1.374     0.6405     0.9109         63        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.0s\n",
            "                   all       1261       1527      0.953      0.942      0.973      0.614\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    118/150      7.42G      1.362     0.6413     0.9084         63        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.953      0.943      0.973      0.614\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    119/150      7.42G      1.359     0.6385     0.9064         66        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.962      0.936      0.973      0.613\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    120/150      7.42G      1.361     0.6399     0.9096         50        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.0s\n",
            "                   all       1261       1527      0.962      0.936      0.973      0.615\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    121/150      7.42G      1.354     0.6333     0.9077         56        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527       0.96      0.938      0.973      0.615\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    122/150      7.42G      1.353     0.6356     0.9081         67        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.1it/s 3.9s\n",
            "                   all       1261       1527      0.961      0.938      0.973      0.613\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    123/150      7.42G      1.358     0.6346     0.9086         73        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.961      0.938      0.973      0.614\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    124/150      7.42G       1.36     0.6286      0.912         52        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.7it/s 4.2s\n",
            "                   all       1261       1527       0.96      0.938      0.973      0.615\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    125/150      7.42G       1.35     0.6271     0.9061         73        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.957      0.939      0.973      0.615\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    126/150      7.42G       1.34     0.6232     0.9033         88        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.1s\n",
            "                   all       1261       1527      0.955      0.938      0.973      0.614\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    127/150      7.42G      1.347     0.6271     0.9039         64        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.2s\n",
            "                   all       1261       1527      0.954      0.938      0.973      0.614\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    128/150      7.42G       1.35       0.62     0.9063         53        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.6it/s 4.4s\n",
            "                   all       1261       1527      0.955      0.939      0.973      0.614\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    129/150      7.42G      1.332     0.6176     0.9026         72        640: 100% ━━━━━━━━━━━━ 316/316 6.8it/s 46.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.954      0.939      0.973      0.613\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    130/150      7.42G      1.338     0.6198     0.9032         59        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527       0.95      0.945      0.973      0.614\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    131/150      7.42G      1.332     0.6142     0.9041         58        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.0s\n",
            "                   all       1261       1527      0.949      0.945      0.973      0.613\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    132/150      7.42G      1.339     0.6132      0.906         48        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.2s\n",
            "                   all       1261       1527      0.948      0.948      0.973      0.614\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    133/150      7.42G      1.328     0.6102     0.9023         66        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.1it/s 3.9s\n",
            "                   all       1261       1527      0.948       0.95      0.973      0.613\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    134/150      7.42G      1.332     0.6113     0.9044         51        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.4it/s 4.6s\n",
            "                   all       1261       1527      0.951      0.949      0.974      0.613\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    135/150      7.42G      1.319     0.6106     0.9018         66        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.946      0.947      0.974      0.612\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    136/150      7.42G      1.318     0.6073     0.9015         66        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.7it/s 4.3s\n",
            "                   all       1261       1527      0.949      0.946      0.974      0.611\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    137/150      7.42G       1.31      0.595     0.9017         67        640: 100% ━━━━━━━━━━━━ 316/316 6.8it/s 46.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.2s\n",
            "                   all       1261       1527      0.951      0.946      0.974      0.612\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    138/150      7.42G      1.299     0.5965     0.8996         58        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.951      0.946      0.975      0.612\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    139/150      7.42G       1.31     0.5956     0.8968         52        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.951      0.946      0.974      0.613\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    140/150      7.42G      1.306     0.5942     0.8998         62        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 46.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.951      0.945      0.974      0.612\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    141/150      7.42G      1.317       0.58     0.8918         35        640: 100% ━━━━━━━━━━━━ 316/316 6.7it/s 46.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.948      0.946      0.974      0.614\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    142/150      7.42G      1.307     0.5737     0.8854         32        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.1it/s 3.9s\n",
            "                   all       1261       1527      0.951      0.946      0.974      0.615\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    143/150      7.42G      1.296     0.5672     0.8831         38        640: 100% ━━━━━━━━━━━━ 316/316 7.0it/s 45.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.2it/s 3.9s\n",
            "                   all       1261       1527      0.948      0.946      0.974      0.614\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    144/150      7.42G      1.279     0.5681     0.8822         36        640: 100% ━━━━━━━━━━━━ 316/316 7.0it/s 45.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.948      0.945      0.974      0.615\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    145/150      7.42G      1.281     0.5661     0.8832         43        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.1s\n",
            "                   all       1261       1527      0.951      0.946      0.974      0.614\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    146/150      7.42G      1.274     0.5634     0.8829         34        640: 100% ━━━━━━━━━━━━ 316/316 7.0it/s 45.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.8it/s 4.2s\n",
            "                   all       1261       1527      0.948      0.946      0.974      0.614\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    147/150      7.42G      1.274     0.5632     0.8825         33        640: 100% ━━━━━━━━━━━━ 316/316 7.0it/s 45.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.6it/s 4.3s\n",
            "                   all       1261       1527      0.951      0.949      0.974      0.614\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    148/150      7.42G      1.275     0.5584     0.8804         38        640: 100% ━━━━━━━━━━━━ 316/316 7.0it/s 45.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 4.9it/s 4.1s\n",
            "                   all       1261       1527      0.951      0.945      0.974      0.614\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    149/150      7.42G      1.263     0.5537     0.8786         36        640: 100% ━━━━━━━━━━━━ 316/316 7.0it/s 45.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527      0.948      0.947      0.974      0.616\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K    150/150      7.42G      1.264     0.5491     0.8798         40        640: 100% ━━━━━━━━━━━━ 316/316 6.9it/s 45.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 5.0it/s 4.0s\n",
            "                   all       1261       1527       0.95      0.947      0.974      0.615\n",
            "\n",
            "150 epochs completed in 2.369 hours.\n",
            "Optimizer stripped from /content/drive/My Drive/YoloV8/testing/data/weights/last.pt, 22.5MB\n",
            "Optimizer stripped from /content/drive/My Drive/YoloV8/testing/data/weights/best.pt, 22.5MB\n",
            "\n",
            "Validating /content/drive/My Drive/YoloV8/testing/data/weights/best.pt...\n",
            "Ultralytics 8.3.199 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "Model summary (fused): 72 layers, 11,126,745 parameters, 0 gradients, 28.4 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 20/20 3.8it/s 5.3s\n",
            "                   all       1261       1527      0.948      0.946      0.974      0.616\n",
            "                 balon       1175       1236       0.98      0.982      0.983      0.578\n",
            "              lineagol        183        184      0.921      0.948      0.979      0.676\n",
            "               palogol        106        107      0.942      0.908       0.96      0.593\n",
            "Speed: 0.1ms preprocess, 0.6ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/My Drive/YoloV8/testing/data\u001b[0m\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/train\n",
            "\n",
            "Copy the command above and run it in a new cell (either code or terminal) to start training.\n",
            "Alternatively, uncomment the line `!{train_command}` above and run this cell.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CHECKPOINT LAST.PT**"
      ],
      "metadata": {
        "id": "Pe6GDgDp0uqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define la ruta a tu archivo last.pt en Google Drive\n",
        "# Este archivo se crea automáticamente en la carpeta de resultados de tu entrenamiento anterior.\n",
        "# Debería estar en '/content/drive/My Drive/YoloV8/testing/data2/weights/last.pt'\n",
        "# Verifica la ubicación exacta de tu archivo last.pt.\n",
        "last_model_path = '/content/drive/My Drive/YoloV8/testing/data/weights/last.pt' # <-- ASEGÚRATE DE QUE ESTA RUTA SEA CORRECTA\n",
        "\n",
        "# Define la ruta a tu archivo dataset.yaml\n",
        "yaml_path = '/content/drive/My Drive/YoloV8/dataset.yaml' # <-- ASEGÚRATE DE QUE ESTA RUTA SEA CORRECTA\n",
        "\n",
        "# Define el número de épocas ADICIONALES para entrenar\n",
        "# Si tu entrenamiento anterior fue de 50 épocas y quieres entrenar 50 más, pon epochs = 50 aquí.\n",
        "epochs_to_add = 5 # Puedes ajustar el número de épocas adicionales\n",
        "\n",
        "# Define el número de épocas de paciencia para Early Stopping\n",
        "# El entrenamiento se detendrá si el rendimiento en validación no mejora durante este número de épocas.\n",
        "patience_epochs = 0 # <--- Ajusta este valor según necesites (ej: 50, 100)\n",
        "\n",
        "# El tamaño de imagen debe ser el mismo que usaste en el entrenamiento anterior\n",
        "img_size = 640 # Asegúrate de que sea el mismo valor que antes\n",
        "batch_size_add = 32 # <--- Define el tamaño del batch aquí (ej: 16, 32, 64)\n",
        "\n",
        "# La carpeta de proyecto y nombre del run deben ser los mismos para que retome el entrenamiento\n",
        "output_project_dir = '/content/drive/My Drive/YoloV8/testing'\n",
        "run_name = 'data' # Asegúrate de que sea el mismo nombre que el run anterior\n",
        "\n",
        "# Construye el comando para retomar el entrenamiento\n",
        "# Usamos el argumento 'resume' y especificamos la ruta al archivo last.pt\n",
        "# Encerrar las rutas con espacios en comillas dobles\n",
        "train_command_resume = f\"yolo train data=\\\"{yaml_path}\\\" model=\\\"{last_model_path}\\\" epochs={epochs_to_add} imgsz={img_size} project=\\\"{output_project_dir}\\\" name=\\\"{run_name}\\\" resume patience={patience_epochs}\\\" batch={batch_size_add}\" # Añadimos project, name y batch\n",
        "\n",
        "print(f\"Generated resume training command: {train_command_resume}\")\n",
        "\n",
        "# Para ejecutar este comando, puedes copiarlo y pegarlo en una celda de terminal de Colab\n",
        "# O puedes usar el magic command ! para ejecutarlo directamente\n",
        "\n",
        "# !!! DESCOMENTA LA SIGUIENTE LÍNEA PARA EJECUTAR EL COMANDO DE ENTRENAMIENTO !!!\n",
        "# !{train_command_resume}\n",
        "\n",
        "print(\"\\nCopy the command above and run it in a new cell (either code or terminal) to resume training.\")\n",
        "print(\"Alternatively, uncomment the line `!{train_command_resume}` above and run this cell.\")"
      ],
      "metadata": {
        "id": "pgdyOwDEzv_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a72b39f7-1d31-43a3-9608-24f7b35b0fdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated resume training command: yolo train data=\"/content/drive/My Drive/YoloV8/dataset.yaml\" model=\"/content/drive/My Drive/YoloV8/testing/data/weights/last.pt\" epochs=5 imgsz=640 project=\"/content/drive/My Drive/YoloV8/testing\" name=\"data\" resume patience=50\" batch=32\n",
            "\n",
            "Copy the command above and run it in a new cell (either code or terminal) to resume training.\n",
            "Alternatively, uncomment the line `!{train_command_resume}` above and run this cell.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "861ab9fc",
        "outputId": "d85e5666-089a-43d3-bc6a-9dbd8f9605c8"
      },
      "source": [
        "from ultralytics import YOLO\n",
        "import os\n",
        "\n",
        "# Define la ruta a tu archivo best.pt en Google Drive\n",
        "# Reemplaza esto con la ruta exacta donde se guardó tu best.pt\n",
        "# Por defecto, si no cambiaste 'project' y 'name', podría estar en:\n",
        "# '/content/drive/My Drive/YoloV8/testing/data/weights/best.pt'\n",
        "# Verifica la carpeta de resultados de tu entrenamiento para encontrar la ruta correcta.\n",
        "best_model_path = '/content/drive/My Drive/YoloV8/testing/data/weights/best.pt' # <-- ASEGÚRATE DE QUE ESTA RUTA SEA CORRECTA\n",
        "\n",
        "# Cargar el modelo entrenado desde el archivo .pt\n",
        "try:\n",
        "    model = YOLO(best_model_path)\n",
        "    print(f\"Modelo cargado exitosamente desde: {best_model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar el modelo: {e}\")\n",
        "    print(f\"Por favor, verifica que la ruta '{best_model_path}' sea correcta y que el archivo best.pt exista.\")\n",
        "\n",
        "# Define la ruta al video que quieres monitorizar\n",
        "# Puedes usar una ruta en tu Google Drive, o subir un video a Colab\n",
        "video_source = '/content/drive/My Drive/YoloV8/testing/videoprueba/' # <-- ASEGÚRATE DE QUE ESTA RUTA SEA CORRECTA a tu video\n",
        "\n",
        "# Directorio donde se guardarán los resultados del video\n",
        "# Por defecto, se guarda en 'runs/detect/predict'\n",
        "# Si quieres guardarlo en tu Drive, puedes especificar una ruta aquí:\n",
        "output_video_dir = '/content/drive/My Drive/YoloV8/testing/videoresultado' # <-- Opcional: especifica una carpeta en Drive\n",
        "\n",
        "# Realizar la predicción en el video\n",
        "# Los resultados (video con detecciones) se guardarán en la carpeta de salida especificada o por defecto.\n",
        "if 'model' in locals(): # Asegurarse de que el modelo se cargó correctamente\n",
        "    print(f\"Ejecutando predicción en el video: {video_source}\")\n",
        "    # Argumento 'save=True' guarda el video con las detecciones\n",
        "    # Argumento 'project' y 'name' para especificar la carpeta de salida si no quieres usar la por defecto\n",
        "    results = model.predict(source=video_source, save=True, project=output_video_dir, name='video_results')\n",
        "\n",
        "    print(\"\\nPredicción en video finalizada.\")\n",
        "    print(f\"Los resultados (video con detecciones) deberían estar en: {os.path.join(output_video_dir, 'video_results') if output_video_dir else 'runs/detect/predict'} \")\n",
        "    print(\"Busca un archivo .mp4 o similar en esa carpeta.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo cargado exitosamente desde: /content/drive/My Drive/YoloV8/testing/data/weights/best.pt\n",
            "Ejecutando predicción en el video: /content/drive/My Drive/YoloV8/testing/videoprueba/\n",
            "\n",
            "WARNING ⚠️ \n",
            "inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
            "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
            "\n",
            "Example:\n",
            "    results = model(source=..., stream=True)  # generator of Results objects\n",
            "    for r in results:\n",
            "        boxes = r.boxes  # Boxes object for bbox outputs\n",
            "        masks = r.masks  # Masks object for segment masks outputs\n",
            "        probs = r.probs  # Class probabilities for classification outputs\n",
            "\n",
            "video 1/1 (frame 1/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 16.2ms\n",
            "video 1/1 (frame 2/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.8ms\n",
            "video 1/1 (frame 3/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.7ms\n",
            "video 1/1 (frame 4/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.7ms\n",
            "video 1/1 (frame 5/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 32.8ms\n",
            "video 1/1 (frame 6/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 22.8ms\n",
            "video 1/1 (frame 7/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.8ms\n",
            "video 1/1 (frame 8/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.7ms\n",
            "video 1/1 (frame 9/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 15.0ms\n",
            "video 1/1 (frame 10/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.7ms\n",
            "video 1/1 (frame 11/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.7ms\n",
            "video 1/1 (frame 12/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.7ms\n",
            "video 1/1 (frame 13/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.8ms\n",
            "video 1/1 (frame 14/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.8ms\n",
            "video 1/1 (frame 15/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 14.8ms\n",
            "video 1/1 (frame 16/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 12.7ms\n",
            "video 1/1 (frame 17/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 14.7ms\n",
            "video 1/1 (frame 18/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 31.7ms\n",
            "video 1/1 (frame 19/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 47.9ms\n",
            "video 1/1 (frame 20/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 32.4ms\n",
            "video 1/1 (frame 21/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 16.0ms\n",
            "video 1/1 (frame 22/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.0ms\n",
            "video 1/1 (frame 23/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.0ms\n",
            "video 1/1 (frame 24/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.0ms\n",
            "video 1/1 (frame 25/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.0ms\n",
            "video 1/1 (frame 26/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 11.9ms\n",
            "video 1/1 (frame 27/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 11.9ms\n",
            "video 1/1 (frame 28/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 11.9ms\n",
            "video 1/1 (frame 29/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 11.9ms\n",
            "video 1/1 (frame 30/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 11.8ms\n",
            "video 1/1 (frame 31/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 12.7ms\n",
            "video 1/1 (frame 32/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 10.0ms\n",
            "video 1/1 (frame 33/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 12.8ms\n",
            "video 1/1 (frame 34/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 15.9ms\n",
            "video 1/1 (frame 35/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 12.6ms\n",
            "video 1/1 (frame 36/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 10.0ms\n",
            "video 1/1 (frame 37/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 10.0ms\n",
            "video 1/1 (frame 38/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 10.0ms\n",
            "video 1/1 (frame 39/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 9.8ms\n",
            "video 1/1 (frame 40/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 9.7ms\n",
            "video 1/1 (frame 41/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 42.6ms\n",
            "video 1/1 (frame 42/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 39.4ms\n",
            "video 1/1 (frame 43/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 22.3ms\n",
            "video 1/1 (frame 44/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.5ms\n",
            "video 1/1 (frame 45/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 35.6ms\n",
            "video 1/1 (frame 46/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 24.3ms\n",
            "video 1/1 (frame 47/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.9ms\n",
            "video 1/1 (frame 48/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.3ms\n",
            "video 1/1 (frame 49/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.7ms\n",
            "video 1/1 (frame 50/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.8ms\n",
            "video 1/1 (frame 51/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.8ms\n",
            "video 1/1 (frame 52/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 37.4ms\n",
            "video 1/1 (frame 53/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 34.1ms\n",
            "video 1/1 (frame 54/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 27.7ms\n",
            "video 1/1 (frame 55/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 11.8ms\n",
            "video 1/1 (frame 56/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.4ms\n",
            "video 1/1 (frame 57/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.1ms\n",
            "video 1/1 (frame 58/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 18.9ms\n",
            "video 1/1 (frame 59/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.5ms\n",
            "video 1/1 (frame 60/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 15.1ms\n",
            "video 1/1 (frame 61/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 16.2ms\n",
            "video 1/1 (frame 62/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 21.3ms\n",
            "video 1/1 (frame 63/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 15.1ms\n",
            "video 1/1 (frame 64/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 34.9ms\n",
            "video 1/1 (frame 65/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 19.6ms\n",
            "video 1/1 (frame 66/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 9.8ms\n",
            "video 1/1 (frame 67/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.7ms\n",
            "video 1/1 (frame 68/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 9.7ms\n",
            "video 1/1 (frame 69/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.7ms\n",
            "video 1/1 (frame 70/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.7ms\n",
            "video 1/1 (frame 71/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.7ms\n",
            "video 1/1 (frame 72/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.7ms\n",
            "video 1/1 (frame 73/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.7ms\n",
            "video 1/1 (frame 74/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.7ms\n",
            "video 1/1 (frame 75/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.4ms\n",
            "video 1/1 (frame 76/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.3ms\n",
            "video 1/1 (frame 77/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.3ms\n",
            "video 1/1 (frame 78/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.3ms\n",
            "video 1/1 (frame 79/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.3ms\n",
            "video 1/1 (frame 80/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.3ms\n",
            "video 1/1 (frame 81/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.1ms\n",
            "video 1/1 (frame 82/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.2ms\n",
            "video 1/1 (frame 83/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.2ms\n",
            "video 1/1 (frame 84/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.1ms\n",
            "video 1/1 (frame 85/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.1ms\n",
            "video 1/1 (frame 86/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.1ms\n",
            "video 1/1 (frame 87/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.8ms\n",
            "video 1/1 (frame 88/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.3ms\n",
            "video 1/1 (frame 89/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.9ms\n",
            "video 1/1 (frame 90/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.9ms\n",
            "video 1/1 (frame 91/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.8ms\n",
            "video 1/1 (frame 92/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.8ms\n",
            "video 1/1 (frame 93/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.8ms\n",
            "video 1/1 (frame 94/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.8ms\n",
            "video 1/1 (frame 95/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.8ms\n",
            "video 1/1 (frame 96/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 10.2ms\n",
            "video 1/1 (frame 97/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 11.2ms\n",
            "video 1/1 (frame 98/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 11.4ms\n",
            "video 1/1 (frame 99/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 13.9ms\n",
            "video 1/1 (frame 100/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 12.3ms\n",
            "video 1/1 (frame 101/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.8ms\n",
            "video 1/1 (frame 102/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.1ms\n",
            "video 1/1 (frame 103/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.1ms\n",
            "video 1/1 (frame 104/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.3ms\n",
            "video 1/1 (frame 105/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.3ms\n",
            "video 1/1 (frame 106/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.9ms\n",
            "video 1/1 (frame 107/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.0ms\n",
            "video 1/1 (frame 108/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.2ms\n",
            "video 1/1 (frame 109/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.9ms\n",
            "video 1/1 (frame 110/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.8ms\n",
            "video 1/1 (frame 111/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.7ms\n",
            "video 1/1 (frame 112/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 9.8ms\n",
            "video 1/1 (frame 113/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.1ms\n",
            "video 1/1 (frame 114/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.9ms\n",
            "video 1/1 (frame 115/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.7ms\n",
            "video 1/1 (frame 116/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.3ms\n",
            "video 1/1 (frame 117/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.0ms\n",
            "video 1/1 (frame 118/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.0ms\n",
            "video 1/1 (frame 119/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.7ms\n",
            "video 1/1 (frame 120/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.8ms\n",
            "video 1/1 (frame 121/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 9.4ms\n",
            "video 1/1 (frame 122/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.5ms\n",
            "video 1/1 (frame 123/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.5ms\n",
            "video 1/1 (frame 124/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.5ms\n",
            "video 1/1 (frame 125/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.4ms\n",
            "video 1/1 (frame 126/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.3ms\n",
            "video 1/1 (frame 127/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.4ms\n",
            "video 1/1 (frame 128/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.3ms\n",
            "video 1/1 (frame 129/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.7ms\n",
            "video 1/1 (frame 130/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.4ms\n",
            "video 1/1 (frame 131/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.7ms\n",
            "video 1/1 (frame 132/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.3ms\n",
            "video 1/1 (frame 133/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.0ms\n",
            "video 1/1 (frame 134/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.5ms\n",
            "video 1/1 (frame 135/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.1ms\n",
            "video 1/1 (frame 136/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.7ms\n",
            "video 1/1 (frame 137/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.0ms\n",
            "video 1/1 (frame 138/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.3ms\n",
            "video 1/1 (frame 139/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.0ms\n",
            "video 1/1 (frame 140/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.4ms\n",
            "video 1/1 (frame 141/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.9ms\n",
            "video 1/1 (frame 142/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.3ms\n",
            "video 1/1 (frame 143/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.1ms\n",
            "video 1/1 (frame 144/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.9ms\n",
            "video 1/1 (frame 145/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.0ms\n",
            "video 1/1 (frame 146/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.1ms\n",
            "video 1/1 (frame 147/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.0ms\n",
            "video 1/1 (frame 148/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.9ms\n",
            "video 1/1 (frame 149/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.2ms\n",
            "video 1/1 (frame 150/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.6ms\n",
            "video 1/1 (frame 151/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.9ms\n",
            "video 1/1 (frame 152/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.1ms\n",
            "video 1/1 (frame 153/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.8ms\n",
            "video 1/1 (frame 154/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.6ms\n",
            "video 1/1 (frame 155/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.9ms\n",
            "video 1/1 (frame 156/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.8ms\n",
            "video 1/1 (frame 157/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.7ms\n",
            "video 1/1 (frame 158/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 14.0ms\n",
            "video 1/1 (frame 159/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.5ms\n",
            "video 1/1 (frame 160/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 10.5ms\n",
            "video 1/1 (frame 161/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.7ms\n",
            "video 1/1 (frame 162/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 10.1ms\n",
            "video 1/1 (frame 163/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.3ms\n",
            "video 1/1 (frame 164/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.3ms\n",
            "video 1/1 (frame 165/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.7ms\n",
            "video 1/1 (frame 166/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 12.2ms\n",
            "video 1/1 (frame 167/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.1ms\n",
            "video 1/1 (frame 168/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.3ms\n",
            "video 1/1 (frame 169/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 12.1ms\n",
            "video 1/1 (frame 170/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.4ms\n",
            "video 1/1 (frame 171/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 12.0ms\n",
            "video 1/1 (frame 172/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 11.2ms\n",
            "video 1/1 (frame 173/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 9.9ms\n",
            "video 1/1 (frame 174/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.7ms\n",
            "video 1/1 (frame 175/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.7ms\n",
            "video 1/1 (frame 176/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.5ms\n",
            "video 1/1 (frame 177/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.7ms\n",
            "video 1/1 (frame 178/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 9.9ms\n",
            "video 1/1 (frame 179/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.8ms\n",
            "video 1/1 (frame 180/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.4ms\n",
            "video 1/1 (frame 181/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.2ms\n",
            "video 1/1 (frame 182/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.9ms\n",
            "video 1/1 (frame 183/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.1ms\n",
            "video 1/1 (frame 184/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.8ms\n",
            "video 1/1 (frame 185/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 12.0ms\n",
            "video 1/1 (frame 186/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 10.2ms\n",
            "video 1/1 (frame 187/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.8ms\n",
            "video 1/1 (frame 188/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.3ms\n",
            "video 1/1 (frame 189/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.0ms\n",
            "video 1/1 (frame 190/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.1ms\n",
            "video 1/1 (frame 191/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.1ms\n",
            "video 1/1 (frame 192/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.6ms\n",
            "video 1/1 (frame 193/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.9ms\n",
            "video 1/1 (frame 194/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.3ms\n",
            "video 1/1 (frame 195/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.9ms\n",
            "video 1/1 (frame 196/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.5ms\n",
            "video 1/1 (frame 197/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.5ms\n",
            "video 1/1 (frame 198/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.9ms\n",
            "video 1/1 (frame 199/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.7ms\n",
            "video 1/1 (frame 200/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.9ms\n",
            "video 1/1 (frame 201/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.7ms\n",
            "video 1/1 (frame 202/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.9ms\n",
            "video 1/1 (frame 203/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.4ms\n",
            "video 1/1 (frame 204/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.8ms\n",
            "video 1/1 (frame 205/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.1ms\n",
            "video 1/1 (frame 206/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.8ms\n",
            "video 1/1 (frame 207/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.8ms\n",
            "video 1/1 (frame 208/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.0ms\n",
            "video 1/1 (frame 209/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.7ms\n",
            "video 1/1 (frame 210/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.9ms\n",
            "video 1/1 (frame 211/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.6ms\n",
            "video 1/1 (frame 212/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.9ms\n",
            "video 1/1 (frame 213/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.4ms\n",
            "video 1/1 (frame 214/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.2ms\n",
            "video 1/1 (frame 215/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.4ms\n",
            "video 1/1 (frame 216/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.3ms\n",
            "video 1/1 (frame 217/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 10.0ms\n",
            "video 1/1 (frame 218/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.2ms\n",
            "video 1/1 (frame 219/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.0ms\n",
            "video 1/1 (frame 220/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.7ms\n",
            "video 1/1 (frame 221/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.0ms\n",
            "video 1/1 (frame 222/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.5ms\n",
            "video 1/1 (frame 223/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.9ms\n",
            "video 1/1 (frame 224/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.5ms\n",
            "video 1/1 (frame 225/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.8ms\n",
            "video 1/1 (frame 226/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.9ms\n",
            "video 1/1 (frame 227/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.5ms\n",
            "video 1/1 (frame 228/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 9.0ms\n",
            "video 1/1 (frame 229/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.3ms\n",
            "video 1/1 (frame 230/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.2ms\n",
            "video 1/1 (frame 231/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.0ms\n",
            "video 1/1 (frame 232/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.8ms\n",
            "video 1/1 (frame 233/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.1ms\n",
            "video 1/1 (frame 234/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.1ms\n",
            "video 1/1 (frame 235/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.0ms\n",
            "video 1/1 (frame 236/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.6ms\n",
            "video 1/1 (frame 237/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.1ms\n",
            "video 1/1 (frame 238/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.1ms\n",
            "video 1/1 (frame 239/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.1ms\n",
            "video 1/1 (frame 240/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.2ms\n",
            "video 1/1 (frame 241/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.5ms\n",
            "video 1/1 (frame 242/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.7ms\n",
            "video 1/1 (frame 243/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.5ms\n",
            "video 1/1 (frame 244/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.0ms\n",
            "video 1/1 (frame 245/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.4ms\n",
            "video 1/1 (frame 246/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.2ms\n",
            "video 1/1 (frame 247/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.4ms\n",
            "video 1/1 (frame 248/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.1ms\n",
            "video 1/1 (frame 249/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.9ms\n",
            "video 1/1 (frame 250/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.9ms\n",
            "video 1/1 (frame 251/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.1ms\n",
            "video 1/1 (frame 252/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.1ms\n",
            "video 1/1 (frame 253/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.9ms\n",
            "video 1/1 (frame 254/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.8ms\n",
            "video 1/1 (frame 255/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 11.9ms\n",
            "video 1/1 (frame 256/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.5ms\n",
            "video 1/1 (frame 257/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 11.0ms\n",
            "video 1/1 (frame 258/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.4ms\n",
            "video 1/1 (frame 259/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.0ms\n",
            "video 1/1 (frame 260/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.5ms\n",
            "video 1/1 (frame 261/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.4ms\n",
            "video 1/1 (frame 262/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.3ms\n",
            "video 1/1 (frame 263/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.9ms\n",
            "video 1/1 (frame 264/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.0ms\n",
            "video 1/1 (frame 265/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.3ms\n",
            "video 1/1 (frame 266/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.7ms\n",
            "video 1/1 (frame 267/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.0ms\n",
            "video 1/1 (frame 268/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.8ms\n",
            "video 1/1 (frame 269/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.6ms\n",
            "video 1/1 (frame 270/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.7ms\n",
            "video 1/1 (frame 271/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.8ms\n",
            "video 1/1 (frame 272/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.3ms\n",
            "video 1/1 (frame 273/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.5ms\n",
            "video 1/1 (frame 274/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.1ms\n",
            "video 1/1 (frame 275/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.6ms\n",
            "video 1/1 (frame 276/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.3ms\n",
            "video 1/1 (frame 277/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.3ms\n",
            "video 1/1 (frame 278/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 11.5ms\n",
            "video 1/1 (frame 279/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 10.4ms\n",
            "video 1/1 (frame 280/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 11.1ms\n",
            "video 1/1 (frame 281/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 10.4ms\n",
            "video 1/1 (frame 282/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 11.3ms\n",
            "video 1/1 (frame 283/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 10.8ms\n",
            "video 1/1 (frame 284/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 10.5ms\n",
            "video 1/1 (frame 285/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 11.2ms\n",
            "video 1/1 (frame 286/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 11.9ms\n",
            "video 1/1 (frame 287/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 12.2ms\n",
            "video 1/1 (frame 288/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.8ms\n",
            "video 1/1 (frame 289/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.3ms\n",
            "video 1/1 (frame 290/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.2ms\n",
            "video 1/1 (frame 291/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.2ms\n",
            "video 1/1 (frame 292/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.3ms\n",
            "video 1/1 (frame 293/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.2ms\n",
            "video 1/1 (frame 294/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.2ms\n",
            "video 1/1 (frame 295/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.3ms\n",
            "video 1/1 (frame 296/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.8ms\n",
            "video 1/1 (frame 297/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.7ms\n",
            "video 1/1 (frame 298/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.2ms\n",
            "video 1/1 (frame 299/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.2ms\n",
            "video 1/1 (frame 300/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.2ms\n",
            "video 1/1 (frame 301/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.3ms\n",
            "video 1/1 (frame 302/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.2ms\n",
            "video 1/1 (frame 303/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.2ms\n",
            "video 1/1 (frame 304/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.2ms\n",
            "video 1/1 (frame 305/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.3ms\n",
            "video 1/1 (frame 306/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.8ms\n",
            "video 1/1 (frame 307/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.7ms\n",
            "video 1/1 (frame 308/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.7ms\n",
            "video 1/1 (frame 309/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.7ms\n",
            "video 1/1 (frame 310/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.7ms\n",
            "video 1/1 (frame 311/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.7ms\n",
            "video 1/1 (frame 312/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.7ms\n",
            "video 1/1 (frame 313/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 314/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.9ms\n",
            "video 1/1 (frame 315/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 316/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 317/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.6ms\n",
            "video 1/1 (frame 318/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.6ms\n",
            "video 1/1 (frame 319/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.1ms\n",
            "video 1/1 (frame 320/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.1ms\n",
            "video 1/1 (frame 321/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.6ms\n",
            "video 1/1 (frame 322/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.2ms\n",
            "video 1/1 (frame 323/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.1ms\n",
            "video 1/1 (frame 324/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.5ms\n",
            "video 1/1 (frame 325/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 6.1ms\n",
            "video 1/1 (frame 326/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.1ms\n",
            "video 1/1 (frame 327/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.9ms\n",
            "video 1/1 (frame 328/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 14.5ms\n",
            "video 1/1 (frame 329/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.2ms\n",
            "video 1/1 (frame 330/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.7ms\n",
            "video 1/1 (frame 331/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.1ms\n",
            "video 1/1 (frame 332/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.1ms\n",
            "video 1/1 (frame 333/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.1ms\n",
            "video 1/1 (frame 334/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.1ms\n",
            "video 1/1 (frame 335/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.2ms\n",
            "video 1/1 (frame 336/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.2ms\n",
            "video 1/1 (frame 337/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.1ms\n",
            "video 1/1 (frame 338/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.0ms\n",
            "video 1/1 (frame 339/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.0ms\n",
            "video 1/1 (frame 340/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.1ms\n",
            "video 1/1 (frame 341/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.1ms\n",
            "video 1/1 (frame 342/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.1ms\n",
            "video 1/1 (frame 343/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.1ms\n",
            "video 1/1 (frame 344/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.2ms\n",
            "video 1/1 (frame 345/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.4ms\n",
            "video 1/1 (frame 346/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.8ms\n",
            "video 1/1 (frame 347/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.8ms\n",
            "video 1/1 (frame 348/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.8ms\n",
            "video 1/1 (frame 349/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.8ms\n",
            "video 1/1 (frame 350/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.4ms\n",
            "video 1/1 (frame 351/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.8ms\n",
            "video 1/1 (frame 352/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.8ms\n",
            "video 1/1 (frame 353/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.8ms\n",
            "video 1/1 (frame 354/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.8ms\n",
            "video 1/1 (frame 355/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.5ms\n",
            "video 1/1 (frame 356/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.7ms\n",
            "video 1/1 (frame 357/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.8ms\n",
            "video 1/1 (frame 358/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.8ms\n",
            "video 1/1 (frame 359/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.8ms\n",
            "video 1/1 (frame 360/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.1ms\n",
            "video 1/1 (frame 361/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.7ms\n",
            "video 1/1 (frame 362/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.7ms\n",
            "video 1/1 (frame 363/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.7ms\n",
            "video 1/1 (frame 364/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.9ms\n",
            "video 1/1 (frame 365/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 9.6ms\n",
            "video 1/1 (frame 366/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.9ms\n",
            "video 1/1 (frame 367/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.0ms\n",
            "video 1/1 (frame 368/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.7ms\n",
            "video 1/1 (frame 369/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.7ms\n",
            "video 1/1 (frame 370/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.3ms\n",
            "video 1/1 (frame 371/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.9ms\n",
            "video 1/1 (frame 372/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.2ms\n",
            "video 1/1 (frame 373/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.7ms\n",
            "video 1/1 (frame 374/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.9ms\n",
            "video 1/1 (frame 375/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.8ms\n",
            "video 1/1 (frame 376/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 377/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 378/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 379/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.8ms\n",
            "video 1/1 (frame 380/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 381/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 382/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 383/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.1ms\n",
            "video 1/1 (frame 384/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 6.2ms\n",
            "video 1/1 (frame 385/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 5.7ms\n",
            "video 1/1 (frame 386/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 387/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.6ms\n",
            "video 1/1 (frame 388/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.1ms\n",
            "video 1/1 (frame 389/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 390/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 391/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 392/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.2ms\n",
            "video 1/1 (frame 393/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.9ms\n",
            "video 1/1 (frame 394/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.7ms\n",
            "video 1/1 (frame 395/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.6ms\n",
            "video 1/1 (frame 396/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 397/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 11.3ms\n",
            "video 1/1 (frame 398/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.0ms\n",
            "video 1/1 (frame 399/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 11.3ms\n",
            "video 1/1 (frame 400/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.4ms\n",
            "video 1/1 (frame 401/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.7ms\n",
            "video 1/1 (frame 402/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 403/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.3ms\n",
            "video 1/1 (frame 404/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 405/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 406/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 407/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 408/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.4ms\n",
            "video 1/1 (frame 409/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.4ms\n",
            "video 1/1 (frame 410/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 411/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.1ms\n",
            "video 1/1 (frame 412/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 413/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 414/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 415/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 416/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.4ms\n",
            "video 1/1 (frame 417/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.6ms\n",
            "video 1/1 (frame 418/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 419/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.6ms\n",
            "video 1/1 (frame 420/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 421/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.1ms\n",
            "video 1/1 (frame 422/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 423/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 424/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.8ms\n",
            "video 1/1 (frame 425/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 6.8ms\n",
            "video 1/1 (frame 426/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 5.9ms\n",
            "video 1/1 (frame 427/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 6.0ms\n",
            "video 1/1 (frame 428/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 429/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 430/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.1ms\n",
            "video 1/1 (frame 431/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 432/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 433/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 434/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.1ms\n",
            "video 1/1 (frame 435/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 436/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 437/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.4ms\n",
            "video 1/1 (frame 438/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 439/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 440/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 441/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 442/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.8ms\n",
            "video 1/1 (frame 443/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.1ms\n",
            "video 1/1 (frame 444/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 5.7ms\n",
            "video 1/1 (frame 445/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 446/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 447/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 448/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 449/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 450/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 451/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 5.7ms\n",
            "video 1/1 (frame 452/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 453/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 454/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 455/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 456/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 457/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 458/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.4ms\n",
            "video 1/1 (frame 459/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.6ms\n",
            "video 1/1 (frame 460/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 461/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 462/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.2ms\n",
            "video 1/1 (frame 463/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 464/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 465/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 466/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.7ms\n",
            "video 1/1 (frame 467/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.0ms\n",
            "video 1/1 (frame 468/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.9ms\n",
            "video 1/1 (frame 469/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 470/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.0ms\n",
            "video 1/1 (frame 471/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 472/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 473/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 474/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 475/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.4ms\n",
            "video 1/1 (frame 476/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.1ms\n",
            "video 1/1 (frame 477/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.8ms\n",
            "video 1/1 (frame 478/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 479/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.8ms\n",
            "video 1/1 (frame 480/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.6ms\n",
            "video 1/1 (frame 481/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.1ms\n",
            "video 1/1 (frame 482/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 483/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.1ms\n",
            "video 1/1 (frame 484/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.2ms\n",
            "video 1/1 (frame 485/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 486/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 487/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 488/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 489/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 490/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 491/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 492/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.3ms\n",
            "video 1/1 (frame 493/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 494/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 495/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 496/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.8ms\n",
            "video 1/1 (frame 497/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.0ms\n",
            "video 1/1 (frame 498/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.2ms\n",
            "video 1/1 (frame 499/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.7ms\n",
            "video 1/1 (frame 500/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.0ms\n",
            "video 1/1 (frame 501/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.9ms\n",
            "video 1/1 (frame 502/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.0ms\n",
            "video 1/1 (frame 503/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.0ms\n",
            "video 1/1 (frame 504/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.9ms\n",
            "video 1/1 (frame 505/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.7ms\n",
            "video 1/1 (frame 506/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.7ms\n",
            "video 1/1 (frame 507/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.7ms\n",
            "video 1/1 (frame 508/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.1ms\n",
            "video 1/1 (frame 509/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.7ms\n",
            "video 1/1 (frame 510/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.0ms\n",
            "video 1/1 (frame 511/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.7ms\n",
            "video 1/1 (frame 512/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.7ms\n",
            "video 1/1 (frame 513/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.9ms\n",
            "video 1/1 (frame 514/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.8ms\n",
            "video 1/1 (frame 515/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.9ms\n",
            "video 1/1 (frame 516/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.8ms\n",
            "video 1/1 (frame 517/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.4ms\n",
            "video 1/1 (frame 518/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.7ms\n",
            "video 1/1 (frame 519/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.5ms\n",
            "video 1/1 (frame 520/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.1ms\n",
            "video 1/1 (frame 521/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.7ms\n",
            "video 1/1 (frame 522/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.6ms\n",
            "video 1/1 (frame 523/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.1ms\n",
            "video 1/1 (frame 524/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.3ms\n",
            "video 1/1 (frame 525/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.3ms\n",
            "video 1/1 (frame 526/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 527/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 528/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 529/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 530/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 531/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 532/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 533/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 534/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.9ms\n",
            "video 1/1 (frame 535/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 536/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.4ms\n",
            "video 1/1 (frame 537/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 13.3ms\n",
            "video 1/1 (frame 538/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 539/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.8ms\n",
            "video 1/1 (frame 540/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 541/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 542/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 543/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.8ms\n",
            "video 1/1 (frame 544/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.1ms\n",
            "video 1/1 (frame 545/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.8ms\n",
            "video 1/1 (frame 546/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 547/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 548/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 549/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 550/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 551/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 552/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 553/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 554/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 5.7ms\n",
            "video 1/1 (frame 555/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 5.7ms\n",
            "video 1/1 (frame 556/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 557/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 558/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.4ms\n",
            "video 1/1 (frame 559/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 560/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.1ms\n",
            "video 1/1 (frame 561/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.1ms\n",
            "video 1/1 (frame 562/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 563/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 564/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 565/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.1ms\n",
            "video 1/1 (frame 566/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 567/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.1ms\n",
            "video 1/1 (frame 568/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 569/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 570/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 571/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 572/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 573/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 574/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 575/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 576/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.4ms\n",
            "video 1/1 (frame 577/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 578/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.4ms\n",
            "video 1/1 (frame 579/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 580/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 581/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 582/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 583/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 584/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.8ms\n",
            "video 1/1 (frame 585/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 586/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 587/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 588/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.1ms\n",
            "video 1/1 (frame 589/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 590/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 591/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 592/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 593/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 594/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 595/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.1ms\n",
            "video 1/1 (frame 596/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 5.8ms\n",
            "video 1/1 (frame 597/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 598/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 6.1ms\n",
            "video 1/1 (frame 599/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 7.6ms\n",
            "video 1/1 (frame 600/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 601/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.1ms\n",
            "video 1/1 (frame 602/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 603/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.1ms\n",
            "video 1/1 (frame 604/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.5ms\n",
            "video 1/1 (frame 605/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 606/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 607/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 608/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.2ms\n",
            "video 1/1 (frame 609/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 610/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 611/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.9ms\n",
            "video 1/1 (frame 612/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.8ms\n",
            "video 1/1 (frame 613/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.7ms\n",
            "video 1/1 (frame 614/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.7ms\n",
            "video 1/1 (frame 615/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.7ms\n",
            "video 1/1 (frame 616/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 10.1ms\n",
            "video 1/1 (frame 617/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.9ms\n",
            "video 1/1 (frame 618/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 619/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.6ms\n",
            "video 1/1 (frame 620/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.0ms\n",
            "video 1/1 (frame 621/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 622/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.8ms\n",
            "video 1/1 (frame 623/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 624/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.6ms\n",
            "video 1/1 (frame 625/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 626/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.7ms\n",
            "video 1/1 (frame 627/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.7ms\n",
            "video 1/1 (frame 628/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.7ms\n",
            "video 1/1 (frame 629/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.9ms\n",
            "video 1/1 (frame 630/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.9ms\n",
            "video 1/1 (frame 631/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.3ms\n",
            "video 1/1 (frame 632/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.8ms\n",
            "video 1/1 (frame 633/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.1ms\n",
            "video 1/1 (frame 634/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 635/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 636/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 637/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.4ms\n",
            "video 1/1 (frame 638/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 639/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 640/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 641/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.0ms\n",
            "video 1/1 (frame 642/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 643/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.8ms\n",
            "video 1/1 (frame 644/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.4ms\n",
            "video 1/1 (frame 645/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.8ms\n",
            "video 1/1 (frame 646/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.2ms\n",
            "video 1/1 (frame 647/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 648/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 649/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.2ms\n",
            "video 1/1 (frame 650/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 651/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 652/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 653/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.4ms\n",
            "video 1/1 (frame 654/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.4ms\n",
            "video 1/1 (frame 655/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.4ms\n",
            "video 1/1 (frame 656/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 657/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 658/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.6ms\n",
            "video 1/1 (frame 659/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 660/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 661/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.3ms\n",
            "video 1/1 (frame 662/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.8ms\n",
            "video 1/1 (frame 663/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 664/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.7ms\n",
            "video 1/1 (frame 665/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.6ms\n",
            "video 1/1 (frame 666/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 667/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 668/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 669/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 670/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 671/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 11.4ms\n",
            "video 1/1 (frame 672/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.9ms\n",
            "video 1/1 (frame 673/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 674/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 675/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 676/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 677/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.2ms\n",
            "video 1/1 (frame 678/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 679/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 680/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 681/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.5ms\n",
            "video 1/1 (frame 682/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 683/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.7ms\n",
            "video 1/1 (frame 684/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 685/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 8.6ms\n",
            "video 1/1 (frame 686/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 687/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 688/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.8ms\n",
            "video 1/1 (frame 689/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 690/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 691/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 692/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 8.1ms\n",
            "video 1/1 (frame 693/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 5.7ms\n",
            "video 1/1 (frame 694/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 7.0ms\n",
            "video 1/1 (frame 695/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 696/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.0ms\n",
            "video 1/1 (frame 697/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 698/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 699/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 700/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 701/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 702/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 703/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 704/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.6ms\n",
            "video 1/1 (frame 705/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 706/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 707/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 708/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.7ms\n",
            "video 1/1 (frame 709/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.4ms\n",
            "video 1/1 (frame 710/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 711/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.8ms\n",
            "video 1/1 (frame 712/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.3ms\n",
            "video 1/1 (frame 713/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.2ms\n",
            "video 1/1 (frame 714/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 715/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 716/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.6ms\n",
            "video 1/1 (frame 717/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.2ms\n",
            "video 1/1 (frame 718/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 719/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.3ms\n",
            "video 1/1 (frame 720/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.9ms\n",
            "video 1/1 (frame 721/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.2ms\n",
            "video 1/1 (frame 722/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 723/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 724/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.1ms\n",
            "video 1/1 (frame 725/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 726/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 727/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 728/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.7ms\n",
            "video 1/1 (frame 729/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.0ms\n",
            "video 1/1 (frame 730/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 731/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 732/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 733/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 734/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 735/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 736/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.4ms\n",
            "video 1/1 (frame 737/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.7ms\n",
            "video 1/1 (frame 738/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 11.9ms\n",
            "video 1/1 (frame 739/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 6.4ms\n",
            "video 1/1 (frame 740/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 741/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 742/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 743/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 744/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.0ms\n",
            "video 1/1 (frame 745/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.1ms\n",
            "video 1/1 (frame 746/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.2ms\n",
            "video 1/1 (frame 747/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.1ms\n",
            "video 1/1 (frame 748/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 749/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 750/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 751/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 752/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.1ms\n",
            "video 1/1 (frame 753/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.6ms\n",
            "video 1/1 (frame 754/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.3ms\n",
            "video 1/1 (frame 755/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 756/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.7ms\n",
            "video 1/1 (frame 757/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.3ms\n",
            "video 1/1 (frame 758/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.1ms\n",
            "video 1/1 (frame 759/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.2ms\n",
            "video 1/1 (frame 760/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.7ms\n",
            "video 1/1 (frame 761/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 9.0ms\n",
            "video 1/1 (frame 762/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.6ms\n",
            "video 1/1 (frame 763/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 5.8ms\n",
            "video 1/1 (frame 764/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 5.8ms\n",
            "video 1/1 (frame 765/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.8ms\n",
            "video 1/1 (frame 766/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.1ms\n",
            "video 1/1 (frame 767/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.7ms\n",
            "video 1/1 (frame 768/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 769/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 770/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.8ms\n",
            "video 1/1 (frame 771/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 772/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 773/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 774/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.7ms\n",
            "video 1/1 (frame 775/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 776/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 777/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.7ms\n",
            "video 1/1 (frame 778/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.8ms\n",
            "video 1/1 (frame 779/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 780/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 781/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 782/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.2ms\n",
            "video 1/1 (frame 783/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 784/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.1ms\n",
            "video 1/1 (frame 785/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 786/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 787/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 788/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 789/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 790/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.9ms\n",
            "video 1/1 (frame 791/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.1ms\n",
            "video 1/1 (frame 792/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 793/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.1ms\n",
            "video 1/1 (frame 794/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.7ms\n",
            "video 1/1 (frame 795/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.3ms\n",
            "video 1/1 (frame 796/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 797/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 798/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.6ms\n",
            "video 1/1 (frame 799/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.2ms\n",
            "video 1/1 (frame 800/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 801/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 802/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.8ms\n",
            "video 1/1 (frame 803/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.3ms\n",
            "video 1/1 (frame 804/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.5ms\n",
            "video 1/1 (frame 805/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 11.5ms\n",
            "video 1/1 (frame 806/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.9ms\n",
            "video 1/1 (frame 807/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.4ms\n",
            "video 1/1 (frame 808/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 809/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 810/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.0ms\n",
            "video 1/1 (frame 811/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.7ms\n",
            "video 1/1 (frame 812/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.1ms\n",
            "video 1/1 (frame 813/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.8ms\n",
            "video 1/1 (frame 814/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 815/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.8ms\n",
            "video 1/1 (frame 816/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.3ms\n",
            "video 1/1 (frame 817/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 818/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 819/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.9ms\n",
            "video 1/1 (frame 820/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 821/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 822/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 823/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.2ms\n",
            "video 1/1 (frame 824/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.2ms\n",
            "video 1/1 (frame 825/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.0ms\n",
            "video 1/1 (frame 826/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.7ms\n",
            "video 1/1 (frame 827/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.1ms\n",
            "video 1/1 (frame 828/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.4ms\n",
            "video 1/1 (frame 829/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 830/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.5ms\n",
            "video 1/1 (frame 831/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 832/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 833/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 834/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 835/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.2ms\n",
            "video 1/1 (frame 836/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 837/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 838/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 839/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.3ms\n",
            "video 1/1 (frame 840/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 841/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 842/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.1ms\n",
            "video 1/1 (frame 843/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 844/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.4ms\n",
            "video 1/1 (frame 845/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 846/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.2ms\n",
            "video 1/1 (frame 847/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.2ms\n",
            "video 1/1 (frame 848/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 849/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 850/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 851/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 852/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.2ms\n",
            "video 1/1 (frame 853/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 854/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 855/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.1ms\n",
            "video 1/1 (frame 856/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 857/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 858/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.5ms\n",
            "video 1/1 (frame 859/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.4ms\n",
            "video 1/1 (frame 860/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.0ms\n",
            "video 1/1 (frame 861/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.9ms\n",
            "video 1/1 (frame 862/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.0ms\n",
            "video 1/1 (frame 863/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.3ms\n",
            "video 1/1 (frame 864/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.6ms\n",
            "video 1/1 (frame 865/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 866/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 867/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 868/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 10.1ms\n",
            "video 1/1 (frame 869/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 13.7ms\n",
            "video 1/1 (frame 870/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.1ms\n",
            "video 1/1 (frame 871/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 5.9ms\n",
            "video 1/1 (frame 872/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.2ms\n",
            "video 1/1 (frame 873/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.9ms\n",
            "video 1/1 (frame 874/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.9ms\n",
            "video 1/1 (frame 875/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.0ms\n",
            "video 1/1 (frame 876/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.0ms\n",
            "video 1/1 (frame 877/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 878/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 5.9ms\n",
            "video 1/1 (frame 879/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.5ms\n",
            "video 1/1 (frame 880/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 881/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.0ms\n",
            "video 1/1 (frame 882/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 883/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.2ms\n",
            "video 1/1 (frame 884/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.3ms\n",
            "video 1/1 (frame 885/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.1ms\n",
            "video 1/1 (frame 886/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 887/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.5ms\n",
            "video 1/1 (frame 888/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.0ms\n",
            "video 1/1 (frame 889/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 890/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 891/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.2ms\n",
            "video 1/1 (frame 892/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 893/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 894/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 895/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 896/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.4ms\n",
            "video 1/1 (frame 897/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 898/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 899/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.6ms\n",
            "video 1/1 (frame 900/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 901/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 902/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 903/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 904/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.3ms\n",
            "video 1/1 (frame 905/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 906/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 907/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.7ms\n",
            "video 1/1 (frame 908/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 909/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.0ms\n",
            "video 1/1 (frame 910/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.3ms\n",
            "video 1/1 (frame 911/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.0ms\n",
            "video 1/1 (frame 912/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 913/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 914/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 915/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.7ms\n",
            "video 1/1 (frame 916/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 917/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 918/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.6ms\n",
            "video 1/1 (frame 919/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.2ms\n",
            "video 1/1 (frame 920/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 921/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 922/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.2ms\n",
            "video 1/1 (frame 923/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.5ms\n",
            "video 1/1 (frame 924/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.0ms\n",
            "video 1/1 (frame 925/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 11.0ms\n",
            "video 1/1 (frame 926/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.3ms\n",
            "video 1/1 (frame 927/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.4ms\n",
            "video 1/1 (frame 928/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.1ms\n",
            "video 1/1 (frame 929/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 930/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.0ms\n",
            "video 1/1 (frame 931/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.1ms\n",
            "video 1/1 (frame 932/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.9ms\n",
            "video 1/1 (frame 933/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 14.7ms\n",
            "video 1/1 (frame 934/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 16.2ms\n",
            "video 1/1 (frame 935/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.8ms\n",
            "video 1/1 (frame 936/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.1ms\n",
            "video 1/1 (frame 937/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.0ms\n",
            "video 1/1 (frame 938/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.6ms\n",
            "video 1/1 (frame 939/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.7ms\n",
            "video 1/1 (frame 940/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 11.2ms\n",
            "video 1/1 (frame 941/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.8ms\n",
            "video 1/1 (frame 942/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.2ms\n",
            "video 1/1 (frame 943/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.8ms\n",
            "video 1/1 (frame 944/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.5ms\n",
            "video 1/1 (frame 945/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.1ms\n",
            "video 1/1 (frame 946/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.5ms\n",
            "video 1/1 (frame 947/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 10.7ms\n",
            "video 1/1 (frame 948/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 12.1ms\n",
            "video 1/1 (frame 949/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.9ms\n",
            "video 1/1 (frame 950/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.1ms\n",
            "video 1/1 (frame 951/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.3ms\n",
            "video 1/1 (frame 952/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.4ms\n",
            "video 1/1 (frame 953/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.1ms\n",
            "video 1/1 (frame 954/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.5ms\n",
            "video 1/1 (frame 955/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.8ms\n",
            "video 1/1 (frame 956/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.5ms\n",
            "video 1/1 (frame 957/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.9ms\n",
            "video 1/1 (frame 958/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.6ms\n",
            "video 1/1 (frame 959/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.5ms\n",
            "video 1/1 (frame 960/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.1ms\n",
            "video 1/1 (frame 961/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.6ms\n",
            "video 1/1 (frame 962/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.8ms\n",
            "video 1/1 (frame 963/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.3ms\n",
            "video 1/1 (frame 964/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.4ms\n",
            "video 1/1 (frame 965/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.3ms\n",
            "video 1/1 (frame 966/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.9ms\n",
            "video 1/1 (frame 967/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.6ms\n",
            "video 1/1 (frame 968/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.1ms\n",
            "video 1/1 (frame 969/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.3ms\n",
            "video 1/1 (frame 970/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.3ms\n",
            "video 1/1 (frame 971/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.1ms\n",
            "video 1/1 (frame 972/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.9ms\n",
            "video 1/1 (frame 973/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 10.2ms\n",
            "video 1/1 (frame 974/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 9.4ms\n",
            "video 1/1 (frame 975/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.6ms\n",
            "video 1/1 (frame 976/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.6ms\n",
            "video 1/1 (frame 977/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.2ms\n",
            "video 1/1 (frame 978/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 11.5ms\n",
            "video 1/1 (frame 979/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 6.9ms\n",
            "video 1/1 (frame 980/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 11.7ms\n",
            "video 1/1 (frame 981/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 9.3ms\n",
            "video 1/1 (frame 982/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 12.5ms\n",
            "video 1/1 (frame 983/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.6ms\n",
            "video 1/1 (frame 984/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 10.2ms\n",
            "video 1/1 (frame 985/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 11.1ms\n",
            "video 1/1 (frame 986/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.7ms\n",
            "video 1/1 (frame 987/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.7ms\n",
            "video 1/1 (frame 988/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.5ms\n",
            "video 1/1 (frame 989/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.2ms\n",
            "video 1/1 (frame 990/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.6ms\n",
            "video 1/1 (frame 991/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.2ms\n",
            "video 1/1 (frame 992/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.2ms\n",
            "video 1/1 (frame 993/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.9ms\n",
            "video 1/1 (frame 994/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.2ms\n",
            "video 1/1 (frame 995/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.3ms\n",
            "video 1/1 (frame 996/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.6ms\n",
            "video 1/1 (frame 997/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.4ms\n",
            "video 1/1 (frame 998/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.9ms\n",
            "video 1/1 (frame 999/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.1ms\n",
            "video 1/1 (frame 1000/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.5ms\n",
            "video 1/1 (frame 1001/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 13.3ms\n",
            "video 1/1 (frame 1002/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.3ms\n",
            "video 1/1 (frame 1003/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.0ms\n",
            "video 1/1 (frame 1004/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.8ms\n",
            "video 1/1 (frame 1005/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.5ms\n",
            "video 1/1 (frame 1006/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.0ms\n",
            "video 1/1 (frame 1007/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.7ms\n",
            "video 1/1 (frame 1008/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.2ms\n",
            "video 1/1 (frame 1009/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.8ms\n",
            "video 1/1 (frame 1010/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 11.8ms\n",
            "video 1/1 (frame 1011/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 11.2ms\n",
            "video 1/1 (frame 1012/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.3ms\n",
            "video 1/1 (frame 1013/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.5ms\n",
            "video 1/1 (frame 1014/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.0ms\n",
            "video 1/1 (frame 1015/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.5ms\n",
            "video 1/1 (frame 1016/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.2ms\n",
            "video 1/1 (frame 1017/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.8ms\n",
            "video 1/1 (frame 1018/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.6ms\n",
            "video 1/1 (frame 1019/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.3ms\n",
            "video 1/1 (frame 1020/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.4ms\n",
            "video 1/1 (frame 1021/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 7.0ms\n",
            "video 1/1 (frame 1022/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 7.0ms\n",
            "video 1/1 (frame 1023/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 8.6ms\n",
            "video 1/1 (frame 1024/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 9.7ms\n",
            "video 1/1 (frame 1025/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 7.3ms\n",
            "video 1/1 (frame 1026/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 9.6ms\n",
            "video 1/1 (frame 1027/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 8.6ms\n",
            "video 1/1 (frame 1028/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 9.0ms\n",
            "video 1/1 (frame 1029/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.7ms\n",
            "video 1/1 (frame 1030/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.6ms\n",
            "video 1/1 (frame 1031/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 11.4ms\n",
            "video 1/1 (frame 1032/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 10.0ms\n",
            "video 1/1 (frame 1033/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.7ms\n",
            "video 1/1 (frame 1034/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 11.6ms\n",
            "video 1/1 (frame 1035/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 13.4ms\n",
            "video 1/1 (frame 1036/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 13.5ms\n",
            "video 1/1 (frame 1037/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.6ms\n",
            "video 1/1 (frame 1038/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.9ms\n",
            "video 1/1 (frame 1039/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.5ms\n",
            "video 1/1 (frame 1040/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.2ms\n",
            "video 1/1 (frame 1041/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.9ms\n",
            "video 1/1 (frame 1042/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.8ms\n",
            "video 1/1 (frame 1043/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.1ms\n",
            "video 1/1 (frame 1044/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.0ms\n",
            "video 1/1 (frame 1045/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.9ms\n",
            "video 1/1 (frame 1046/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.0ms\n",
            "video 1/1 (frame 1047/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.8ms\n",
            "video 1/1 (frame 1048/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.7ms\n",
            "video 1/1 (frame 1049/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 12.8ms\n",
            "video 1/1 (frame 1050/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.2ms\n",
            "video 1/1 (frame 1051/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.6ms\n",
            "video 1/1 (frame 1052/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.6ms\n",
            "video 1/1 (frame 1053/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.3ms\n",
            "video 1/1 (frame 1054/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.5ms\n",
            "video 1/1 (frame 1055/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.2ms\n",
            "video 1/1 (frame 1056/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 9.9ms\n",
            "video 1/1 (frame 1057/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 9.1ms\n",
            "video 1/1 (frame 1058/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 9.5ms\n",
            "video 1/1 (frame 1059/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.3ms\n",
            "video 1/1 (frame 1060/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.0ms\n",
            "video 1/1 (frame 1061/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.2ms\n",
            "video 1/1 (frame 1062/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.3ms\n",
            "video 1/1 (frame 1063/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.6ms\n",
            "video 1/1 (frame 1064/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 10.4ms\n",
            "video 1/1 (frame 1065/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.3ms\n",
            "video 1/1 (frame 1066/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.6ms\n",
            "video 1/1 (frame 1067/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.3ms\n",
            "video 1/1 (frame 1068/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 9.0ms\n",
            "video 1/1 (frame 1069/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.5ms\n",
            "video 1/1 (frame 1070/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 9.3ms\n",
            "video 1/1 (frame 1071/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.9ms\n",
            "video 1/1 (frame 1072/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.4ms\n",
            "video 1/1 (frame 1073/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.0ms\n",
            "video 1/1 (frame 1074/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.1ms\n",
            "video 1/1 (frame 1075/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.9ms\n",
            "video 1/1 (frame 1076/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.9ms\n",
            "video 1/1 (frame 1077/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.3ms\n",
            "video 1/1 (frame 1078/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.5ms\n",
            "video 1/1 (frame 1079/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.1ms\n",
            "video 1/1 (frame 1080/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.4ms\n",
            "video 1/1 (frame 1081/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 9.3ms\n",
            "video 1/1 (frame 1082/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 10.3ms\n",
            "video 1/1 (frame 1083/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.3ms\n",
            "video 1/1 (frame 1084/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.9ms\n",
            "video 1/1 (frame 1085/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.5ms\n",
            "video 1/1 (frame 1086/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.7ms\n",
            "video 1/1 (frame 1087/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 9.2ms\n",
            "video 1/1 (frame 1088/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.6ms\n",
            "video 1/1 (frame 1089/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.8ms\n",
            "video 1/1 (frame 1090/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 9.6ms\n",
            "video 1/1 (frame 1091/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 11.1ms\n",
            "video 1/1 (frame 1092/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 12.3ms\n",
            "video 1/1 (frame 1093/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 11.2ms\n",
            "video 1/1 (frame 1094/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.9ms\n",
            "video 1/1 (frame 1095/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.9ms\n",
            "video 1/1 (frame 1096/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.1ms\n",
            "video 1/1 (frame 1097/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.4ms\n",
            "video 1/1 (frame 1098/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.6ms\n",
            "video 1/1 (frame 1099/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 8.0ms\n",
            "video 1/1 (frame 1100/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.5ms\n",
            "video 1/1 (frame 1101/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.0ms\n",
            "video 1/1 (frame 1102/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.4ms\n",
            "video 1/1 (frame 1103/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.1ms\n",
            "video 1/1 (frame 1104/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.0ms\n",
            "video 1/1 (frame 1105/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.3ms\n",
            "video 1/1 (frame 1106/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 11.8ms\n",
            "video 1/1 (frame 1107/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 11.9ms\n",
            "video 1/1 (frame 1108/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.6ms\n",
            "video 1/1 (frame 1109/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.0ms\n",
            "video 1/1 (frame 1110/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.5ms\n",
            "video 1/1 (frame 1111/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.6ms\n",
            "video 1/1 (frame 1112/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.3ms\n",
            "video 1/1 (frame 1113/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.7ms\n",
            "video 1/1 (frame 1114/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 9.4ms\n",
            "video 1/1 (frame 1115/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.9ms\n",
            "video 1/1 (frame 1116/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.6ms\n",
            "video 1/1 (frame 1117/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.9ms\n",
            "video 1/1 (frame 1118/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 13.7ms\n",
            "video 1/1 (frame 1119/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.7ms\n",
            "video 1/1 (frame 1120/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 10.2ms\n",
            "video 1/1 (frame 1121/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.5ms\n",
            "video 1/1 (frame 1122/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 11.0ms\n",
            "video 1/1 (frame 1123/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.4ms\n",
            "video 1/1 (frame 1124/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.2ms\n",
            "video 1/1 (frame 1125/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.9ms\n",
            "video 1/1 (frame 1126/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.6ms\n",
            "video 1/1 (frame 1127/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.9ms\n",
            "video 1/1 (frame 1128/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.4ms\n",
            "video 1/1 (frame 1129/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.9ms\n",
            "video 1/1 (frame 1130/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 11.1ms\n",
            "video 1/1 (frame 1131/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.6ms\n",
            "video 1/1 (frame 1132/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 11.2ms\n",
            "video 1/1 (frame 1133/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.8ms\n",
            "video 1/1 (frame 1134/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 11.1ms\n",
            "video 1/1 (frame 1135/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 11.1ms\n",
            "video 1/1 (frame 1136/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.7ms\n",
            "video 1/1 (frame 1137/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.6ms\n",
            "video 1/1 (frame 1138/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.1ms\n",
            "video 1/1 (frame 1139/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.7ms\n",
            "video 1/1 (frame 1140/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 13.9ms\n",
            "video 1/1 (frame 1141/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 14.4ms\n",
            "video 1/1 (frame 1142/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 13.7ms\n",
            "video 1/1 (frame 1143/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 12.3ms\n",
            "video 1/1 (frame 1144/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.9ms\n",
            "video 1/1 (frame 1145/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 15.3ms\n",
            "video 1/1 (frame 1146/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 9.9ms\n",
            "video 1/1 (frame 1147/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 10.3ms\n",
            "video 1/1 (frame 1148/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.4ms\n",
            "video 1/1 (frame 1149/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.2ms\n",
            "video 1/1 (frame 1150/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.9ms\n",
            "video 1/1 (frame 1151/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 8.8ms\n",
            "video 1/1 (frame 1152/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.9ms\n",
            "video 1/1 (frame 1153/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.2ms\n",
            "video 1/1 (frame 1154/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 6.9ms\n",
            "video 1/1 (frame 1155/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 6.8ms\n",
            "video 1/1 (frame 1156/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 7.3ms\n",
            "video 1/1 (frame 1157/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.9ms\n",
            "video 1/1 (frame 1158/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 7.7ms\n",
            "video 1/1 (frame 1159/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 8.4ms\n",
            "video 1/1 (frame 1160/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 9.7ms\n",
            "video 1/1 (frame 1161/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.1ms\n",
            "video 1/1 (frame 1162/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.7ms\n",
            "video 1/1 (frame 1163/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.1ms\n",
            "video 1/1 (frame 1164/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.1ms\n",
            "video 1/1 (frame 1165/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.1ms\n",
            "video 1/1 (frame 1166/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.0ms\n",
            "video 1/1 (frame 1167/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.2ms\n",
            "video 1/1 (frame 1168/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.9ms\n",
            "video 1/1 (frame 1169/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.0ms\n",
            "video 1/1 (frame 1170/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.9ms\n",
            "video 1/1 (frame 1171/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.0ms\n",
            "video 1/1 (frame 1172/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 7.0ms\n",
            "video 1/1 (frame 1173/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 6.7ms\n",
            "video 1/1 (frame 1174/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 8.6ms\n",
            "video 1/1 (frame 1175/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 6.9ms\n",
            "video 1/1 (frame 1176/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 6.7ms\n",
            "video 1/1 (frame 1177/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.7ms\n",
            "video 1/1 (frame 1178/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.7ms\n",
            "video 1/1 (frame 1179/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.1ms\n",
            "video 1/1 (frame 1180/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.7ms\n",
            "video 1/1 (frame 1181/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.6ms\n",
            "video 1/1 (frame 1182/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.6ms\n",
            "video 1/1 (frame 1183/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.7ms\n",
            "video 1/1 (frame 1184/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.9ms\n",
            "video 1/1 (frame 1185/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.6ms\n",
            "video 1/1 (frame 1186/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.6ms\n",
            "video 1/1 (frame 1187/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.1ms\n",
            "video 1/1 (frame 1188/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.3ms\n",
            "video 1/1 (frame 1189/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.9ms\n",
            "video 1/1 (frame 1190/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.6ms\n",
            "video 1/1 (frame 1191/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 1192/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 1193/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 1194/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 1195/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.2ms\n",
            "video 1/1 (frame 1196/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 1197/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 1198/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 1199/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.6ms\n",
            "video 1/1 (frame 1200/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.6ms\n",
            "video 1/1 (frame 1201/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 10.6ms\n",
            "video 1/1 (frame 1202/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.4ms\n",
            "video 1/1 (frame 1203/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.9ms\n",
            "video 1/1 (frame 1204/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.5ms\n",
            "video 1/1 (frame 1205/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 1206/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 1207/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 1208/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.5ms\n",
            "video 1/1 (frame 1209/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 1210/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 7.3ms\n",
            "video 1/1 (frame 1211/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 7.5ms\n",
            "video 1/1 (frame 1212/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.8ms\n",
            "video 1/1 (frame 1213/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 1214/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 6.5ms\n",
            "video 1/1 (frame 1215/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 8.0ms\n",
            "video 1/1 (frame 1216/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 2 balons, 6.5ms\n",
            "video 1/1 (frame 1217/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 1218/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 1219/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.1ms\n",
            "video 1/1 (frame 1220/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.2ms\n",
            "video 1/1 (frame 1221/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.5ms\n",
            "video 1/1 (frame 1222/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.6ms\n",
            "video 1/1 (frame 1223/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.7ms\n",
            "video 1/1 (frame 1224/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.7ms\n",
            "video 1/1 (frame 1225/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 1226/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.4ms\n",
            "video 1/1 (frame 1227/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.0ms\n",
            "video 1/1 (frame 1228/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.4ms\n",
            "video 1/1 (frame 1229/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.4ms\n",
            "video 1/1 (frame 1230/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.4ms\n",
            "video 1/1 (frame 1231/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.7ms\n",
            "video 1/1 (frame 1232/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.3ms\n",
            "video 1/1 (frame 1233/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.4ms\n",
            "video 1/1 (frame 1234/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.3ms\n",
            "video 1/1 (frame 1235/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.7ms\n",
            "video 1/1 (frame 1236/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.9ms\n",
            "video 1/1 (frame 1237/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.3ms\n",
            "video 1/1 (frame 1238/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.3ms\n",
            "video 1/1 (frame 1239/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 7.5ms\n",
            "video 1/1 (frame 1240/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 (no detections), 6.3ms\n",
            "video 1/1 (frame 1241/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.3ms\n",
            "video 1/1 (frame 1242/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.3ms\n",
            "video 1/1 (frame 1243/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.1ms\n",
            "video 1/1 (frame 1244/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.4ms\n",
            "video 1/1 (frame 1245/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.4ms\n",
            "video 1/1 (frame 1246/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.3ms\n",
            "video 1/1 (frame 1247/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.6ms\n",
            "video 1/1 (frame 1248/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.8ms\n",
            "video 1/1 (frame 1249/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.4ms\n",
            "video 1/1 (frame 1250/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 9.6ms\n",
            "video 1/1 (frame 1251/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.2ms\n",
            "video 1/1 (frame 1252/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.6ms\n",
            "video 1/1 (frame 1253/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.3ms\n",
            "video 1/1 (frame 1254/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.3ms\n",
            "video 1/1 (frame 1255/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.4ms\n",
            "video 1/1 (frame 1256/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.7ms\n",
            "video 1/1 (frame 1257/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.3ms\n",
            "video 1/1 (frame 1258/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.3ms\n",
            "video 1/1 (frame 1259/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.6ms\n",
            "video 1/1 (frame 1260/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.1ms\n",
            "video 1/1 (frame 1261/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.3ms\n",
            "video 1/1 (frame 1262/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.3ms\n",
            "video 1/1 (frame 1263/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.9ms\n",
            "video 1/1 (frame 1264/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.7ms\n",
            "video 1/1 (frame 1265/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 11.9ms\n",
            "video 1/1 (frame 1266/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 8.1ms\n",
            "video 1/1 (frame 1267/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.5ms\n",
            "video 1/1 (frame 1268/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 7.1ms\n",
            "video 1/1 (frame 1269/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.3ms\n",
            "video 1/1 (frame 1270/1270) /content/drive/My Drive/YoloV8/testing/videoprueba/9-converted.mp4: 448x640 1 balon, 6.3ms\n",
            "Speed: 2.7ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1m/content/drive/My Drive/YoloV8/testing/videoresultado/video_results\u001b[0m\n",
            "\n",
            "Predicción en video finalizada.\n",
            "Los resultados (video con detecciones) deberían estar en: /content/drive/My Drive/YoloV8/testing/videoresultado/video_results \n",
            "Busca un archivo .mp4 o similar en esa carpeta.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformar .pt a .Tflite**"
      ],
      "metadata": {
        "id": "jfvHhy4fJafT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from ultralytics import YOLO\n",
        "import tensorflow as tf # Asegúrate de que TensorFlow esté instalado\n",
        "\n",
        "# !!! IMPORTANTE: Define la ruta COMPLETA a tu archivo best.pt en Google Drive !!!\n",
        "# Reemplaza esto con la ruta real donde se guardó tu best.pt después del entrenamiento.\n",
        "best_model_path_pt = '/content/drive/My Drive/YoloV8/testing/data/weights/best.pt' # <-- ASEGÚRATE DE QUE ESTA RUTA SEA CORRECTA\n",
        "\n",
        "# Define la ruta donde quieres guardar el archivo .tflite convertido en Google Drive\n",
        "# Se recomienda guardarlo en la misma carpeta o en una subcarpeta de resultados.\n",
        "output_tflite_path = '/content/drive/My Drive/YoloV8/testing/data/weights/' # <-- ASEGÚRATE DE QUE ESTA RUTA EXISTA\n",
        "\n",
        "# Define el nombre del archivo de salida .tflite\n",
        "output_tflite_filename = 'best.tflite'\n",
        "\n",
        "# Combina la ruta de salida y el nombre del archivo\n",
        "full_output_tflite_path = os.path.join(output_tflite_path, output_tflite_filename)\n",
        "\n",
        "\n",
        "# Cargar el modelo entrenado desde el archivo .pt\n",
        "try:\n",
        "    model = YOLO(best_model_path_pt)\n",
        "    print(f\"Modelo cargado exitosamente desde: {best_model_path_pt}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar el modelo .pt: {e}\")\n",
        "    print(f\"Por favor, verifica que la ruta '{best_model_path_pt}' sea correcta y que el archivo best.pt exista.\")\n",
        "    # Puedes salir del script si el modelo no se puede cargar\n",
        "    # exit()\n",
        "\n",
        "# Realizar la exportación al formato TensorFlow Lite (.tflite)\n",
        "# El argumento 'format' especifica el formato de destino.\n",
        "# El archivo .tflite se guardará automáticamente en la carpeta de resultados del modelo original,\n",
        "# o puedes especificar una ruta con el argumento 'save_dir'.\n",
        "# Aquí usaremos el argumento save_dir para guardarlo donde definimos.\n",
        "try:\n",
        "    print(f\"Iniciando exportación a TFLite para {best_model_path_pt}...\")\n",
        "\n",
        "    # La exportación a TFLite puede requerir un tamaño de imagen específico.\n",
        "    # Usaremos el tamaño de imagen que usaste para entrenar (640x640 por defecto en tu caso).\n",
        "    exported_model = model.export(format='tflite', imgsz=640, save_dir=output_tflite_path)\n",
        "\n",
        "    print(\"Exportación a TFLite finalizada.\")\n",
        "    print(f\"El modelo TFLite se guardó en: {exported_model}\") # exported_model contiene la ruta completa al archivo guardado\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error durante la exportación a TFLite: {e}\")\n",
        "    print(\"Asegúrate de que TensorFlow esté instalado (`pip install tensorflow`) y de que no haya problemas de compatibilidad.\")\n",
        "\n",
        "\n",
        "print(\"\\nProceso de conversión a TFLite completado.\")\n",
        "print(f\"Busca el archivo '{output_tflite_filename}' en la carpeta '{output_tflite_path}' en tu Google Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oElXXsyJnlJ",
        "outputId": "97f131bb-6770-4d64-c8c1-ab4fdf3753db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Modelo cargado exitosamente desde: /content/drive/My Drive/YoloV8/testing/data/weights/best.pt\n",
            "Iniciando exportación a TFLite para /content/drive/My Drive/YoloV8/testing/data/weights/best.pt...\n",
            "Ultralytics 8.3.199 🚀 Python-3.12.11 torch-2.8.0+cu126 CPU (Intel Xeon CPU @ 2.00GHz)\n",
            "💡 ProTip: Export to OpenVINO format for best performance on Intel hardware. Learn more at https://docs.ultralytics.com/integrations/openvino/\n",
            "Model summary (fused): 72 layers, 11,126,745 parameters, 0 gradients, 28.4 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/content/drive/My Drive/YoloV8/testing/data/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 7, 8400) (21.5 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['sng4onnx>=1.0.1', 'onnx_graphsurgeon>=0.3.26', 'ai-edge-litert>=1.2.0,<1.4.0', 'onnx>=1.12.0', 'onnx2tf>=1.26.3', 'onnxslim>=0.1.67', 'onnxruntime'] not found, attempting AutoUpdate...\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 10.9s\n",
            "WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.19.0...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/calibration_image_sample_data_20x128x128x3_float32.npy.zip to 'calibration_image_sample_data_20x128x128x3_float32.npy.zip': 100% ━━━━━━━━━━━━ 1.1MB 83.6MB/s 0.0s\n",
            "\u001b[KUnzipping calibration_image_sample_data_20x128x128x3_float32.npy.zip to /content/calibration_image_sample_data_20x128x128x3_float32.npy...: 100% ━━━━━━━━━━━━ 1/1 38.6files/s 0.0s\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.19.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.68...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 4.9s, saved as '/content/drive/My Drive/YoloV8/testing/data/weights/best.onnx' (42.8 MB)\n",
            "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting TFLite export with onnx2tf 1.28.2...\n",
            "Saved artifact at '/content/drive/My Drive/YoloV8/testing/data/weights/best_saved_model'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serving_default'\n",
            "  inputs_0 (POSITIONAL_ONLY): TensorSpec(shape=(1, 640, 640, 3), dtype=tf.float32, name='images')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(1, 7, 8400), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  136089282097616: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  136089282098192: TensorSpec(shape=(3, 3, 3, 32), dtype=tf.float32, name=None)\n",
            "  136089282098384: TensorSpec(shape=(32,), dtype=tf.float32, name=None)\n",
            "  136089273698064: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  136089282099920: TensorSpec(shape=(3, 3, 32, 64), dtype=tf.float32, name=None)\n",
            "  136089273697104: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136089273698256: TensorSpec(shape=(1, 1, 64, 64), dtype=tf.float32, name=None)\n",
            "  136089273698448: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136089273699216: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089273698640: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089282098576: TensorSpec(shape=(3, 3, 32, 32), dtype=tf.float32, name=None)\n",
            "  136089273701136: TensorSpec(shape=(32,), dtype=tf.float32, name=None)\n",
            "  136089273701712: TensorSpec(shape=(3, 3, 32, 32), dtype=tf.float32, name=None)\n",
            "  136089273700752: TensorSpec(shape=(32,), dtype=tf.float32, name=None)\n",
            "  136089273699792: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089273699408: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089273701904: TensorSpec(shape=(1, 1, 96, 64), dtype=tf.float32, name=None)\n",
            "  136089273702480: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136089273699600: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  136089273702864: TensorSpec(shape=(3, 3, 64, 128), dtype=tf.float32, name=None)\n",
            "  136089273700560: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  136089273703056: TensorSpec(shape=(1, 1, 128, 128), dtype=tf.float32, name=None)\n",
            "  136089273702672: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  136089273704016: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089273703248: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089273704400: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  136089273705360: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136089273701328: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  136089273702096: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136089273704592: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  136089273705552: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136089273702288: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  136089273705936: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136089273703440: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089273703824: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089273706320: TensorSpec(shape=(1, 1, 256, 128), dtype=tf.float32, name=None)\n",
            "  136089273705744: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  136089273706512: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  136089273704784: TensorSpec(shape=(3, 3, 128, 256), dtype=tf.float32, name=None)\n",
            "  136089273706128: TensorSpec(shape=(256,), dtype=tf.float32, name=None)\n",
            "  136089273706896: TensorSpec(shape=(1, 1, 256, 256), dtype=tf.float32, name=None)\n",
            "  136089273707088: TensorSpec(shape=(256,), dtype=tf.float32, name=None)\n",
            "  136089273707664: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089273707472: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089273709584: TensorSpec(shape=(3, 3, 128, 128), dtype=tf.float32, name=None)\n",
            "  136089273709776: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  136089273707280: TensorSpec(shape=(3, 3, 128, 128), dtype=tf.float32, name=None)\n",
            "  136089273706704: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  136089273709008: TensorSpec(shape=(3, 3, 128, 128), dtype=tf.float32, name=None)\n",
            "  136089273709968: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  136089273708624: TensorSpec(shape=(3, 3, 128, 128), dtype=tf.float32, name=None)\n",
            "  136089273710352: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  136089273707856: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089273708048: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089273710736: TensorSpec(shape=(1, 1, 512, 256), dtype=tf.float32, name=None)\n",
            "  136089273710160: TensorSpec(shape=(256,), dtype=tf.float32, name=None)\n",
            "  136089273710928: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  136089273709200: TensorSpec(shape=(3, 3, 256, 512), dtype=tf.float32, name=None)\n",
            "  136089273710544: TensorSpec(shape=(512,), dtype=tf.float32, name=None)\n",
            "  136089273711312: TensorSpec(shape=(1, 1, 512, 512), dtype=tf.float32, name=None)\n",
            "  136089273711504: TensorSpec(shape=(512,), dtype=tf.float32, name=None)\n",
            "  136089263194576: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089263194960: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089273711888: TensorSpec(shape=(3, 3, 256, 256), dtype=tf.float32, name=None)\n",
            "  136089273711120: TensorSpec(shape=(256,), dtype=tf.float32, name=None)\n",
            "  136089273712080: TensorSpec(shape=(3, 3, 256, 256), dtype=tf.float32, name=None)\n",
            "  136089273711696: TensorSpec(shape=(256,), dtype=tf.float32, name=None)\n",
            "  136089273712272: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089273712464: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089263195920: TensorSpec(shape=(1, 1, 768, 512), dtype=tf.float32, name=None)\n",
            "  136089263195728: TensorSpec(shape=(512,), dtype=tf.float32, name=None)\n",
            "  136089263195536: TensorSpec(shape=(1, 1, 512, 256), dtype=tf.float32, name=None)\n",
            "  136089263196304: TensorSpec(shape=(256,), dtype=tf.float32, name=None)\n",
            "  136089263195152: TensorSpec(shape=(1, 1, 1024, 512), dtype=tf.float32, name=None)\n",
            "  136089263196112: TensorSpec(shape=(512,), dtype=tf.float32, name=None)\n",
            "  136089263197072: TensorSpec(shape=(1, 1, 768, 256), dtype=tf.float32, name=None)\n",
            "  136089263197264: TensorSpec(shape=(256,), dtype=tf.float32, name=None)\n",
            "  136089263197648: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089263196880: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089263198992: TensorSpec(shape=(3, 3, 128, 128), dtype=tf.float32, name=None)\n",
            "  136089263199184: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  136089263197456: TensorSpec(shape=(3, 3, 128, 128), dtype=tf.float32, name=None)\n",
            "  136089263196496: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  136089263198416: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089263196688: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089263199952: TensorSpec(shape=(1, 1, 384, 256), dtype=tf.float32, name=None)\n",
            "  136089263199760: TensorSpec(shape=(256,), dtype=tf.float32, name=None)\n",
            "  136089263198608: TensorSpec(shape=(1, 1, 384, 128), dtype=tf.float32, name=None)\n",
            "  136089263198800: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  136089263200528: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089263200336: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089263202448: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  136089263202640: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136089263200144: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  136089263198224: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136089263200720: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089263200912: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089263203024: TensorSpec(shape=(1, 1, 192, 128), dtype=tf.float32, name=None)\n",
            "  136089263202832: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  136089263203216: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  136089263201872: TensorSpec(shape=(3, 3, 128, 128), dtype=tf.float32, name=None)\n",
            "  136089263201488: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  136089263205136: TensorSpec(shape=(1, 1, 384, 256), dtype=tf.float32, name=None)\n",
            "  136089263204560: TensorSpec(shape=(256,), dtype=tf.float32, name=None)\n",
            "  136089263206288: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089263206096: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089263208784: TensorSpec(shape=(3, 3, 128, 128), dtype=tf.float32, name=None)\n",
            "  136089263208976: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  136089263206480: TensorSpec(shape=(3, 3, 128, 128), dtype=tf.float32, name=None)\n",
            "  136089263209360: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  136089263206672: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089263206864: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089263208400: TensorSpec(shape=(1, 1, 384, 256), dtype=tf.float32, name=None)\n",
            "  136089263207824: TensorSpec(shape=(256,), dtype=tf.float32, name=None)\n",
            "  136089263209552: TensorSpec(shape=(4, 2), dtype=tf.int32, name=None)\n",
            "  136089263205712: TensorSpec(shape=(3, 3, 256, 256), dtype=tf.float32, name=None)\n",
            "  136089263207440: TensorSpec(shape=(256,), dtype=tf.float32, name=None)\n",
            "  136089282090512: TensorSpec(shape=(1, 1, 768, 512), dtype=tf.float32, name=None)\n",
            "  136089282099152: TensorSpec(shape=(512,), dtype=tf.float32, name=None)\n",
            "  136089282092624: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089319064016: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089479367376: TensorSpec(shape=(3, 3, 256, 256), dtype=tf.float32, name=None)\n",
            "  136089282091280: TensorSpec(shape=(256,), dtype=tf.float32, name=None)\n",
            "  136089282093008: TensorSpec(shape=(3, 3, 256, 256), dtype=tf.float32, name=None)\n",
            "  136089282091472: TensorSpec(shape=(256,), dtype=tf.float32, name=None)\n",
            "  136089282090128: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089282089552: TensorSpec(shape=(4,), dtype=tf.int64, name=None)\n",
            "  136089319063824: TensorSpec(shape=(1, 1, 768, 512), dtype=tf.float32, name=None)\n",
            "  136089319063632: TensorSpec(shape=(512,), dtype=tf.float32, name=None)\n",
            "  136089320060880: TensorSpec(shape=(3, 3, 512, 128), dtype=tf.float32, name=None)\n",
            "  136089282091856: TensorSpec(shape=(3, 3, 512, 64), dtype=tf.float32, name=None)\n",
            "  136089263209936: TensorSpec(shape=(3, 3, 256, 128), dtype=tf.float32, name=None)\n",
            "  136089263208592: TensorSpec(shape=(3, 3, 256, 64), dtype=tf.float32, name=None)\n",
            "  136089263203984: TensorSpec(shape=(3, 3, 128, 128), dtype=tf.float32, name=None)\n",
            "  136089263203600: TensorSpec(shape=(3, 3, 128, 64), dtype=tf.float32, name=None)\n",
            "  136089320061840: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  136089320060112: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136089263209744: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  136089263210320: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136089263202064: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  136089263203408: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136089263209168: TensorSpec(shape=(3, 3, 128, 128), dtype=tf.float32, name=None)\n",
            "  136089263210128: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  136089282096656: TensorSpec(shape=(3, 3, 128, 128), dtype=tf.float32, name=None)\n",
            "  136089290621392: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  136089263204752: TensorSpec(shape=(3, 3, 128, 128), dtype=tf.float32, name=None)\n",
            "  136089263203792: TensorSpec(shape=(3, 3, 64, 64), dtype=tf.float32, name=None)\n",
            "  136089294274640: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  136089263208016: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136089282087824: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  136089282099728: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136089263204368: TensorSpec(shape=(128,), dtype=tf.float32, name=None)\n",
            "  136089263204176: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136089294275600: TensorSpec(shape=(1, 1, 128, 3), dtype=tf.float32, name=None)\n",
            "  136089294275024: TensorSpec(shape=(1, 1, 64, 64), dtype=tf.float32, name=None)\n",
            "  136089282093200: TensorSpec(shape=(1, 1, 128, 3), dtype=tf.float32, name=None)\n",
            "  136089282097808: TensorSpec(shape=(1, 1, 64, 64), dtype=tf.float32, name=None)\n",
            "  136089263205904: TensorSpec(shape=(1, 1, 128, 3), dtype=tf.float32, name=None)\n",
            "  136089263205520: TensorSpec(shape=(1, 1, 64, 64), dtype=tf.float32, name=None)\n",
            "  136089294275408: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136089294275216: TensorSpec(shape=(3,), dtype=tf.float32, name=None)\n",
            "  136089282090320: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136089282092240: TensorSpec(shape=(3,), dtype=tf.float32, name=None)\n",
            "  136089263204944: TensorSpec(shape=(64,), dtype=tf.float32, name=None)\n",
            "  136089263205328: TensorSpec(shape=(3,), dtype=tf.float32, name=None)\n",
            "  136089294277136: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  136089294275792: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  136089294278672: TensorSpec(shape=(1, 1, 16, 1), dtype=tf.float32, name=None)\n",
            "  136089294277328: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  136089294279248: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  136089294279056: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  136089294279632: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  136089294279824: TensorSpec(shape=(1, 2, 8400), dtype=tf.float32, name=None)\n",
            "  136089294277712: TensorSpec(shape=(1, 2, 8400), dtype=tf.float32, name=None)\n",
            "  136089294276368: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "  136089294276560: TensorSpec(shape=(3,), dtype=tf.int64, name=None)\n",
            "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success ✅ 36.3s, saved as '('/content/drive/My Drive/YoloV8/testing/data/weights/best_saved_model', <tf_keras.src.engine.functional.Functional object at 0x7bc5c0098260>)' (0.0 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m starting export with tensorflow 2.19.0...\n",
            "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m export success ✅ 0.0s, saved as '/content/drive/My Drive/YoloV8/testing/data/weights/best_saved_model/best_float32.tflite' (42.7 MB)\n",
            "\n",
            "Export complete (37.6s)\n",
            "Results saved to \u001b[1m/content/drive/My Drive/YoloV8/testing/data/weights\u001b[0m\n",
            "Predict:         yolo predict task=detect model=/content/drive/My Drive/YoloV8/testing/data/weights/best_saved_model/best_float32.tflite imgsz=640  \n",
            "Validate:        yolo val task=detect model=/content/drive/My Drive/YoloV8/testing/data/weights/best_saved_model/best_float32.tflite imgsz=640 data=/content/drive/My Drive/YoloV8/dataset.yaml  \n",
            "Visualize:       https://netron.app\n",
            "Exportación a TFLite finalizada.\n",
            "El modelo TFLite se guardó en: /content/drive/My Drive/YoloV8/testing/data/weights/best_saved_model/best_float32.tflite\n",
            "\n",
            "Proceso de conversión a TFLite completado.\n",
            "Busca el archivo 'best.tflite' en la carpeta '/content/drive/My Drive/YoloV8/testing/data/weights/' en tu Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91c6558d"
      },
      "source": [
        "# Task\n",
        "Ayúdame a implementar un sistema en una Raspberry Pi 4 de 4GB para detectar una pelota y una línea de meta usando un modelo YOLOv8 entrenado (archivo .pt) con una cámara (tipo HT-SUA33GC-T1V-C), y activar una acción cuando la pelota cruce la línea de meta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af574b8d"
      },
      "source": [
        "## Transferir el modelo entrenado a la raspberry pi\n",
        "\n",
        "### Subtask:\n",
        "Copia el archivo `best.pt` (el resultado de tu entrenamiento en Google Colab u otro entorno) a tu Raspberry Pi.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47868102"
      },
      "source": [
        "## Configurar el entorno en la raspberry pi\n",
        "\n",
        "### Subtask:\n",
        "Instala las librerías necesarias en la Raspberry Pi para ejecutar YOLOv8 (principalmente `ultralytics` y sus dependencias, incluyendo una versión compatible de PyTorch o TensorFlow Lite).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92c4c994"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires installing necessary libraries on a Raspberry Pi. Since I cannot directly interact with a Raspberry Pi's terminal to execute system commands or install packages, I will provide the shell commands that the user needs to execute on their Raspberry Pi. I will include the steps for updating the system, installing pip, installing ultralytics, and suggest the user install PyTorch or TensorFlow Lite based on their preference and the Raspberry Pi's capabilities, as outlined in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d1c9ad9"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because I tried to execute shell commands directly in a Python code block. I need to present these commands as instructions for the user to run in their Raspberry Pi's terminal instead of trying to run them as Python code. I will provide the same instructions in a markdown block for clarity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNcrRoG4YmAk"
      },
      "source": [
        "# Instructions for installing libraries on your Raspberry Pi terminal:\n",
        "\n",
        "# 1. Access the terminal on your Raspberry Pi.\n",
        "\n",
        "# 2. Update your system's package list and upgrade installed packages:\n",
        "# sudo apt update\n",
        "# sudo apt upgrade -y\n",
        "\n",
        "# 3. Install pip for Python 3 if it's not already installed:\n",
        "# sudo apt install python3-pip -y\n",
        "\n",
        "# 4. Install the ultralytics library using pip:\n",
        "# pip install ultralytics\n",
        "\n",
        "# 5. Install a compatible version of PyTorch or TensorFlow Lite.\n",
        "# This step is highly dependent on your specific Raspberry Pi model,\n",
        "# the operating system version, and the Python version.\n",
        "\n",
        "# Option 1: Attempt to install PyTorch (might require specific wheels or compiling)\n",
        "# Search for pre-compiled wheels for PyTorch on Raspberry Pi (ARM architecture)\n",
        "# Example (replace <version> and <python_version> with appropriate values):\n",
        "# pip install torch==<version>+cpu torchvision==<version>+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# or look for community-built wheels.\n",
        "\n",
        "# Option 2: Install TensorFlow Lite (often easier on Raspberry Pi)\n",
        "# Follow the official TensorFlow Lite installation guide for Raspberry Pi:\n",
        "# https://www.tensorflow.org/lite/guide/python#install_tensorflow_lite_for_python_on_raspberry_pi\n",
        "# This usually involves:\n",
        "# pip install tflite-runtime\n",
        "\n",
        "# 6. Verify the installations by trying to import the libraries in a Python 3 session on your Raspberry Pi:\n",
        "# python3\n",
        "# >>> import ultralytics\n",
        "# >>> import torch # if you installed PyTorch\n",
        "# >>> import tensorflow.lite as tflite # if you installed TensorFlow Lite\n",
        "# >>> exit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91fd2349"
      },
      "source": [
        "## Conectar la cámara a la raspberry pi\n",
        "\n",
        "### Subtask:\n",
        "Asegúrate de que la cámara (tipo HT-SUA33GC-T1V-C) esté correctamente conectada y configurada en tu Raspberry Pi para que puedas acceder a su flujo de video.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edd52b29"
      },
      "source": [
        "**Reasoning**:\n",
        "The current subtask requires checking the camera connection and configuration on the Raspberry Pi. Since I cannot directly interact with the Raspberry Pi's hardware or terminal, the only way to accomplish this subtask is by providing instructions for the user to follow on their own device. I will provide instructions for checking camera detection and testing it with fswebcam.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e95fdd24"
      },
      "source": [
        "# Instructions to be executed on your Raspberry Pi terminal:\n",
        "\n",
        "# Step 1: Verify camera detection\n",
        "# Open a terminal on your Raspberry Pi and run the following command to list video devices:\n",
        "# ls /dev/video*\n",
        "\n",
        "# Expected Output: You should see output similar to /dev/video0, /dev/video1, etc.\n",
        "# If you don't see any /dev/video* devices, try the following:\n",
        "# - Ensure the camera is securely connected to a USB port.\n",
        "# - Reboot your Raspberry Pi with the camera connected: sudo reboot\n",
        "# - Consult the camera's documentation (HT-SUA33GC-T1V-C) for specific driver installation or configuration steps if needed.\n",
        "\n",
        "# Step 2: Test the camera using fswebcam (optional but recommended)\n",
        "# If the camera is detected, you can test it by capturing an image.\n",
        "# First, install fswebcam if you don't have it:\n",
        "# sudo apt update\n",
        "# sudo apt install fswebcam -y\n",
        "\n",
        "# Then, capture a test image:\n",
        "# fswebcam test_image.jpg\n",
        "\n",
        "# Expected Output: A file named 'test_image.jpg' should be created in the directory where you ran the command.\n",
        "# You can view this image to confirm the camera is working.\n",
        "\n",
        "# If you encounter issues with fswebcam, you can also try testing with OpenCV in Python\n",
        "# (assuming you have OpenCV installed, which might be required for the next steps of the project).\n",
        "# Example Python code (run in a Python environment on your Pi):\n",
        "# import cv2\n",
        "# cap = cv2.VideoCapture(0) # 0 is typically the default camera\n",
        "# if not cap.isOpened():\n",
        "#     print(\"Error: Could not open camera.\")\n",
        "# else:\n",
        "#     ret, frame = cap.read()\n",
        "#     if ret:\n",
        "#         cv2.imwrite(\"test_opencv_image.jpg\", frame)\n",
        "#         print(\"Captured test_opencv_image.jpg\")\n",
        "#     else:\n",
        "#         print(\"Error: Could not read frame from camera.\")\n",
        "# cap.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f3ecf68"
      },
      "source": [
        "## Escribir código para la captura de video\n",
        "\n",
        "### Subtask:\n",
        "Desarrolla un script de Python en la Raspberry Pi para capturar frames de video desde la cámara.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "758b0e13"
      },
      "source": [
        "**Reasoning**:\n",
        "Develop the Python script to capture video frames from the camera using OpenCV based on the provided instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a76db641",
        "outputId": "2bef3d21-6133-49ea-c589-b40eecf2e148"
      },
      "source": [
        "import cv2\n",
        "import time\n",
        "\n",
        "# Camera index (usually 0 for the default camera)\n",
        "# If you have multiple cameras, you might need to change this.\n",
        "# Use 'ls /dev/video*' in your Raspberry Pi terminal to list available video devices.\n",
        "camera_index = 0\n",
        "\n",
        "# Initialize video capture\n",
        "cap = cv2.VideoCapture(camera_index)\n",
        "\n",
        "# Check if the camera opened successfully\n",
        "if not cap.isOpened():\n",
        "    print(f\"Error: Could not open camera with index {camera_index}.\")\n",
        "    print(\"Please check if the camera is connected and the index is correct.\")\n",
        "else:\n",
        "    print(f\"Camera with index {camera_index} opened successfully.\")\n",
        "\n",
        "    # Loop to continuously read frames\n",
        "    while True:\n",
        "        # Read a frame from the camera\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # If the frame was not read successfully, break the loop\n",
        "        if not ret:\n",
        "            print(\"Error: Could not read frame.\")\n",
        "            break\n",
        "\n",
        "        # --- Optional: Display the frame (can be slow on RPi without optimization) ---\n",
        "        # cv2.imshow('Camera Feed', frame)\n",
        "\n",
        "        # --- Add a small delay to reduce CPU usage (optional) ---\n",
        "        # time.sleep(0.01) # Adjust delay as needed\n",
        "\n",
        "        # Break the loop if the 'q' key is pressed\n",
        "        # This part will only work if cv2.imshow is enabled and a window is active\n",
        "        # If not displaying, you might need a different mechanism to stop the script (e.g., Ctrl+C)\n",
        "        # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        #     break\n",
        "\n",
        "        # For a headless environment or to just test capture without display,\n",
        "        # you can remove the cv2.imshow and waitKey lines and rely on Ctrl+C to stop.\n",
        "        # Or add a simple counter to capture a few frames and then exit.\n",
        "        # For this subtask, we'll assume it runs until manually interrupted (e.g., Ctrl+C).\n",
        "        # In a real application, you would process the 'frame' here.\n",
        "\n",
        "        # Example: Print frame shape to confirm capture is working\n",
        "        print(f\"Captured frame shape: {frame.shape}\")\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()\n",
        "\n",
        "    # Destroy any OpenCV windows (if cv2.imshow was used)\n",
        "    # cv2.destroyAllWindows()\n",
        "\n",
        "    print(\"Video capture stopped.\")\n",
        "\n",
        "# Instructions on how to run this script on your Raspberry Pi:\n",
        "# 1. Save the code as a Python file (e.g., capture_video.py).\n",
        "# 2. Open a terminal on your Raspberry Pi.\n",
        "# 3. Navigate to the directory where you saved the file.\n",
        "# 4. Run the script using: python3 capture_video.py\n",
        "# 5. If you enabled cv2.imshow, press 'q' to exit the loop.\n",
        "# 6. If you did not enable cv2.imshow, press Ctrl+C in the terminal to stop the script."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Could not open camera with index 0.\n",
            "Please check if the camera is connected and the index is correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9b047aa"
      },
      "source": [
        "## Integrar el modelo yolov8 para detección en tiempo real\n",
        "\n",
        "### Subtask:\n",
        "Modifica el script para cargar el modelo `.pt` y ejecutar la inferencia (detección de objetos) en cada frame capturado.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3219ac30"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the script from the previous step to include loading the YOLOv8 model and running inference on each captured frame, as requested by the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "6eae8edb",
        "outputId": "b8175978-db5c-459e-b88c-3ebde035e6af"
      },
      "source": [
        "import cv2\n",
        "import time\n",
        "from ultralytics import YOLO # Import the YOLO class\n",
        "\n",
        "# Camera index (usually 0 for the default camera)\n",
        "# If you have multiple cameras, you might need to change this.\n",
        "# Use 'ls /dev/video*' in your Raspberry Pi terminal to list available video devices.\n",
        "camera_index = 0\n",
        "\n",
        "# Define the path to your trained best.pt model file on your Raspberry Pi\n",
        "# !!! IMPORTANT: Make sure this path is correct on your Raspberry Pi !!!\n",
        "best_model_path = '/path/to/your/best.pt' # <--- CHANGE THIS TO YOUR ACTUAL PATH\n",
        "\n",
        "# Load the YOLOv8 model\n",
        "try:\n",
        "    model = YOLO(best_model_path)\n",
        "    print(f\"Modelo YOLOv8 cargado exitosamente desde: {best_model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar el modelo: {e}\")\n",
        "    print(f\"Por favor, verifica que la ruta '{best_model_path}' sea correcta y que el archivo best.pt exista en tu Raspberry Pi.\")\n",
        "    # Exit the script if the model cannot be loaded\n",
        "    exit()\n",
        "\n",
        "\n",
        "# Initialize video capture\n",
        "cap = cv2.VideoCapture(camera_index)\n",
        "\n",
        "# Check if the camera opened successfully\n",
        "if not cap.isOpened():\n",
        "    print(f\"Error: Could not open camera with index {camera_index}.\")\n",
        "    print(\"Please check if the camera is connected and the index is correct.\")\n",
        "else:\n",
        "    print(f\"Camera with index {camera_index} opened successfully.\")\n",
        "\n",
        "    # Loop to continuously read frames\n",
        "    while True:\n",
        "        # Read a frame from the camera\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # If the frame was not read successfully, break the loop\n",
        "        if not ret:\n",
        "            print(\"Error: Could not read frame.\")\n",
        "            break\n",
        "\n",
        "        # --- Run YOLOv8 inference on the captured frame ---\n",
        "        # The 'results' object will contain detections (bounding boxes, classes, confidence scores)\n",
        "        results = model(frame, verbose=False) # Set verbose=False to reduce console output\n",
        "\n",
        "        # --- Store the results for further processing ---\n",
        "        # The 'results' object itself contains all the necessary information.\n",
        "        # You can access bounding boxes, classes, confidence scores, etc. like this:\n",
        "        # detections = results[0].boxes # Assuming batch size is 1\n",
        "\n",
        "        # For this subtask, we just store the results object.\n",
        "        # In the next steps, we will process 'results' to find the ball and the goal line.\n",
        "        print(f\"Inference performed on frame. Detected {len(results[0].boxes)} objects.\")\n",
        "\n",
        "\n",
        "        # --- Optional: Display the frame with detections (can be slow) ---\n",
        "        # annotated_frame = results[0].plot() # This draws bounding boxes and labels\n",
        "        # cv2.imshow('YOLOv8 Inference', annotated_frame)\n",
        "\n",
        "        # --- Add a small delay (optional) ---\n",
        "        # time.sleep(0.01)\n",
        "\n",
        "        # Break the loop if the 'q' key is pressed (requires a display window)\n",
        "        # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        #     break\n",
        "\n",
        "        # To stop in a headless environment, use Ctrl+C.\n",
        "        # Or implement a more robust shutdown mechanism if needed.\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()\n",
        "\n",
        "    # Destroy any OpenCV windows (if cv2.imshow was used)\n",
        "    # cv2.destroyAllWindows()\n",
        "\n",
        "    print(\"Video capture and inference stopped.\")\n",
        "\n",
        "# Instructions on how to run this script on your Raspberry Pi:\n",
        "# 1. Save the code as a Python file (e.g., detect_ball.py).\n",
        "# 2. Replace '/path/to/your/best.pt' with the actual path to your model file on your Pi.\n",
        "# 3. Open a terminal on your Raspberry Pi.\n",
        "# 4. Navigate to the directory where you saved the file.\n",
        "# 5. Run the script using: python3 detect_ball.py\n",
        "# 6. If you enabled cv2.imshow, press 'q' to exit the loop.\n",
        "# 7. If you did not enable cv2.imshow, press Ctrl+C in the terminal to stop the script."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'ultralytics'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3462303562.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0multralytics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mYOLO\u001b[0m \u001b[0;31m# Import the YOLO class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Camera index (usually 0 for the default camera)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ultralytics'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5dca7c8"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt failed because the `ultralytics` library is not installed in the current execution environment. Although the goal is to run this script on a Raspberry Pi where `ultralytics` is assumed to be installed from a previous step, the code execution environment attempts to run the code. To allow the code block to execute without error in this environment (and thus verify its syntax and structure before providing it to the user for the Raspberry Pi), I will install the missing library using `pip`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n00n62qEZYUR",
        "outputId": "d7e4a7f1-a25b-466a-9848-194314cd0441"
      },
      "source": [
        "%pip install ultralytics -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f8b2496"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that `ultralytics` is installed in the environment, I will re-run the code from the previous attempt that loads the model and integrates inference into the video capture loop. This will verify the code's syntax and structure before providing it as the final solution for the user to run on their Raspberry Pi.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f79f9e9"
      },
      "source": [
        "#import cv2\n",
        "#import time\n",
        "#from ultralytics import YOLO # Import the YOLO class\n",
        "\n",
        "# Camera index (usually 0 for the default camera)\n",
        "# If you have multiple cameras, you might need to change this.\n",
        "# Use 'ls /dev/video*' in your Raspberry Pi terminal to list available video devices.\n",
        "#camera_index = 0\n",
        "\n",
        "# Define the path to your trained best.pt model file on your Raspberry Pi\n",
        "# !!! IMPORTANT: Make sure this path is correct on your Raspberry Pi !!!\n",
        "#best_model_path = '/path/to/your/best.pt' # <--- CHANGE THIS TO YOUR ACTUAL PATH\n",
        "\n",
        "# Load the YOLOv8 model\n",
        "#try:\n",
        "    #model = YOLO(best_model_path)\n",
        "    #print(f\"Modelo YOLOv8 cargado exitosamente desde: {best_model_path}\")\n",
        "#except Exception as e:\n",
        "    #print(f\"Error al cargar el modelo: {e}\")\n",
        "    #print(f\"Por favor, verifica que la ruta '{best_model_path}' sea correcta y que el archivo best.pt exista en tu Raspberry Pi.\")\n",
        "    # Exit the script if the model cannot be loaded\n",
        "    #exit()\n",
        "\n",
        "\n",
        "# Initialize video capture\n",
        "#cap = cv2.VideoCapture(camera_index)\n",
        "\n",
        "# Check if the camera opened successfully\n",
        "#if not cap.isOpened():\n",
        "    #print(f\"Error: Could not open camera with index {camera_index}.\")\n",
        "    #print(\"Please check if the camera is connected and the index is correct.\")\n",
        "#else:\n",
        "    #print(f\"Camera with index {camera_index} opened successfully.\")\n",
        "\n",
        "    # Loop to continuously read frames\n",
        "    #while True:\n",
        "        # Read a frame from the camera\n",
        "        #ret, frame = cap.read()\n",
        "\n",
        "        # If the frame was not read successfully, break the loop\n",
        "        #if not ret:\n",
        "            #print(\"Error: Could not read frame.\")\n",
        "            #break\n",
        "\n",
        "        # --- Run YOLOv8 inference on the captured frame ---\n",
        "        # The 'results' object will contain detections (bounding boxes, classes, confidence scores)\n",
        "        #results = model(frame, verbose=False) # Set verbose=False to reduce console output\n",
        "\n",
        "        # --- Store the results for further processing ---\n",
        "        # The 'results' object itself contains all the necessary information.\n",
        "        # You can access bounding boxes, classes, confidence scores, etc. like this:\n",
        "        # detections = results[0].boxes # Assuming batch size is 1\n",
        "\n",
        "        # For this subtask, we just store the results object.\n",
        "        # In the next steps, we will process 'results' to find the ball and the goal line.\n",
        "        #print(f\"Inference performed on frame. Detected {len(results[0].boxes)} objects.\")\n",
        "\n",
        "\n",
        "        # --- Optional: Display the frame with detections (can be slow) ---\n",
        "        # annotated_frame = results[0].plot() # This draws bounding boxes and labels\n",
        "        # cv2.imshow('YOLOv8 Inference', annotated_frame)\n",
        "\n",
        "        # --- Add a small delay (optional) ---\n",
        "        # time.sleep(0.01)\n",
        "\n",
        "        # Break the loop if the 'q' key is pressed (requires a display window)\n",
        "        # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        #     break\n",
        "\n",
        "        # To stop in a headless environment, use Ctrl+C.\n",
        "        # Or implement a more robust shutdown mechanism if needed.\n",
        "\n",
        "    # Release the video capture object\n",
        "    #cap.release()\n",
        "\n",
        "    # Destroy any OpenCV windows (if cv2.imshow was used)\n",
        "    # cv2.destroyAllWindows()\n",
        "\n",
        "    #print(\"Video capture and inference stopped.\")\n",
        "\n",
        "# Instructions on how to run this script on your Raspberry Pi:\n",
        "# 1. Save the code as a Python file (e.g., detect_ball.py).\n",
        "# 2. Replace '/path/to/your/best.pt' with the actual path to your model file on your Pi.\n",
        "# 3. Open a terminal on your Raspberry Pi.\n",
        "# 4. Navigate to the directory where you saved the file.\n",
        "# 5. Run the script using: python3 detect_ball.py\n",
        "# 6. If you enabled cv2.imshow, press 'q' to exit the loop.\n",
        "# 7. If you did not enable cv2.imshow, press Ctrl+C in the terminal to stop the script."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99591187"
      },
      "source": [
        "## Procesar los resultados de la detección\n",
        "\n",
        "### Subtask:\n",
        "Extrae las coordenadas de las bounding boxes detectadas para la pelota y la línea de meta.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db06c755"
      },
      "source": [
        "**Reasoning**:\n",
        "Access the detection results from the YOLOv8 model, iterate through them, and extract the bounding box coordinates and confidence scores for the 'pelota' and 'línea de meta' classes, applying a confidence threshold. Then, print the extracted information.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "452914bc"
      },
      "source": [
        "#import cv2\n",
        "#import time\n",
        "#from ultralytics import YOLO\n",
        "\n",
        "# Camera index (usually 0 for the default camera)\n",
        "#camera_index = 0\n",
        "\n",
        "# Define the path to your trained best.pt model file on your Raspberry Pi\n",
        "# !!! IMPORTANT: Make sure this path is correct on your Raspberry Pi !!!\n",
        "#best_model_path = '/path/to/your/best.pt' # <--- CHANGE THIS TO YOUR ACTUAL PATH\n",
        "\n",
        "# Load the YOLOv8 model\n",
        "#try:\n",
        "    #model = YOLO(best_model_path)\n",
        "    #print(f\"Modelo YOLOv8 cargado exitosamente desde: {best_model_path}\")\n",
        "#except Exception as e:\n",
        "    #print(f\"Error al cargar el modelo: {e}\")\n",
        "    #print(f\"Por favor, verifica que la ruta '{best_model_path}' sea correcta y que el archivo best.pt exista en tu Raspberry Pi.\")\n",
        "    #exit()\n",
        "\n",
        "# !!! IMPORTANT: Define the class IDs for 'pelota' and 'línea de meta' !!!\n",
        "# You need to get these from your dataset.yaml file or training logs.\n",
        "# Example: If 'pelota' is class 0 and 'línea de meta' is class 1\n",
        "#pelota_class_id = 0 # <--- CHANGE THIS TO THE ACTUAL CLASS ID FOR 'pelota'\n",
        "#linea_meta_class_id = 1 # <--- CHANGE THIS TO THE ACTUAL CLASS ID FOR 'línea de meta'\n",
        "\n",
        "# Define a confidence threshold\n",
        "#confidence_threshold = 0.5 # <--- Adjust as needed\n",
        "\n",
        "# Initialize video capture\n",
        "#cap = cv2.VideoCapture(camera_index)\n",
        "\n",
        "# Check if the camera opened successfully\n",
        "#if not cap.isOpened():\n",
        "    #print(f\"Error: Could not open camera with index {camera_index}.\")\n",
        "    #print(\"Please check if the camera is connected and the index is correct.\")\n",
        "#else:\n",
        "    #print(f\"Camera with index {camera_index} opened successfully.\")\n",
        "\n",
        "    # Loop to continuously read frames\n",
        "    #while True:\n",
        "        # Read a frame from the camera\n",
        "        #ret, frame = cap.read()\n",
        "\n",
        "        # If the frame was not read successfully, break the loop\n",
        "        #if not ret:\n",
        "            #print(\"Error: Could not read frame.\")\n",
        "            #break\n",
        "\n",
        "        # Run YOLOv8 inference on the captured frame\n",
        "        #results = model(frame, verbose=False)\n",
        "\n",
        "        # --- Extract and process detection results ---\n",
        "        #pelota_detections = []\n",
        "        #linea_meta_detections = []\n",
        "\n",
        "        #if results and results[0].boxes:\n",
        "            #detections = results[0].boxes # Access the Boxes object\n",
        "\n",
        "            #for det in detections:\n",
        "                # Get bounding box coordinates [x1, y1, x2, y2]\n",
        "                #bbox = det.xyxy[0].tolist() # Convert tensor to list\n",
        "\n",
        "                # Get confidence score\n",
        "                #conf = det.conf[0].item() # Convert tensor to scalar\n",
        "\n",
        "                # Get class index\n",
        "                #cls = int(det.cls[0].item()) # Convert tensor to integer\n",
        "\n",
        "                # Filter by confidence threshold\n",
        "                #if conf >= confidence_threshold:\n",
        "                    #if cls == pelota_class_id:\n",
        "                        #pelota_detections.append({'bbox': bbox, 'conf': conf})\n",
        "                    #elif cls == linea_meta_class_id:\n",
        "                        #linea_meta_detections.append({'bbox': bbox, 'conf': conf})\n",
        "\n",
        "        # --- Print extracted information ---\n",
        "        #print(\"-\" * 30)\n",
        "        #print(f\"Frame processed at {time.time()}\")\n",
        "\n",
        "        #if pelota_detections:\n",
        "            #print(\"Detected Pelota(s):\")\n",
        "            #for det in pelota_detections:\n",
        "                #print(f\"  BBox: {det['bbox']}, Confidence: {det['conf']:.2f}\")\n",
        "        #else:\n",
        "            #print(\"No Pelota detected.\")\n",
        "\n",
        "        #if linea_meta_detections:\n",
        "            #print(\"Detected Línea de Meta(s):\")\n",
        "            #for det in linea_meta_detections:\n",
        "                #print(f\"  BBox: {det['bbox']}, Confidence: {det['conf']:.2f}\")\n",
        "        #else:\n",
        "            #print(\"No Línea de Meta detected.\")\n",
        "        #print(\"-\" * 30)\n",
        "\n",
        "\n",
        "        # --- Optional: Display the frame with detections (can be slow) ---\n",
        "        # annotated_frame = results[0].plot()\n",
        "        # cv2.imshow('YOLOv8 Inference', annotated_frame)\n",
        "\n",
        "        # --- Add a small delay (optional) ---\n",
        "        # time.sleep(0.01)\n",
        "\n",
        "        # Break the loop if the 'q' key is pressed (requires a display window)\n",
        "        # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        #     break\n",
        "\n",
        "        # To stop in a headless environment, use Ctrl+C.\n",
        "        # Or implement a more robust shutdown mechanism if needed.\n",
        "\n",
        "    # Release the video capture object\n",
        "    #cap.release()\n",
        "\n",
        "    # Destroy any OpenCV windows (if cv2.imshow was used)\n",
        "    # cv2.destroyAllWindows()\n",
        "\n",
        "    #print(\"Video capture and inference stopped.\")\n",
        "\n",
        "# Instructions on how to run this script on your Raspberry Pi:\n",
        "# 1. Save the code as a Python file (e.g., detect_objects.py).\n",
        "# 2. Replace '/path/to/your/best.pt' with the actual path to your model file on your Pi.\n",
        "# 3. !!! IMPORTANT: Replace pelota_class_id and linea_meta_class_id with the correct class IDs from your dataset.yaml !!!\n",
        "# 4. Adjust confidence_threshold if needed.\n",
        "# 5. Open a terminal on your Raspberry Pi.\n",
        "# 6. Navigate to the directory where you saved the file.\n",
        "# 7. Run the script using: python3 detect_objects.py\n",
        "# 8. If you enabled cv2.imshow, press 'q' to exit the loop.\n",
        "# 9. If you did not enable cv2.imshow, press Ctrl+C in the terminal to stop the script."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2ffd19c"
      },
      "source": [
        "## Implementar la lógica de \"gol\"\n",
        "\n",
        "### Subtask:\n",
        "Escribe código para determinar si la pelota ha cruzado la línea de meta basándose en las coordenadas detectadas y la posición relativa de ambos objetos. Esto puede implicar definir una región de interés para la línea de meta.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc1fd527"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the logic to check if the ball has crossed the goal line based on the extracted coordinates and relative positions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ede3d730",
        "outputId": "af167fcf-df43-47e4-c871-3b31d85aca22"
      },
      "source": [
        "import cv2\n",
        "import time\n",
        "from ultralytics import YOLO\n",
        "import numpy as np\n",
        "\n",
        "# Camera index (usually 0 for the default camera)\n",
        "camera_index = 0\n",
        "\n",
        "# Define the path to your trained best.pt model file on your Raspberry Pi\n",
        "# !!! IMPORTANT: Make sure this path is correct on your Raspberry Pi !!!\n",
        "best_model_path = '/path/to/your/best.pt' # <--- CHANGE THIS TO YOUR ACTUAL PATH\n",
        "\n",
        "# Load the YOLOv8 model\n",
        "try:\n",
        "    model = YOLO(best_model_path)\n",
        "    print(f\"Modelo YOLOv8 cargado exitosamente desde: {best_model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar el modelo: {e}\")\n",
        "    print(f\"Por favor, verifica que la ruta '{best_model_path}' sea correcta y que el archivo best.pt exista en tu Raspberry Pi.\")\n",
        "    exit()\n",
        "\n",
        "# !!! IMPORTANT: Define the class IDs for your \"valid\" ball classes and the goal line !!!\n",
        "# You need to get these from your dataset.yaml file or training logs.\n",
        "# Example: If 'balon_blanco' is class 0, 'balon_azul' is class 1, 'balon_verde' is class 2,\n",
        "# and 'línea_de_meta' is class 3\n",
        "valid_ball_class_ids = [0, 1, 2] # <--- CHANGE THIS TO THE ACTUAL CLASS IDs FOR YOUR VALID BALLS (e.g., [0, 1, 2])\n",
        "linea_meta_class_id = 3 # <--- CHANGE THIS TO THE ACTUAL CLASS ID FOR 'línea_de_meta'\n",
        "\n",
        "# Define a confidence threshold\n",
        "confidence_threshold = 0.5 # <--- Adjust as needed\n",
        "\n",
        "# Initialize video capture\n",
        "cap = cv2.VideoCapture(camera_index)\n",
        "\n",
        "# Check if the camera opened successfully\n",
        "if not cap.isOpened():\n",
        "    print(f\"Error: Could not open camera with index {camera_index}.\")\n",
        "    print(\"Please check if the camera is connected and the index is correct.\")\n",
        "else:\n",
        "    print(f\"Camera with index {camera_index} opened successfully.\")\n",
        "\n",
        "    # Define goal line orientation and crossing direction\n",
        "    # !!! IMPORTANT: Adjust these based on your setup and goal line orientation !!!\n",
        "    # Example: If the goal line is horizontal and crossing happens when the ball goes below the line\n",
        "    goal_line_orientation = 'horizontal' # 'horizontal' or 'vertical'\n",
        "    crossing_direction = 'below' # For horizontal: 'above' or 'below'\n",
        "                                 # For vertical: 'left' or 'right'\n",
        "\n",
        "    # Variable to track goal state\n",
        "    # We'll use this to trigger the action only once per crossing event\n",
        "    # For simplicity in this example, we'll just print a message every time the condition is met,\n",
        "    # but in a real application, you'd need a more robust state management.\n",
        "    goal_detected = False\n",
        "    prev_goal_detected = False # To track state change\n",
        "\n",
        "    # --- Placeholder function for goal action ---\n",
        "    def activate_goal_action():\n",
        "        \"\"\"\n",
        "        Placeholder function to perform the action when a goal is detected.\n",
        "        \"\"\"\n",
        "        print(\"¡¡¡ GOL !!!\")\n",
        "        # !!! IMPORTANT: Integrate your hardware control code here !!!\n",
        "        # If controlling an LED using RPi.GPIO:\n",
        "        # import RPi.GPIO as GPIO\n",
        "        # GPIO.setmode(GPIO.BCM) # Use BCM numbering\n",
        "        # led_pin = 17 # Example GPIO pin number\n",
        "        # GPIO.setup(led_pin, GPIO.OUT)\n",
        "        # GPIO.output(led_pin, GPIO.HIGH) # Turn LED on\n",
        "        # time.sleep(1) # Keep LED on for 1 second\n",
        "        # GPIO.output(led_pin, GPIO.LOW)  # Turn LED off\n",
        "        # GPIO.cleanup() # Clean up GPIO settings when done\n",
        "\n",
        "        # If sending a signal to another component:\n",
        "        # (Example using a simple print, replace with actual communication protocol)\n",
        "        # print(\"Sending signal to robot component...\")\n",
        "\n",
        "        # For this example, we just print.\n",
        "        pass # Remove this line when you add your code\n",
        "\n",
        "    # Loop to continuously read frames\n",
        "    while True:\n",
        "        # Read a frame from the camera\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # If the frame was not read successfully, break the loop\n",
        "        if not ret:\n",
        "            print(\"Error: Could not read frame.\")\n",
        "            break\n",
        "\n",
        "        # Run YOLOv8 inference on the captured frame\n",
        "        results = model(frame, verbose=False)\n",
        "\n",
        "        # --- Extract and process detection results ---\n",
        "        valid_ball_detections = [] # Now we collect all valid ball detections\n",
        "        linea_meta_detections = []\n",
        "\n",
        "        if results and results[0].boxes:\n",
        "            detections = results[0].boxes # Access the Boxes object\n",
        "\n",
        "            for det in detections:\n",
        "                # Get bounding box coordinates [x1, y1, x2, y2]\n",
        "                bbox = det.xyxy[0].tolist() # Convert tensor to list\n",
        "\n",
        "                # Get confidence score\n",
        "                conf = det.conf[0].item() # Convert tensor to scalar\n",
        "\n",
        "                # Get class index\n",
        "                cls = int(det.cls[0].item()) # Convert tensor to integer\n",
        "\n",
        "                # Filter by confidence threshold\n",
        "                if conf >= confidence_threshold:\n",
        "                    # Check if the detected class is one of the valid ball classes\n",
        "                    if cls in valid_ball_class_ids:\n",
        "                        valid_ball_detections.append({'bbox': bbox, 'conf': conf, 'class_id': cls}) # Store class_id too\n",
        "                    elif cls == linea_meta_class_id:\n",
        "                        linea_meta_detections.append({'bbox': bbox, 'conf': conf})\n",
        "\n",
        "        # --- Goal Logic Implementation ---\n",
        "        current_goal_detected = False # Flag for the current frame\n",
        "\n",
        "        # We only proceed if at least one valid ball and at least one goal line are detected\n",
        "        if valid_ball_detections and linea_meta_detections:\n",
        "            # For simplicity, assume the first detected valid ball is the one we care about\n",
        "            # You might need more sophisticated tracking if multiple valid balls are present\n",
        "            ball_bbox = valid_ball_detections[0]['bbox']\n",
        "            # Calculate the center of the ball's bounding box\n",
        "            ball_center_x = (ball_bbox[0] + ball_bbox[2]) / 2\n",
        "            ball_center_y = (ball_bbox[1] + ball_bbox[3]) / 2\n",
        "\n",
        "            # For simplicity, assume the first detected goal line is the relevant one\n",
        "            # You might need more sophisticated logic here if multiple lines are present\n",
        "            goal_line_bbox = linea_meta_detections[0]['bbox']\n",
        "\n",
        "            # Determine crossing based on orientation and direction\n",
        "            if goal_line_orientation == 'horizontal':\n",
        "                # Calculate the y-coordinate of the horizontal line (e.g., the top or bottom edge)\n",
        "                # Assuming the line is represented by a thin horizontal bbox\n",
        "                line_y = (goal_line_bbox[1] + goal_line_bbox[3]) / 2 # Using the center y-coordinate\n",
        "                # Or use goal_line_bbox[1] for the top edge, goal_line_bbox[3] for the bottom edge\n",
        "\n",
        "                if crossing_direction == 'below':\n",
        "                    # Check if the ball's center y-coordinate is below the line's y-coordinate\n",
        "                    if ball_center_y > line_y:\n",
        "                        current_goal_detected = True\n",
        "                elif crossing_direction == 'above':\n",
        "                    # Check if the ball's center y-coordinate is above the line's y-coordinate\n",
        "                    if ball_center_y < line_y:\n",
        "                        current_goal_detected = True\n",
        "\n",
        "            elif goal_line_orientation == 'vertical':\n",
        "                # Calculate the x-coordinate of the vertical line (e.g., the left or right edge)\n",
        "                 # Assuming the line is represented by a thin vertical bbox\n",
        "                line_x = (goal_line_bbox[0] + goal_line_bbox[2]) / 2 # Using the center x-coordinate\n",
        "                # Or use goal_line_bbox[0] for the left edge, goal_line_bbox[2] for the right edge\n",
        "\n",
        "                if crossing_direction == 'right':\n",
        "                    # Check if the ball's center x-coordinate is to the right of the line's x-coordinate\n",
        "                    if ball_center_x > line_x:\n",
        "                        current_goal_detected = True\n",
        "                elif crossing_direction == 'left':\n",
        "                     # Check if the ball's center x-coordinate is to the left of the line's x-coordinate\n",
        "                    if ball_center_x < line_x:\n",
        "                        current_goal_detected = True\n",
        "\n",
        "        # --- Activate Goal Action ---\n",
        "        # Trigger the action if a goal is detected in the current frame and it wasn't detected in the previous frame\n",
        "        # Note: This is a simple edge detection. More complex scenarios might require state machines\n",
        "        # or tracking the ball's movement across the line over multiple frames.\n",
        "        # Simple trigger for this step: activate every time the condition is met\n",
        "        if current_goal_detected:\n",
        "             activate_goal_action()\n",
        "\n",
        "        # Update previous state\n",
        "        prev_goal_detected = current_goal_detected\n",
        "\n",
        "\n",
        "        # --- Print extracted information (optional, can be removed for performance) ---\n",
        "        # print(\"-\" * 30)\n",
        "        # print(f\"Frame processed at {time.time()}. Goal Detected (current frame): {current_goal_detected}\")\n",
        "\n",
        "        # if valid_ball_detections:\n",
        "        #     print(\"Detected Valid Ball(s):\")\n",
        "        #     for det in valid_ball_detections:\n",
        "        #         print(f\"  Class ID: {det['class_id']}, BBox: {det['bbox']}, Confidence: {det['conf']:.2f}\")\n",
        "        # else:\n",
        "        #     print(\"No Valid Ball detected.\")\n",
        "\n",
        "        # if linea_meta_detections:\n",
        "        #     print(\"Detected Línea de Meta(s):\")\n",
        "        #     for det in linea_meta_detections:\n",
        "        #         print(f\"  BBox: {det['bbox']}, Confidence: {det['conf']:.2f}\")\n",
        "        # else:\n",
        "        #     print(\"No Línea de Meta detected.\")\n",
        "        # print(\"-\" * 30)\n",
        "\n",
        "\n",
        "        # --- Optional: Display the frame with detections (can be slow) ---\n",
        "        # annotated_frame = results[0].plot()\n",
        "        # cv2.imshow('YOLOv8 Inference', annotated_frame)\n",
        "\n",
        "        # --- Add a small delay (optional) ---\n",
        "        # time.sleep(0.01)\n",
        "\n",
        "        # Break the loop if the 'q' key is pressed (requires a display window)\n",
        "        # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        #     break\n",
        "\n",
        "        # To stop in a headless environment, use Ctrl+C.\n",
        "        # Or implement a more robust shutdown mechanism if needed.\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()\n",
        "\n",
        "    # Destroy any OpenCV windows (if cv2.imshow was used)\n",
        "    # cv2.destroyAllWindows()\n",
        "\n",
        "    print(\"Video capture and inference stopped.\")\n",
        "\n",
        "# Instructions on how to run this script on your Raspberry Pi:\n",
        "# 1. Save the code as a Python file (e.g., goal_system_filtered.py).\n",
        "# 2. Replace '/path/to/your/best.pt' with the actual path to your model file on your Pi.\n",
        "# 3. !!! IMPORTANT: Replace valid_ball_class_ids with the correct list of class IDs for YOUR valid balls (e.g., [0, 1, 2]) !!!\n",
        "# 4. !!! IMPORTANT: Replace linea_meta_class_id with the correct class ID for 'línea_de_meta' (e.g., 3) !!!\n",
        "# 5. !!! IMPORTANT: Adjust goal_line_orientation and crossing_direction based on your setup !!!\n",
        "# 6. Adjust confidence_threshold if needed.\n",
        "# 7. !!! IMPORTANT: Implement your specific goal action code inside the activate_goal_action() function !!!\n",
        "# 8. Open a terminal on your Raspberry Pi.\n",
        "# 9. Navigate to the directory where you saved the file.\n",
        "# 10. Run the script using: python3 goal_system_filtered.py\n",
        "# 11. If you enabled cv2.imshow, press 'q' to exit the loop.\n",
        "# 12. If you did not enable cv2.imshow, press Ctrl+C in the terminal to stop the script."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Error al cargar el modelo: [Errno 2] No such file or directory: '/path/to/your/best.pt'\n",
            "Por favor, verifica que la ruta '/path/to/your/best.pt' sea correcta y que el archivo best.pt exista en tu Raspberry Pi.\n",
            "Error: Could not open camera with index 0.\n",
            "Please check if the camera is connected and the index is correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9286cb4f"
      },
      "source": [
        "## Activar la acción de \"gol\"\n",
        "\n",
        "### Subtask:\n",
        "Cuando se detecte que la pelota cruza la línea, activa alguna acción (por ejemplo, imprimir un mensaje \"¡Gol!\", encender un LED, enviar una señal a otro componente del robot, etc.).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adc0cab8"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the script to include a placeholder function for the goal action and call it within the goal detection logic.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "597a7edc",
        "outputId": "671444c6-29ab-45a9-ccc4-a60c83877e9a"
      },
      "source": [
        "import cv2\n",
        "import time\n",
        "from ultralytics import YOLO\n",
        "import numpy as np\n",
        "\n",
        "# Camera index (usually 0 for the default camera)\n",
        "camera_index = 0\n",
        "\n",
        "# Define the path to your trained best.pt model file on your Raspberry Pi\n",
        "# !!! IMPORTANT: Make sure this path is correct on your Raspberry Pi !!!\n",
        "best_model_path = '/path/to/your/best.pt' # <--- CHANGE THIS TO YOUR ACTUAL PATH\n",
        "\n",
        "# Load the YOLOv8 model\n",
        "try:\n",
        "    model = YOLO(best_model_path)\n",
        "    print(f\"Modelo YOLOv8 cargado exitosamente desde: {best_model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar el modelo: {e}\")\n",
        "    print(f\"Por favor, verifica que la ruta '{best_model_path}' sea correcta y que el archivo best.pt exista en tu Raspberry Pi.\")\n",
        "    exit()\n",
        "\n",
        "# !!! IMPORTANT: Define the class IDs for 'pelota' and 'línea de meta' !!!\n",
        "# You need to get these from your dataset.yaml file or training logs.\n",
        "# Example: If 'pelota' is class 0 and 'línea de meta' is class 1\n",
        "pelota_class_id = 0 # <--- CHANGE THIS TO THE ACTUAL CLASS ID FOR 'pelota'\n",
        "linea_meta_class_id = 1 # <--- CHANGE THIS TO THE ACTUAL CLASS ID FOR 'línea_de_meta'\n",
        "\n",
        "# Define a confidence threshold\n",
        "confidence_threshold = 0.5 # <--- Adjust as needed\n",
        "\n",
        "# Initialize video capture\n",
        "cap = cv2.VideoCapture(camera_index)\n",
        "\n",
        "# Check if the camera opened successfully\n",
        "if not cap.isOpened():\n",
        "    print(f\"Error: Could not open camera with index {camera_index}.\")\n",
        "    print(\"Please check if the camera is connected and the index is correct.\")\n",
        "else:\n",
        "    print(f\"Camera with index {camera_index} opened successfully.\")\n",
        "\n",
        "    # Define goal line orientation and crossing direction\n",
        "    # !!! IMPORTANT: Adjust these based on your setup and goal line orientation !!!\n",
        "    # Example: If the goal line is horizontal and crossing happens when the ball goes below the line\n",
        "    goal_line_orientation = 'horizontal' # 'horizontal' or 'vertical'\n",
        "    crossing_direction = 'below' # For horizontal: 'above' or 'below'\n",
        "                                 # For vertical: 'left' or 'right'\n",
        "\n",
        "    # Variable to track goal state\n",
        "    # We'll use this to trigger the action only once per crossing event\n",
        "    # For simplicity in this example, we'll just print a message every time the condition is met,\n",
        "    # but in a real application, you'd need a more robust state management.\n",
        "    goal_detected = False\n",
        "    prev_goal_detected = False # To track state change\n",
        "\n",
        "    # --- Placeholder function for goal action ---\n",
        "    def activate_goal_action():\n",
        "        \"\"\"\n",
        "        Placeholder function to perform the action when a goal is detected.\n",
        "        \"\"\"\n",
        "        print(\"¡¡¡ GOL !!!\")\n",
        "        # !!! IMPORTANT: Integrate your hardware control code here !!!\n",
        "        # If controlling an LED using RPi.GPIO:\n",
        "        # import RPi.GPIO as GPIO\n",
        "        # GPIO.setmode(GPIO.BCM) # Use BCM numbering\n",
        "        # led_pin = 17 # Example GPIO pin number\n",
        "        # GPIO.setup(led_pin, GPIO.OUT)\n",
        "        # GPIO.output(led_pin, GPIO.HIGH) # Turn LED on\n",
        "        # time.sleep(1) # Keep LED on for 1 second\n",
        "        # GPIO.output(led_pin, GPIO.LOW)  # Turn LED off\n",
        "        # GPIO.cleanup() # Clean up GPIO settings when done\n",
        "\n",
        "        # If sending a signal to another component:\n",
        "        # (Example using a simple print, replace with actual communication protocol)\n",
        "        # print(\"Sending signal to robot component...\")\n",
        "\n",
        "        # For this example, we just print.\n",
        "        pass # Remove this line when you add your code\n",
        "\n",
        "    # Loop to continuously read frames\n",
        "    while True:\n",
        "        # Read a frame from the camera\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # If the frame was not read successfully, break the loop\n",
        "        if not ret:\n",
        "            print(\"Error: Could not read frame.\")\n",
        "            break\n",
        "\n",
        "        # Run YOLOv8 inference on the captured frame\n",
        "        results = model(frame, verbose=False)\n",
        "\n",
        "        # --- Extract and process detection results ---\n",
        "        pelota_detections = []\n",
        "        linea_meta_detections = []\n",
        "\n",
        "        if results and results[0].boxes:\n",
        "            detections = results[0].boxes # Access the Boxes object\n",
        "\n",
        "            for det in detections:\n",
        "                # Get bounding box coordinates [x1, y1, x2, y2]\n",
        "                bbox = det.xyxy[0].tolist() # Convert tensor to list\n",
        "\n",
        "                # Get confidence score\n",
        "                conf = det.conf[0].item() # Convert tensor to scalar\n",
        "\n",
        "                # Get class index\n",
        "                cls = int(det.cls[0].item()) # Convert tensor to integer\n",
        "\n",
        "                # Filter by confidence threshold\n",
        "                if conf >= confidence_threshold:\n",
        "                    if cls == pelota_class_id:\n",
        "                        pelota_detections.append({'bbox': bbox, 'conf': conf})\n",
        "                    elif cls == linea_meta_class_id:\n",
        "                        linea_meta_detections.append({'bbox': bbox, 'conf': conf})\n",
        "\n",
        "        # --- Goal Logic Implementation ---\n",
        "        current_goal_detected = False # Flag for the current frame\n",
        "\n",
        "        if pelota_detections and linea_meta_detections:\n",
        "            # Assume the first detected ball is the one we care about\n",
        "            ball_bbox = pelota_detections[0]['bbox']\n",
        "            # Calculate the center of the ball's bounding box\n",
        "            ball_center_x = (ball_bbox[0] + ball_bbox[2]) / 2\n",
        "            ball_center_y = (ball_bbox[1] + ball_bbox[3]) / 2\n",
        "\n",
        "            # For simplicity, assume the first detected goal line is the relevant one\n",
        "            # You might need more sophisticated logic here if multiple lines are present\n",
        "            goal_line_bbox = linea_meta_detections[0]['bbox']\n",
        "\n",
        "            # Determine crossing based on orientation and direction\n",
        "            if goal_line_orientation == 'horizontal':\n",
        "                # Calculate the y-coordinate of the horizontal line (e.g., the top or bottom edge)\n",
        "                # Assuming the line is represented by a thin horizontal bbox\n",
        "                line_y = (goal_line_bbox[1] + goal_line_bbox[3]) / 2 # Using the center y-coordinate\n",
        "                # Or use goal_line_bbox[1] for the top edge, goal_line_bbox[3] for the bottom edge\n",
        "\n",
        "                if crossing_direction == 'below':\n",
        "                    # Check if the ball's center y-coordinate is below the line's y-coordinate\n",
        "                    if ball_center_y > line_y:\n",
        "                        current_goal_detected = True\n",
        "                elif crossing_direction == 'above':\n",
        "                    # Check if the ball's center y-coordinate is above the line's y-coordinate\n",
        "                    if ball_center_y < line_y:\n",
        "                        current_goal_detected = True\n",
        "\n",
        "            elif goal_line_orientation == 'vertical':\n",
        "                # Calculate the x-coordinate of the vertical line (e.g., the left or right edge)\n",
        "                 # Assuming the line is represented by a thin vertical bbox\n",
        "                line_x = (goal_line_bbox[0] + goal_line_bbox[2]) / 2 # Using the center x-coordinate\n",
        "                # Or use goal_line_bbox[0] for the left edge, goal_line_bbox[2] for the right edge\n",
        "\n",
        "                if crossing_direction == 'right':\n",
        "                    # Check if the ball's center x-coordinate is to the right of the line's x-coordinate\n",
        "                    if ball_center_x > line_x:\n",
        "                        current_goal_detected = True\n",
        "                elif crossing_direction == 'left':\n",
        "                     # Check if the ball's center x-coordinate is to the left of the line's x-coordinate\n",
        "                    if ball_center_x < line_x:\n",
        "                        current_goal_detected = True\n",
        "\n",
        "        # --- Activate Goal Action ---\n",
        "        # Trigger the action if a goal is detected in the current frame and it wasn't detected in the previous frame\n",
        "        # Note: This is a simple edge detection. More complex scenarios might require state machines\n",
        "        # or tracking the ball's movement across the line over multiple frames.\n",
        "        if current_goal_detected: # For this step, we trigger every time the condition is met\n",
        "             activate_goal_action()\n",
        "\n",
        "\n",
        "        # --- Print extracted information (optional, can be removed for performance) ---\n",
        "        # print(\"-\" * 30)\n",
        "        # print(f\"Frame processed at {time.time()}. Goal Detected (current frame): {current_goal_detected}\")\n",
        "\n",
        "        # if pelota_detections:\n",
        "        #     print(\"Detected Pelota(s):\")\n",
        "        #     for det in pelota_detections:\n",
        "        #         print(f\"  BBox: {det['bbox']}, Confidence: {det['conf']:.2f}\")\n",
        "        # else:\n",
        "        #     print(\"No Pelota detected.\")\n",
        "\n",
        "        # if linea_meta_detections:\n",
        "        #     print(\"Detected Línea de Meta(s):\")\n",
        "        #     for det in linea_meta_detections:\n",
        "        #         print(f\"  BBox: {det['bbox']}, Confidence: {det['conf']:.2f}\")\n",
        "        # else:\n",
        "        #     print(\"No Línea de Meta detected.\")\n",
        "        # print(\"-\" * 30)\n",
        "\n",
        "\n",
        "        # --- Optional: Display the frame with detections (can be slow) ---\n",
        "        # annotated_frame = results[0].plot()\n",
        "        # cv2.imshow('YOLOv8 Inference', annotated_frame)\n",
        "\n",
        "        # --- Add a small delay (optional) ---\n",
        "        # time.sleep(0.01)\n",
        "\n",
        "        # Break the loop if the 'q' key is pressed (requires a display window)\n",
        "        # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        #     break\n",
        "\n",
        "        # To stop in a headless environment, use Ctrl+C.\n",
        "        # Or implement a more robust shutdown mechanism if needed.\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()\n",
        "\n",
        "    # Destroy any OpenCV windows (if cv2.imshow was used)\n",
        "    # cv2.destroyAllWindows()\n",
        "\n",
        "    print(\"Video capture and inference stopped.\")\n",
        "\n",
        "# Instructions on how to run this script on your Raspberry Pi:\n",
        "# 1. Save the code as a Python file (e.g., goal_system.py).\n",
        "# 2. Replace '/path/to/your/best.pt' with the actual path to your model file on your Pi.\n",
        "# 3. !!! IMPORTANT: Replace pelota_class_id and linea_meta_class_id with the correct class IDs from your dataset.yaml !!!\n",
        "# 4. !!! IMPORTANT: Adjust goal_line_orientation and crossing_direction based on your setup !!!\n",
        "# 5. Adjust confidence_threshold if needed.\n",
        "# 6. !!! IMPORTANT: Implement your specific goal action code inside the activate_goal_action() function !!!\n",
        "# 7. Open a terminal on your Raspberry Pi.\n",
        "# 8. Navigate to the directory where you saved the file.\n",
        "# 9. Run the script using: python3 goal_system.py\n",
        "# 10. If you enabled cv2.imshow, press 'q' to exit the loop.\n",
        "# 11. If you did not enable cv2.imshow, press Ctrl+C in the terminal to stop the script."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error al cargar el modelo: [Errno 2] No such file or directory: '/path/to/your/best.pt'\n",
            "Por favor, verifica que la ruta '/path/to/your/best.pt' sea correcta y que el archivo best.pt exista en tu Raspberry Pi.\n",
            "Error: Could not open camera with index 0.\n",
            "Please check if the camera is connected and the index is correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d26bf6ed"
      },
      "source": [
        "## Optimizar el rendimiento\n",
        "\n",
        "### Subtask:\n",
        "Si la detección en tiempo real es demasiado lenta, explora opciones de optimización como usar una versión cuantizada del modelo (TensorFlow Lite), ajustar parámetros de YOLOv8 (como el umbral de confianza), o usar una versión optimizada de PyTorch para dispositivos ARM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "757ad114"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is about optimizing the real-time detection performance on the Raspberry Pi. This involves running the current script to evaluate performance, and then exploring different optimization techniques. Since I cannot directly run the script on a Raspberry Pi or measure FPS, I will provide instructions and code snippets for the user to perform these steps on their device. The first step is to guide the user on how to run the existing script and evaluate its performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d534c126",
        "outputId": "63187085-cfd6-4691-bc58-9e62227e6a02"
      },
      "source": [
        "# Read the content of the codigo.txt file\n",
        "with open('/content/codigo.txt', 'r') as f:\n",
        "    codigo_content = f.read()\n",
        "\n",
        "# Print the content\n",
        "print(codigo_content)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import cv2\n",
            "import time\n",
            "import numpy as np\n",
            "# Importa la librería tflite_runtime\n",
            "# Asegúrate de haberla instalado en tu Raspberry Pi: pip install tflite-runtime\n",
            "import tflite_runtime.interpreter as tflite\n",
            "\n",
            "# Define el índice de la cámara (usualmente 0).\n",
            "camera_index = 0\n",
            "\n",
            "# --- Define la ruta a tu archivo de modelo .tflite ---\n",
            "# !!! IMPORTANTE: Reemplaza esta ruta con la ruta real donde guardaste\n",
            "# el archivo best.tflite en tu Raspberry Pi. !!!\n",
            "# Por ejemplo: '/home/pi/robot_vision/best.tflite'\n",
            "best_model_path_tflite = '/path/to/your/best.tflite' # <--- CAMBIA ESTO A TU RUTA REAL\n",
            "\n",
            "# --- Define la ruta a tu archivo classes.txt ---\n",
            "# También necesitarás un archivo de texto con los nombres de tus clases,\n",
            "# uno por línea, en el mismo orden que usaste para entrenar.\n",
            "# Este archivo a menudo se crea automáticamente durante el entrenamiento o puedes crearlo manualmente.\n",
            "# Por ejemplo: '/home/pi/robot_vision/classes.txt'\n",
            "classes_file_path = '/path/to/your/classes.txt' # <--- CAMBIA ESTO A LA RUTA REAL\n",
            "\n",
            "# Define un umbral de confianza para las detecciones.\n",
            "# Las detecciones con confianza por debajo de este umbral serán ignoradas.\n",
            "confidence_threshold = 0.5 # <--- Ajusta según necesites\n",
            "\n",
            "# !!! IMPORTANTE: Define los IDs de clase para 'pelota' y 'línea de meta' !!!\n",
            "# Necesitas obtener estos del archivo classes.txt o de cómo definiste tus clases al entrenar.\n",
            "# Si 'pelota' es la primera clase en classes.txt y 'línea de meta' la segunda (índice 0 y 1):\n",
            "pelota_class_id = 0 # <--- CAMBIA ESTO AL ID CORRECTO PARA 'pelota'\n",
            "linea_meta_class_id = 1 # <--- CAMBIA ESTO AL ID CORRECTO PARA 'línea de meta'\n",
            "\n",
            "\n",
            "# --- Carga los nombres de las clases ---\n",
            "# Leemos el archivo classes.txt para tener los nombres de las clases disponibles.\n",
            "try:\n",
            "    with open(classes_file_path, 'r') as f:\n",
            "        class_names = [line.strip() for line in f.readlines()]\n",
            "    print(f\"Nombres de clases cargados desde: {classes_file_path}\")\n",
            "    print(f\"Clases detectadas: {class_names}\")\n",
            "except Exception as e:\n",
            "    print(f\"Error al cargar el archivo de clases {classes_file_path}: {e}\")\n",
            "    print(\"Asegúrate de que el archivo classes.txt exista y la ruta sea correcta.\")\n",
            "    exit() # Sal del script si no se pueden cargar las clases.\n",
            "\n",
            "\n",
            "# --- Carga el modelo TensorFlow Lite ---\n",
            "try:\n",
            "    # Carga el modelo .tflite y obtiene el intérprete.\n",
            "    interpreter = tflite.Interpreter(model_path=best_model_path_tflite)\n",
            "\n",
            "    # Asigna los tensores (input y output). Esto configura el modelo para la inferencia.\n",
            "    interpreter.allocate_tensors()\n",
            "\n",
            "    # Obtiene información sobre los tensores de entrada y salida del modelo.\n",
            "    input_details = interpreter.get_input_details()\n",
            "    output_details = interpreter.get_output_details()\n",
            "\n",
            "    # Obtiene el tamaño de entrada esperado por el modelo (altura, ancho)\n",
            "    # Los modelos TFLite de detección de objetos basados en YOLO a menudo esperan un tamaño fijo.\n",
            "    # input_details[0]['shape'] da la forma del tensor de entrada, usualmente [1, height, width, 3]\n",
            "    input_shape = input_details[0]['shape']\n",
            "    input_height = input_shape[1]\n",
            "    input_width = input_shape[2]\n",
            "\n",
            "    print(f\"Modelo TFLite cargado exitosamente desde: {best_model_path_tflite}\")\n",
            "    print(f\"Tamaño de entrada esperado por el modelo: {input_width}x{input_height}\")\n",
            "\n",
            "except Exception as e:\n",
            "    print(f\"Error al cargar el modelo TFLite: {e}\")\n",
            "    print(f\"Por favor, verifica que la ruta '{best_model_path_tflite}' sea correcta y que el archivo best.tflite exista en tu Raspberry Pi.\")\n",
            "    print(\"Asegúrate también de haber instalado 'tflite-runtime'.\")\n",
            "    exit() # Sal del script si no se pudo cargar el modelo TFLite.\n",
            "\n",
            "\n",
            "# Inicializa la captura de video desde la cámara.\n",
            "\n",
            "# Inicializa la captura de video desde la cámara.\n",
            "cap = cv2.VideoCapture(camera_index)\n",
            "\n",
            "# Verifica si la cámara se abrió correctamente.\n",
            "if not cap.isOpened():\n",
            "    print(f\"Error: No se pudo abrir la cámara con índice {camera_index}.\")\n",
            "    print(\"Por favor, verifica si la cámara está conectada y el índice es correcto.\")\n",
            "    exit()\n",
            "else:\n",
            "    print(f\"Cámara con índice {camera_index} abierta exitosamente.\")\n",
            "\n",
            "    # --- Bucle principal para la captura y detección continua ---\n",
            "    while True:\n",
            "        # Lee un frame de la cámara.\n",
            "        ret, frame = cap.read()\n",
            "\n",
            "        # Si no se pudo leer el frame, rompe el bucle.\n",
            "        if not ret:\n",
            "            print(\"Error: No se pudo leer frame. Saliendo del bucle de captura.\")\n",
            "            break\n",
            "\n",
            "        # --- Pre-procesamiento del frame para el modelo TFLite ---\n",
            "        input_frame = cv2.resize(frame, (input_width, input_height))\n",
            "        input_data = np.expand_dims(input_frame, axis=0)\n",
            "        input_data = input_data.astype(np.uint8) # O np.float32, dependiendo de tu modelo\n",
            "\n",
            "        # --- Ejecuta la inferencia con el modelo TFLite ---\n",
            "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
            "        interpreter.invoke()\n",
            "\n",
            "        # --- Obtiene los resultados de la inferencia ---\n",
            "        # Recuerda que la estructura exacta de estas salidas depende de tu exportación a TFLite.\n",
            "        # Este es un ejemplo común:\n",
            "        output_boxes = interpreter.get_tensor(output_details[0]['index'])[0] # [ymin, xmin, ymax, xmax] normalizado\n",
            "        output_classes = interpreter.get_tensor(output_details[1]['index'])[0] # IDs de clase\n",
            "        output_scores = interpreter.get_tensor(output_details[2]['index'])[0] # Puntajes de confianza\n",
            "\n",
            "        # --- Comienza el procesamiento de los resultados (Paso 6) ---\n",
            "\n",
            "        # Crea listas vacías para almacenar las detecciones de la pelota y la línea de meta\n",
            "        pelota_detections = []\n",
            "        linea_meta_detections = []\n",
            "\n",
            "        # Obtiene las dimensiones originales del frame para desnormalizar las coordenadas\n",
            "        frame_height, frame_width, _ = frame.shape\n",
            "\n",
            "        # Itera sobre cada detección reportada por el modelo TFLite\n",
            "        # output_scores.shape[0] te da el número total de detecciones propuestas por el modelo\n",
            "        for i in range(output_scores.shape[0]):\n",
            "             score = output_scores[i]\n",
            "             class_id = int(output_classes[i])\n",
            "             bbox_normalized = output_boxes[i]\n",
            "\n",
            "             # --- Aplica el umbral de confianza ---\n",
            "             # Solo consideramos las detecciones si su confianza es igual o mayor al umbral definido\n",
            "             if score >= confidence_threshold:\n",
            "\n",
            "                 # --- Desnormaliza las coordenadas de la bounding box ---\n",
            "                 # Convierte las coordenadas que están entre 0 y 1 (normalizadas) a píxeles\n",
            "                 # en el tamaño original del frame.\n",
            "                 # Las coordenadas TFLite son típicamente [ymin, xmin, ymax, xmax]\n",
            "                 x1 = int(bbox_normalized[1] * frame_width) # xmin\n",
            "                 y1 = int(bbox_normalized[0] * frame_height) # ymin\n",
            "                 x2 = int(bbox_normalized[3] * frame_width) # xmax\n",
            "                 y2 = int(bbox_normalized[2] * frame_height) # ymax\n",
            "\n",
            "                 # Asegura que las coordenadas estén dentro de los límites del frame (0 a width/height)\n",
            "                 x1 = max(0, x1)\n",
            "                 y1 = max(0, y1)\n",
            "                 x2 = min(frame_width - 1, x2)\n",
            "                 y2 = min(frame_height - 1, y2)\n",
            "\n",
            "                 bbox_pixel = [x1, y1, x2, y2] # La bounding box en coordenadas de píxeles\n",
            "\n",
            "                 # --- Identifica la clase detectada ---\n",
            "                 # Compara el ID de clase detectado con los IDs de clase que definiste\n",
            "                 # para la pelota y la línea de meta al principio del script.\n",
            "                 if class_id == pelota_class_id:\n",
            "                     # Si es una pelota y pasa el umbral, la añadimos a la lista de detecciones de pelota\n",
            "                     pelota_detections.append({\n",
            "                         'bbox': bbox_pixel,\n",
            "                         'conf': score,\n",
            "                         'class_id': class_id,\n",
            "                         'class_name': class_names[class_id] # Opcional: guarda el nombre de la clase\n",
            "                     })\n",
            "                 elif class_id == linea_meta_class_id:\n",
            "                     # Si es una línea de meta y pasa el umbral, la añadimos a la lista\n",
            "                     linea_meta_detections.append({\n",
            "                         'bbox': bbox_pixel,\n",
            "                         'conf': score,\n",
            "                         'class_id': class_id,\n",
            "                         'class_name': class_names[class_id] # Opcional: guarda el nombre de la clase\n",
            "                     })\n",
            "                 # Puedes añadir 'elif' para otras clases si las tienes y te interesan.\n",
            "\n",
            "        # --- Fin del procesamiento de los resultados (Paso 6 completo para este frame) ---\n",
            "\n",
            "        # En este punto, para cada frame, las listas 'pelota_detections'\n",
            "        # y 'linea_meta_detections' contendrán diccionarios con la información\n",
            "        # de las bounding boxes y confianzas de los objetos detectados\n",
            "        # de esas clases específicas que pasaron el umbral de confianza.\n",
            "\n",
            "        # Puedes imprimir estas listas aquí para depurar y ver qué se está detectando:\n",
            "        # print(\"-\" * 30)\n",
            "        # print(f\"Frame procesado.\")\n",
            "        # print(f\"Pelota(s) detectada(s) en este frame: {pelota_detections}\")\n",
            "        # print(f\"Línea(s) de Meta detectada(s) en este frame: {linea_meta_detections}\")\n",
            "        # print(\"-\" * 30)\n",
            "\n",
            "\n",
            "        # --- Aquí es donde implementarías la lógica de \"gol\" (Paso 7) ---\n",
            "        # Con las listas pelota_detections y linea_meta_detections pobladas,\n",
            "        # el siguiente paso es usarlas para determinar si la pelota cruzó la línea.\n",
            "        # El código para el Paso 7 irá aquí.\n",
            "\n",
            "\n",
            "        # ... (código posterior para visualización opcional, waitKey, release, destroyAllWindows) ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ENTRENAR DE NUEVO**"
      ],
      "metadata": {
        "id": "okzzRwVbOsl7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71bd4b29"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define la ruta a tu archivo dataset.yaml\n",
        "# ASEGÚRATE DE QUE ESTA RUTA SEA CORRECTA\n",
        "yaml_path = '/content/drive/My Drive/YoloV8/dataset.yaml' # <-- ASEGÚRATE DE QUE ESTA RUTA SEA CORRECTA\n",
        "\n",
        "# Define la ruta al modelo .pt con el que quieres empezar el NUEVO entrenamiento.\n",
        "# Puede ser tu best.pt de un entrenamiento anterior, o cualquier otro modelo .pt.\n",
        "# ASEGÚRATE DE QUE ESTA RUTA SEA CORRECTA\n",
        "starting_model_path = '/content/drive/My Drive/YoloV8/testing/data2/weights/best.pt' # <-- CAMBIA ESTO A LA RUTA DE TU MODELO .PT\n",
        "\n",
        "# Define los parámetros de entrenamiento para este NUEVO entrenamiento\n",
        "epochs = 100 # Puedes ajustar el número de épocas\n",
        "img_size = 640 # Puedes ajustar el tamaño de la imagen (debe coincidir con el modelo si no es un modelo base)\n",
        "batch_size = 16 # <--- Define el tamaño del batch aquí (ej: 16, 32, 64)\n",
        "\n",
        "# Define el directorio de salida para los resultados de este NUEVO entrenamiento\n",
        "# Es recomendable usar un nombre de carpeta diferente para no sobrescribir los resultados anteriores.\n",
        "output_project_dir = '/content/drive/My Drive/YoloV8/testing'\n",
        "run_name = 'data_retrain' # <-- CAMBIA ESTO A UN NOMBRE ÚNICO PARA ESTE ENTRENAMIENTO\n",
        "\n",
        "# Construye el comando para el NUEVO entrenamiento\n",
        "# Apuntamos al modelo .pt que queremos usar como punto de partida\n",
        "train_command_retrain = f\"yolo train data=\\\"{yaml_path}\\\" model=\\\"{starting_model_path}\\\" epochs={epochs} imgsz={img_size} project=\\\"{output_project_dir}\\\" name=\\\"{run_name}\\\" batch={batch_size}\"\n",
        "\n",
        "print(f\"Generated retraining command: {train_command_retrain}\")\n",
        "\n",
        "# Para ejecutar este comando, puedes copiarlo y pegarlo en una celda de terminal de Colab\n",
        "# O puedes usar el magic command ! para ejecutarlo directamente\n",
        "\n",
        "# !!! DESCOMENTA LA SIGUIENTE LÍNEA PARA EJECUTAR EL COMANDO DE ENTRENAMIENTO !!!\n",
        "# !{train_command_retrain}\n",
        "\n",
        "print(\"\\nCopy the command above and run it in a new cell (either code or terminal) to start the new training.\")\n",
        "print(\"Alternatively, uncomment the line `!{train_command_retrain}` above and run this cell.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**mas cosas**"
      ],
      "metadata": {
        "id": "fUg37OWgB8fB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d052017"
      },
      "source": [
        "# Task\n",
        "Adapta el código de detección de gol en la Raspberry Pi para usar un modelo entrenado con las clases 'balon', 'linea_meta', y 'travesaño'. Implementa una lógica de detección de gol que considere la posición del balón dentro de una Región de Interés (ROI) del arco, su cruce de la línea de meta, y su posición por debajo del travesaño."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b35cca63"
      },
      "source": [
        "## Confirmar o actualizar dataset y modelo\n",
        "\n",
        "### Subtask:\n",
        "Verificar si tu dataset de entrenamiento incluye las tres clases ('balon', 'linea_meta', 'travesaño') con anotaciones correctas. Si no, te indicaré los pasos para actualizar el dataset y re-entrenar el modelo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dd327c9"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires the user to manually check their dataset files to verify the presence and correctness of annotations and class definitions for 'balon', 'linea_meta', and 'travesaño'. Since I cannot directly access the user's files on their Google Drive or local machine, I need to provide instructions for the user to perform these checks themselves. I will provide clear instructions in a markdown cell explaining where to look and what to verify.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61c0dd4a"
      },
      "source": [
        "# Instructions for the user to check their dataset files:\n",
        "\n",
        "# Step 1: Navigate to your dataset directory in Google Drive or your local machine.\n",
        "# This is the folder containing your original images and their corresponding .txt annotation files.\n",
        "# Based on your previous code, this might be something like:\n",
        "# '/content/drive/My Drive/YoloV8/grupos/0-3900'\n",
        "\n",
        "# Step 2: Open a few of the .txt annotation files.\n",
        "# Use a text editor to open some files like 'Entrenamiento_1.txt', 'Entrenamiento_2.txt', etc.\n",
        "# Each line in these files should represent a detected object and look like:\n",
        "# class_id center_x center_y width height\n",
        "# For example:\n",
        "# 0 0.5 0.5 0.1 0.1  (where 0 is the class_id)\n",
        "\n",
        "# Verify that you see lines corresponding to the objects you expect ('balon', 'linea_meta', 'travesaño').\n",
        "# Note down the class_id (the first number on the line) for each of these objects.\n",
        "\n",
        "# Step 3: Find and open your dataset.yaml file.\n",
        "# Based on your previous code, this might be located at:\n",
        "# '/content/drive/My Drive/YoloV8/dataset.yaml'\n",
        "\n",
        "# Step 4: Examine the contents of the dataset.yaml file.\n",
        "# Look for the 'nc' (number of classes) field and the 'names' list.\n",
        "# Verify that 'nc' is the correct number of classes in your dataset.\n",
        "# Verify that the 'names' list contains 'balon', 'linea_meta', and 'travesaño' in the correct order,\n",
        "# and that their position in this list corresponds to the class_id you noted in Step 2.\n",
        "# For example:\n",
        "# names: ['balon', 'linea_meta', 'travesaño', ...]\n",
        "# In this example, 'balon' would be class_id 0, 'linea_meta' would be class_id 1, and 'travesaño' would be class_id 2.\n",
        "\n",
        "# Step 5: Compare the class_ids from your .txt files with the class names in your dataset.yaml.\n",
        "# Ensure that the numeric class IDs in your annotation files correctly correspond to the names\n",
        "# 'balon', 'linea_meta', and 'travesaño' as defined in your dataset.yaml.\n",
        "\n",
        "# If any of these checks reveal missing classes in annotations or dataset.yaml, or incorrect class IDs,\n",
        "# you will need to update your dataset and potentially re-annotate images.\n",
        "# If you update your dataset, you MUST re-train your YOLOv8 model with the updated dataset.yaml."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f498297"
      },
      "source": [
        "## Modificar el script: cargar modelo y clases\n",
        "\n",
        "### Subtask:\n",
        "Actualizar el script existente en la Raspberry Pi para cargar el nuevo modelo `.pt` (entrenado con las tres clases) y los nombres de las clases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37a90268"
      },
      "source": [
        "**Reasoning**:\n",
        "Update the existing script to load the new model and class information by modifying the relevant variables and ensuring the script is ready for the next steps of processing the detections.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a266372"
      },
      "source": [
        "import cv2\n",
        "import time\n",
        "import numpy as np\n",
        "# Importa la librería tflite_runtime para cargar y ejecutar el modelo TFLite\n",
        "import tflite_runtime.interpreter as tflite\n",
        "# Importa la librería RPi.GPIO para controlar pines GPIO (solo en Raspberry Pi)\n",
        "# Asegúrate de haberla instalado en tu Raspberry Pi: pip install rpi.gpio\n",
        "import RPi.GPIO as GPIO\n",
        "\n",
        "# --- CONFIGURACIONES ---\n",
        "\n",
        "# Define los índices de las cámaras. Usualmente 0 y 1 para dos cámaras.\n",
        "# Usa 'ls /dev/video*' en tu terminal para confirmar los índices.\n",
        "camera_index_1 = 0 # <--- ÍNDICE DE LA PRIMERA CÁMARA\n",
        "camera_index_2 = 1 # <--- ÍNDICE DE LA SEGUNDA CÁMARA\n",
        "\n",
        "# --- Define la ruta a tu archivo de modelo .tflite ---\n",
        "# !!! IMPORTANTE: Reemplaza esta ruta con la ruta real donde guardaste\n",
        "# el archivo best.tflite (o como le hayas llamado) en tu Raspberry Pi. !!!\n",
        "# Por ejemplo: '/home/pi/robot_vision/var.tflite'\n",
        "best_model_path_tflite = '/path/to/your/best.tflite' # <--- CAMBIA ESTO A TU RUTA REAL\n",
        "\n",
        "# --- Define la ruta a tu archivo classes.txt ---\n",
        "# Necesitas un archivo de texto con los nombres de tus clases, uno por línea,\n",
        "# en el mismo orden que usaste para entrenar ('balon', 'linea_meta', 'travesaño', etc.).\n",
        "# Por ejemplo: '/home/pi/robot_vision/classes.txt'\n",
        "classes_file_path = '/path/to/your/classes.txt' # <--- CAMBIA ESTO A LA RUTA REAL\n",
        "\n",
        "# Define un umbral de confianza para las detecciones.\n",
        "# Las detecciones con confianza por debajo de este umbral serán ignoradas.\n",
        "confidence_threshold = 0.5 # <--- Ajusta según necesites (ej: 0.6, 0.7)\n",
        "\n",
        "# !!! IMPORTANTE: Define los IDs de clase para tus clases ('balon', 'linea_meta', 'travesaño') !!!\n",
        "# Estos IDs deben coincidir con el orden de tus clases en classes.txt y tu dataset.yaml.\n",
        "# Si 'balon' es la primera clase, su ID es 0. Si 'línea_meta' es la segunda, su ID es 1, etc.\n",
        "valid_ball_class_ids = [0] # <--- CAMBIA ESTO A LA LISTA DE IDS DE CLASE PARA TUS BALONES VALIDOS (ej: [0, 1, 2] si tienes varios tipos de balones)\n",
        "linea_meta_class_id = 1 # <--- CAMBIA ESTO AL ID DE CLASE PARA 'línea_de_meta'\n",
        "travesano_class_id = 2 # <--- CAMBIA ESTO AL ID DE CLASE PARA 'travesaño'\n",
        "\n",
        "# !!! IMPORTANTE: Define las coordenadas para la Región de Interés (ROI) del arco [x1, y1, x2, y2] !!!\n",
        "# Estas coordenadas definen un rectángulo en píxeles del frame de la cámara que abarca el área del arco.\n",
        "# Necesitarás determinarlas experimentalmente mirando la transmisión de tu cámara y tomando nota de las coordenadas.\n",
        "# Ejemplo: [x_min, y_min, x_max, y_max]\n",
        "# NOTA: Si las dos cámaras ven partes diferentes del arco, podrías necesitar ROIs separadas\n",
        "# o una lógica más compleja. Por ahora, asumimos una ROI que aplica a las detecciones\n",
        "# independientemente de la cámara de origen, o que la ROI se define en un espacio de coordenadas común\n",
        "# si las cámaras están calibradas. Para este ejemplo, usaremos una ROI única.\n",
        "goal_roi_coords = [100, 200, 500, 400] # <--- AJUSTA ESTAS COORDENADAS A TU SETUP\n",
        "print(f\"Región de Interés (ROI) del arco definida como: {goal_roi_coords}\")\n",
        "\n",
        "# Define la orientación de la línea de meta y la dirección en la que se considera gol.\n",
        "# !!! IMPORTANTE: Ajusta estos basados en la orientación real de tu línea de meta en la imagen\n",
        "# y la dirección en la que la pelota debe cruzar para ser gol. !!!\n",
        "# Ejemplo: Si la línea de meta es horizontal y un gol es cuando la pelota pasa por debajo de ella:\n",
        "goal_line_orientation = 'horizontal' # 'horizontal' o 'vertical'\n",
        "crossing_direction = 'below' # Para línea horizontal: 'above' o 'below'. Para línea vertical: 'left' o 'right'.\n",
        "\n",
        "\n",
        "# --- INICIALIZACIÓN ---\n",
        "\n",
        "# --- Carga los nombres de las clases desde el archivo classes.txt ---\n",
        "try:\n",
        "    with open(classes_file_path, 'r') as f:\n",
        "        class_names = [line.strip() for line in f.readlines()]\n",
        "    print(f\"Nombres de clases cargados exitosamente desde: {classes_file_path}\")\n",
        "    print(f\"Clases detectadas: {class_names}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar el archivo de clases {classes_file_path}: {e}\")\n",
        "    print(\"Asegúrate de que el archivo classes.txt exista y la ruta sea correcta en tu Raspberry Pi.\")\n",
        "    exit() # Sal del script si no se pueden cargar las clases.\n",
        "\n",
        "\n",
        "# --- Carga el modelo TensorFlow Lite (.tflite) ---\n",
        "try:\n",
        "    # Carga el modelo .tflite y obtiene el intérprete.\n",
        "    interpreter = tflite.Interpreter(model_path=best_model_path_tflite)\n",
        "\n",
        "    # Asigna los tensores (input y output). Esto configura el modelo para la inferencia.\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    # Obtiene información sobre los tensores de entrada y salida del modelo.\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    # Obtiene el tamaño de entrada esperado por el modelo (altura, ancho)\n",
        "    # input_details[0]['shape'] da la forma del tensor de entrada, usualmente [1, height, width, 3]\n",
        "    input_shape = input_details[0]['shape']\n",
        "    input_height = input_shape[1]\n",
        "    input_width = input_shape[2]\n",
        "\n",
        "    # Determina el tipo de datos de entrada esperado por el modelo (uint8 o float32)\n",
        "    floating_model = input_details[0]['dtype'] == np.float32\n",
        "\n",
        "    print(f\"Modelo TFLite cargado exitosamente desde: {best_model_path_tflite}\")\n",
        "    print(f\"Tamaño de entrada esperado por el modelo: {input_width}x{input_height}\")\n",
        "    print(f\"El modelo espera datos de tipo: {input_details[0]['dtype']}\")\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar el modelo TFLite: {e}\")\n",
        "    print(f\"Por favor, verifica que la ruta '{best_model_path_tflite}' sea correcta y que el archivo best.tflite exista en tu Raspberry Pi.\")\n",
        "    print(\"Asegúrate también de haber instalado 'tflite-runtime' (`pip install tflite-runtime`).\")\n",
        "    exit() # Sal del script si no se pudo cargar el modelo TFLite.\n",
        "\n",
        "\n",
        "# Inicializa la captura de video desde AMBAS cámaras.\n",
        "cap1 = cv2.VideoCapture(camera_index_1)\n",
        "cap2 = cv2.VideoCapture(camera_index_2)\n",
        "\n",
        "\n",
        "# Verifica si AMBAS cámaras se abrieron correctamente.\n",
        "if not cap1.isOpened():\n",
        "    print(f\"Error: No se pudo abrir la Cámara 1 con índice {camera_index_1}.\")\n",
        "    print(\"Por favor, verifica si la cámara está conectada y el índice es correcto (`ls /dev/video*`).\")\n",
        "    exit() # Sal del script si no se pudo abrir la primera cámara.\n",
        "if not cap2.isOpened():\n",
        "    print(f\"Error: No se pudo abrir la Cámara 2 con índice {camera_index_2}.\")\n",
        "    print(\"Por favor, verifica si la cámara está conectada y el índice es correcto (`ls /dev/video*`).\")\n",
        "    exit() # Sal del script si no se pudo abrir la segunda cámara.\n",
        "else:\n",
        "    print(f\"Cámara 1 con índice {camera_index_1} y Cámara 2 con índice {camera_index_2} abiertas exitosamente.\")\n",
        "\n",
        "\n",
        "# --- Configuración de RPi.GPIO para la acción de gol (Ej: LED) ---\n",
        "# !!! IMPORTANTE: Define el pin GPIO que usarás para tu acción (ej: un LED) !!!\n",
        "led_pin = 17 # <--- CAMBIA ESTE NÚMERO AL PIN GPIO REAL QUE USAS (Numeración BCM) !!!\n",
        "try:\n",
        "    GPIO.setmode(GPIO.BCM) # Usa numeración BCM\n",
        "    GPIO.setup(led_pin, GPIO.OUT) # Configura el pin como salida\n",
        "    GPIO.output(led_pin, GPIO.LOW) # Asegúrate de que la acción esté inactiva al inicio (ej: LED apagado)\n",
        "    print(f\"RPi.GPIO configurado para el pin {led_pin}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error configurando RPi.GPIO: {e}\")\n",
        "    print(\"Asegúrate de estar ejecutando en una Raspberry Pi, tener la librería rpi.gpio instalada y los permisos adecuados (puede requerir 'sudo').\")\n",
        "    # Considera cómo manejar este error si la configuración de GPIO es crítica.\n",
        "    # En un script de producción, podrías querer salir si falla la configuración.\n",
        "\n",
        "\n",
        "# Variable para seguir el estado del gol (para activar la acción solo una vez por evento)\n",
        "goal_detected_state = False\n",
        "\n",
        "# --- Función para la acción de gol ---\n",
        "def activate_goal_action():\n",
        "    \"\"\"\n",
        "    Función que realiza la acción cuando se detecta un gol.\n",
        "    En este ejemplo, enciende un LED por un corto tiempo.\n",
        "    !!! REEMPLAZA ESTO CON TU ACCIÓN ESPECÍFICA DE ROBOT !!!\n",
        "    \"\"\"\n",
        "    print(\"¡¡¡ GOL DETECTADO !!! Activando acción...\")\n",
        "    try:\n",
        "        # --- Ejemplo: Controlar un LED ---\n",
        "        GPIO.output(led_pin, GPIO.HIGH) # Enciende el LED\n",
        "        time.sleep(1) # Mantén encendido por 1 segundo (ajusta el tiempo)\n",
        "        GPIO.output(led_pin, GPIO.LOW) # Apaga el LED\n",
        "        print(\"Acción de gol completada (LED apagado).\")\n",
        "\n",
        "        # --- Otros ejemplos de acciones: ---\n",
        "        # - Enviar un comando a un motor\n",
        "        # - Reproducir un sonido\n",
        "        # - Enviar un mensaje por red\n",
        "        # etc.\n",
        "        # Elimina o comenta el código del LED y añade tu propia lógica aquí.\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error durante la ejecución de la acción de gol: {e}\")\n",
        "        # Maneja errores si tu acción de hardware falla.\n",
        "\n",
        "\n",
        "# --- BUCLE PRINCIPAL: Captura, Detección y Lógica de Gol ---\n",
        "\n",
        "    while True:\n",
        "        # Lee un frame de CADA cámara.\n",
        "        ret1, frame1 = cap1.read()\n",
        "        ret2, frame2 = cap2.read()\n",
        "\n",
        "        # Si no se pudo leer frame de ALGUNA cámara, rompe el bucle.\n",
        "        if not ret1:\n",
        "            print(\"Error: No se pudo leer frame de Cámara 1. Saliendo del bucle de captura.\")\n",
        "            break\n",
        "        if not ret2:\n",
        "            print(\"Error: No se pudo leer frame de Cámara 2. Saliendo del bucle de captura.\")\n",
        "            break\n",
        "\n",
        "        # --- AHORA TIENES frame1 y frame2, los frames de cada cámara ---\n",
        "        # El siguiente paso (Paso 2 del plan) es procesar estos frames para la detección.\n",
        "\n",
        "        # --- Pre-procesamiento del frame (ejemplo con frame1) ---\n",
        "        # Deberás hacer esto para frame1 Y frame2.\n",
        "        # input_frame1 = cv2.resize(frame1, (input_width, input_height))\n",
        "        # ... pre-procesamiento para input_data1 ...\n",
        "\n",
        "        # input_frame2 = cv2.resize(frame2, (input_width, input_height))\n",
        "        # ... pre-procesamiento para input_data2 ...\n",
        "\n",
        "        # --- Ejecutar inferencia (ejemplo con frame1) ---\n",
        "        # Deberás ejecutar la inferencia para CADA frame.\n",
        "        # interpreter.set_tensor(input_details[0]['index'], input_data1)\n",
        "        # interpreter.invoke()\n",
        "        # results1 = ... obtener resultados de interpreter ...\n",
        "\n",
        "        # interpreter.set_tensor(input_details[0]['index'], input_data2)\n",
        "        # interpreter.invoke()\n",
        "        # results2 = ... obtener resultados de interpreter ...\n",
        "\n",
        "\n",
        "        # --- Procesamiento de los resultados de detección (ejemplo) ---\n",
        "        # Deberás procesar results1 y results2 para obtener las detecciones de cada cámara.\n",
        "        # valid_ball_detections1 = []\n",
        "        # linea_meta_detecciones1 = []\n",
        "        # travesano_detections1 = []\n",
        "\n",
        "        # valid_ball_detections2 = []\n",
        "        # linea_meta_detecciones2 = []\n",
        "        # travesano_detections2 = []\n",
        "\n",
        "        # ... iterar sobre results1 y results2 para llenar las listas de detecciones ...\n",
        "\n",
        "\n",
        "        # --- Lógica de Detección de \"Gol\" (Paso 3 del plan) ---\n",
        "        # Esta lógica ahora deberá considerar las detecciones de AMBAS cámaras.\n",
        "        # current_frame_goal_detected = False\n",
        "        # if (valid_ball_detections1 or valid_ball_detections2) and (linea_meta_detecciones1 or linea_meta_detecciones2) and (travesano_detections1 or travesano_detections2):\n",
        "        #     ... implementar lógica de gol con detecciones de ambas cámaras ...\n",
        "\n",
        "\n",
        "        # --- Activar Acción de Gol (Paso 4 del plan) ---\n",
        "        # if current_frame_goal_detected and not goal_detected_state:\n",
        "        #      activate_goal_action()\n",
        "        #      goal_detected_state = True\n",
        "        # elif not current_frame_goal_detected and goal_detected_state:\n",
        "        #      goal_detected_state = False\n",
        "\n",
        "\n",
        "        # --- Opcional: Visualización (si aplica) ---\n",
        "        # Si quieres visualizar, deberás decidir cómo mostrar frame1 y frame2 (lado a lado, etc.).\n",
        "        # cv2.imshow('Camara 1', frame1)\n",
        "        # cv2.imshow('Camara 2', frame2)\n",
        "        # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        #      break\n",
        "\n",
        "\n",
        "    # --- LIMPIEZA FINAL ---\n",
        "\n",
        "    # Libera los objetos de captura de video de AMBAS cámaras\n",
        "    cap1.release()\n",
        "    cap2.release()\n",
        "\n",
        "    # Destruye cualquier ventana de OpenCV (si se usó cv2.imshow)\n",
        "    # cv2.destroyAllWindows()\n",
        "\n",
        "    # --- Limpieza de GPIO ---\n",
        "    try:\n",
        "        GPIO.cleanup() # Limpia la configuración de los pines GPIO\n",
        "        print(\"Limpieza de GPIO realizada.\")\n",
        "    except Exception as e:\n",
        "         print(f\"Error durante la limpieza de GPIO: {e}\")\n",
        "\n",
        "\n",
        "    print(\"Sistema de detección de gol detenido.\")\n",
        "\n",
        "# --- INSTRUCCIONES PARA CONTINUAR ---\n",
        "# 1. Guarda el código completo anterior en un archivo Python en tu Raspberry Pi (ej: dual_cam_goal_system.py).\n",
        "# 2. !!! EDITA LAS LÍNEAS EN LA SECCIÓN \"CONFIGURACIONES\" con tus rutas de archivo, IDs de clase, ROI, orientación de línea, dirección de cruce, y pin GPIO. !!!\n",
        "# 3. !!! REEMPLAZA EL CONTENIDO DE LA FUNCIÓN activate_goal_action() con tu código de acción específico. !!!\n",
        "# 4. El código actual solo lee los frames de ambas cámaras. El siguiente paso es añadir la lógica para ejecutar la detección en cada frame (Paso 2 del plan).\n",
        "# 5. Luego, deberás adaptar la lógica de gol para usar las detecciones de ambas cámaras (Paso 3 del plan).\n",
        "# 6. Finalmente, asegúrate de que la acción de gol se active correctamente (Paso 4 del plan) y considera la optimización (Paso 5).\n",
        "# 7. Ejecuta el script con `sudo python3 dual_cam_goal_system.py` (si necesitas acceso a GPIO o cámara)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecae764a"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the necessary libraries (`cv2` and `YOLO`) were commented out and thus not imported. I need to uncomment the import statements for `cv2`, `time`, `ultralytics`, and `numpy` to ensure the script can run and the model can be loaded (even if the dummy path causes a file not found error, the library itself will be available).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c80afd2d"
      },
      "source": [
        "# Informe del Proyecto: Detección de Gol con YOLOv8 en Raspberry Pi\n",
        "\n",
        "Este informe resume el proyecto desarrollado en este cuaderno de Google Colab, cuyo objetivo es implementar un sistema de detección de una pelota y una línea de meta utilizando un modelo YOLOv8 entrenado en una Raspberry Pi con una cámara, y activar una acción cuando la pelota cruce la línea de meta.\n",
        "\n",
        "## 1. Resumen del Proyecto\n",
        "\n",
        "El proyecto busca crear un sistema de visión artificial embebido en una Raspberry Pi 4 que pueda detectar objetos específicos ('balon', 'linea_meta', 'travesaño') en tiempo real a través de una cámara. La aplicación principal es determinar si un \"gol\" ha sido anotado, basándose en la posición del balón en relación con la línea de meta y el travesaño, y activar una respuesta física (como encender un LED) cuando se cumplan las condiciones de gol.\n",
        "\n",
        "## 2. Preparación del Dataset\n",
        "\n",
        "Esta fase implica la organización y preparación de las imágenes y sus anotaciones correspondientes en formato YOLO (`.txt`).\n",
        "\n",
        "*   **Organización de Archivos**: Las imágenes (`.jpg`) y sus archivos de anotación (`.txt`) deben estar en un directorio de origen específico.\n",
        "*   **División del Dataset**: Se implementó una función (`img_train_test_split`) para dividir automáticamente el conjunto total de imágenes y anotaciones en subdirectorios separados para entrenamiento (`train`), validación (`valid`) y prueba (`test`) con porcentajes definidos (80%, 10%, 10% respectivamente).\n",
        "*   **Verificación**: Se proporcionaron instrucciones para verificar el contenido de las carpetas `train`, `valid` y `test` para asegurar que la división se realizó correctamente.\n",
        "\n",
        "## 3. Configuración del Entorno de Entrenamiento\n",
        "\n",
        "Antes de entrenar el modelo, se necesita configurar el entorno con las herramientas necesarias.\n",
        "\n",
        "*   **Instalación de Ultralytics**: Se instaló la librería `ultralytics` utilizando `pip`, la cual proporciona la implementación de YOLOv8.\n",
        "*   **Creación del Archivo `dataset.yaml`**: Se generó un archivo de configuración en formato YAML (`dataset.yaml`) que especifica las rutas a los directorios de entrenamiento, validación y prueba, el número de clases (`nc`), y los nombres de las clases (`names`). Se incluyó una nota importante para el usuario sobre la necesidad de editar este archivo para definir correctamente el número y los nombres de las clases según su dataset.\n",
        "\n",
        "## 4. Entrenamiento del Modelo YOLOv8\n",
        "\n",
        "El entrenamiento del modelo de detección de objetos es una fase crucial.\n",
        "\n",
        "*   **Entrenamiento Inicial**: Se definió un comando `yolo train` especificando la ruta al archivo `dataset.yaml`, el modelo base de YOLOv8 a utilizar (`yolov8n.pt` en el ejemplo), el número de épocas, el tamaño de la imagen y el tamaño del batch. Se configuró una carpeta de proyecto y nombre de ejecución para organizar los resultados del entrenamiento.\n",
        "*   **Reanudar Entrenamiento (Checkpoint)**: Se proporcionó un comando para retomar el entrenamiento desde un archivo de checkpoint existente (`last.pt` o `best.pt`) utilizando el argumento `resume` y especificando la ruta al archivo del modelo. Esto permite continuar el entrenamiento si se interrumpe o añadir más épocas.\n",
        "*   **Nuevo Entrenamiento desde un Modelo Existente**: Se mostró cómo iniciar un nuevo entrenamiento utilizando un modelo `.pt` previamente entrenado como punto de partida, especificando la ruta a ese modelo en el comando `yolo train`.\n",
        "\n",
        "## 5. Implementación en Raspberry Pi\n",
        "\n",
        "Esta fase se centra en llevar el modelo entrenado al entorno de ejecución en la Raspberry Pi.\n",
        "\n",
        "*   **Transferencia del Modelo**: Se indicó la necesidad de copiar el archivo del modelo entrenado (`best.pt`) a la Raspberry Pi.\n",
        "*   **Configuración del Entorno en la Raspberry Pi**: Se proporcionaron instrucciones detalladas (comandos de terminal) para instalar las librerías necesarias en la Raspberry Pi, incluyendo `ultralytics`, `opencv-python`, `tflite-runtime` (para modelos TFLite) y `rpi.gpio` (para control de GPIO). Se enfatizó la importancia de instalar una versión compatible de PyTorch o TensorFlow Lite para la arquitectura ARM de la Raspberry Pi.\n",
        "*   **Conexión de la Cámara**: Se dieron instrucciones para verificar que la cámara esté correctamente conectada y sea detectada por el sistema operativo de la Raspberry Pi, incluyendo el uso de comandos como `ls /dev/video*` y herramientas como `fswebcam`.\n",
        "*   **Script de Captura de Video**: Se desarrolló un script básico de Python utilizando `cv2.VideoCapture` para capturar frames de video desde la cámara de la Raspberry Pi.\n",
        "\n",
        "## 6. Lógica de Detección de Gol\n",
        "\n",
        "Esta fase integra el modelo entrenado para detectar objetos en tiempo real y aplica la lógica para determinar un gol.\n",
        "\n",
        "*   **Carga del Modelo y Clases**: Se modificó el script de captura de video para cargar el modelo YOLOv8 entrenado (`.pt` o `.tflite`) y los nombres de las clases desde un archivo (`classes.txt`). Se incluyeron manejadores de errores en caso de que los archivos no se encuentren.\n",
        "*   **Procesamiento de Detecciones**: Dentro del bucle de video, se ejecuta la inferencia del modelo en cada frame. Se extraen las bounding boxes, confianzas y IDs de clase para las detecciones. Se aplica un umbral de confianza para filtrar las detecciones de baja calidad.\n",
        "*   **Identificación de Objetos Relevantes**: Las detecciones se filtran para identificar específicamente la pelota ('balon'), la línea de meta ('linea_meta') y el travesaño ('travesaño') basándose en sus IDs de clase.\n",
        "*   **Definición de la Región de Interés (ROI)**: Se introdujo el concepto de una ROI para el área del arco y se incluyó código para verificar si el centro de las detecciones relevantes cae dentro de esta ROI.\n",
        "*   **Lógica de Cruce de Línea y Posición del Balón**: Se implementó la lógica para determinar si la pelota ha cruzado la línea de meta, considerando la orientación de la línea y la dirección del cruce. Además, se verifica si la pelota se encuentra por debajo del travesaño.\n",
        "*   **Condición de Gol**: Se combinan las condiciones (balón dentro de la ROI, cruce de línea, balón por debajo del travesaño) para determinar si se ha anotado un gol en el frame actual.\n",
        "\n",
        "## 7. Activación de la Acción de Gol\n",
        "\n",
        "Cuando se detecta un gol, se activa una acción predefinida.\n",
        "\n",
        "*   **Función de Acción**: Se creó una función placeholder (`activate_goal_action`) para encapsular la acción que se realizará al detectar un gol.\n",
        "*   **Integración con Hardware (Ej: LED)**: Se incluyó un ejemplo de cómo usar la librería `RPi.GPIO` para controlar un pin GPIO y encender un LED durante un corto período de tiempo cuando se llama a la función `activate_goal_action`. Se proporcionaron instrucciones sobre la configuración de GPIO y la limpieza de los pines.\n",
        "*   **Manejo del Estado de Gol**: Se implementó una lógica simple para asegurar que la acción de gol se active solo una vez por evento de cruce, evitando múltiples activaciones mientras la pelota permanece cruzando la línea.\n",
        "\n",
        "## 8. Optimización del Rendimiento\n",
        "\n",
        "Para asegurar una detección en tiempo real fluida en la Raspberry Pi, se consideraron opciones de optimización.\n",
        "\n",
        "*   **Evaluación del Rendimiento**: Se indicó la importancia de medir el rendimiento (FPS) del script en la Raspberry Pi antes de aplicar optimizaciones.\n",
        "*   **Uso de Modelos Cuantizados (TensorFlow Lite)**: Se proporcionó una adaptación del script para utilizar un modelo convertido a formato TensorFlow Lite (`.tflite`), que a menudo ofrece mejor rendimiento en dispositivos embebidos como la Raspberry Pi. Se incluyeron los pasos para cargar el modelo TFLite y procesar sus salidas.\n",
        "*   **Ajuste de Parámetros de YOLOv8**: Se mencionó la posibilidad de ajustar parámetros de inferencia como el umbral de confianza para mejorar el rendimiento.\n",
        "*   **Optimización de PyTorch/TensorFlow**: Se hizo referencia a la necesidad de usar versiones optimizadas de las librerías de aprendizaje profundo para la arquitectura ARM si se opta por PyTorch en lugar de TFLite.\n",
        "\n",
        "Este informe cubre las principales etapas y componentes del proyecto de detección de gol en la Raspberry Pi utilizando YOLOv8."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "518571c5"
      },
      "source": [
        "# Informe del Proyecto: Detección de Gol con YOLOv8 en Raspberry Pi\n",
        "\n",
        "Este informe resume el proyecto desarrollado en este cuaderno de Google Colab, cuyo objetivo es implementar un sistema de detección de una pelota y una línea de meta utilizando un modelo YOLOv8 entrenado en una Raspberry Pi con una cámara, y activar una acción cuando la pelota cruce la línea de meta.\n",
        "\n",
        "## 1. Resumen del Proyecto\n",
        "\n",
        "El proyecto busca crear un sistema de visión artificial embebido en una Raspberry Pi 4 que pueda detectar objetos específicos ('balon', 'linea_meta', 'travesaño') en tiempo real a través de una cámara. La aplicación principal es determinar si un \"gol\" ha sido anotado, basándose en la posición del balón en relación con la línea de meta y el travesaño, y activar una respuesta física (como encender un LED) cuando se cumplan las condiciones de gol.\n",
        "\n",
        "## 2. Preparación del Dataset\n",
        "\n",
        "Esta fase implica la organización y preparación de las imágenes y sus anotaciones correspondientes en formato YOLO (`.txt`).\n",
        "\n",
        "*   **Organización de Archivos**: Las imágenes (`.jpg`) y sus archivos de anotación (`.txt`) deben estar en un directorio de origen específico.\n",
        "*   **División del Dataset**: Se implementó una función (`img_train_test_split`) para dividir automáticamente el conjunto total de imágenes y anotaciones en subdirectorios separados para entrenamiento (`train`), validación (`valid`) y prueba (`test`) con porcentajes definidos (80%, 10%, 10% respectivamente)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "341ba59f"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the base path where the dataset was created in Google Drive\n",
        "output_base_path = '/content/drive/My Drive/YoloV8'\n",
        "dataset_path = os.path.join(output_base_path, 'dataset')\n",
        "\n",
        "# Define the paths to the train, valid, and test subdirectories\n",
        "train_path = os.path.join(dataset_path, 'train')\n",
        "valid_path = os.path.join(dataset_path, 'valid')\n",
        "test_path = os.path.join(dataset_path, 'test')\n",
        "\n",
        "print(f\"Contents of {train_path}:\")\n",
        "if os.path.exists(train_path):\n",
        "    print(os.listdir(train_path))\n",
        "else:\n",
        "    print(\"Train directory not found.\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "print(f\"Contents of {valid_path}:\")\n",
        "if os.path.exists(valid_path):\n",
        "    print(os.listdir(valid_path))\n",
        "else:\n",
        "    print(\"Validation directory not found.\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "print(f\"Contents of {test_path}:\")\n",
        "if os.path.exists(test_path):\n",
        "    print(os.listdir(test_path))\n",
        "else:\n",
        "    print(\"Test directory not found.\")\n",
        "print(\"-\" * 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca42757f"
      },
      "source": [
        "# Instalar la librería ultralytics que contiene YOLOv8\n",
        "%pip install ultralytics -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51ab52de"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the base path where the dataset was created in Google Drive\n",
        "output_base_path = '/content/drive/My Drive/YoloV8'\n",
        "dataset_path = os.path.join(output_base_path, 'dataset')\n",
        "\n",
        "# Define the paths to the train and valid subdirectories relative to the dataset path\n",
        "train_dir_relative = 'train'\n",
        "valid_dir_relative = 'valid'\n",
        "\n",
        "# Define the number of classes\n",
        "# !!! IMPORTANT: Change this to the actual number of classes in your dataset !!!\n",
        "num_classes = 1 # Example: change to 2, 3, etc. based on your dataset\n",
        "\n",
        "# Define the names of your classes in a list, in the correct order\n",
        "# !!! IMPORTANT: Replace 'class1', 'class2', etc. with your actual class names !!!\n",
        "class_names = ['your_class_name'] # Example: ['person', 'car', 'bus']\n",
        "\n",
        "# Define the path for the dataset.yaml file\n",
        "yaml_file_path = os.path.join(output_base_path, 'dataset.yaml')\n",
        "\n",
        "# Create the content for the dataset.yaml file\n",
        "yaml_content = f\"\"\"\n",
        "path: {dataset_path}  # Dataset root directory\n",
        "train: {train_dir_relative}  # Train images (relative to 'path')\n",
        "val: {valid_dir_relative}  # Val images (relative to 'path')\n",
        "test: test # Test images (optional)\n",
        "\n",
        "nc: {num_classes}  # Number of classes\n",
        "names: {class_names}  # Class names\n",
        "\"\"\"\n",
        "\n",
        "# Write the content to the dataset.yaml file in Google Drive\n",
        "with open(yaml_file_path, 'w') as f:\n",
        "    f.write(yaml_content)\n",
        "\n",
        "print(f\"Created dataset.yaml file at: {yaml_file_path}\")\n",
        "print(\"Please edit this file in Google Drive to set the correct number of classes (nc) and class names (names).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "614a0547"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the path to your dataset.yaml file in Google Drive\n",
        "# ASEGÚRATE DE QUE ESTA RUTA SEA CORRECTA\n",
        "yaml_path = '/content/drive/My Drive/YoloV8/dataset.yaml' # <-- ASEGÚRATE DE QUE ESTA RUTA SEA CORRECTA\n",
        "\n",
        "# Define the base YOLOv8 model you want to use\n",
        "# Puedes cambiar 'yolov8s.pt' por 'yolov8m.pt', 'yolov8l.pt', 'yolov8x.pt', etc.\n",
        "model_base = 'yolov8n.pt'\n",
        "\n",
        "# Define training parameters\n",
        "epochs = 50 # Puedes ajustar el número de épocas\n",
        "img_size = 640 # Puedes ajustar el tamaño de la imagen\n",
        "batch_size = 16 # <--- Define el tamaño del batch aquí (ej: 16, 32, 64)\n",
        "\n",
        "# Define el directorio de salida para los resultados de este NUEVO entrenamiento\n",
        "# Esto creará '/content/drive/My Drive/YoloV8/testing/data' para los resultados del entrenamiento\n",
        "output_project_dir = '/content/drive/My Drive/YoloV8/testing'\n",
        "run_name = 'data' # Nombre de la subcarpeta para esta ejecución de entrenamiento\n",
        "\n",
        "# Construct the training command\n",
        "# Encerrar la ruta con espacios en comillas dobles\n",
        "train_command = f\"yolo train data=\\\"{yaml_path}\\\" model={model_base} epochs={epochs} imgsz={img_size} project=\\\"{output_project_dir}\\\" name=\\\"{run_name}\\\" batch={batch_size}\" # Añadimos project, name y batch\n",
        "\n",
        "print(f\"Generated training command: {train_command}\")\n",
        "\n",
        "# Para ejecutar este comando, puedes copiarlo y pegarlo en una celda de terminal de Colab\n",
        "# O puedes usar el magic command %run o ! para ejecutarlo directamente\n",
        "!{train_command}\n",
        "# %run -m ultralytics.yolo.engine.model train {train_command.split('train ')[1]} # Otra forma de ejecutarlo\n",
        "\n",
        "print(\"\\nCopy the command above and run it in a new cell (either code or terminal) to start training.\")\n",
        "print(\"Alternatively, uncomment the line `!{train_command}` above and run this cell.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3949d228"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define la ruta a tu archivo last.pt en Google Drive\n",
        "# Este archivo se crea automáticamente en la carpeta de resultados de tu entrenamiento anterior.\n",
        "# Debería estar en '/content/drive/My Drive/YoloV8/testing/data2/weights/last.pt'\n",
        "# Verifica la ubicación exacta de tu archivo last.pt.\n",
        "last_model_path = '/content/drive/My Drive/YoloV8/testing/data/weights/last.pt' # <-- ASEGÚRATE DE QUE ESTA RUTA SEA CORRECTA\n",
        "\n",
        "# Define la ruta a tu archivo dataset.yaml\n",
        "yaml_path = '/content/drive/My Drive/YoloV8/dataset.yaml' # <-- ASEGÚRATE DE QUE ESTA RUTA SEA CORRECTA\n",
        "\n",
        "# Define el número de épocas ADICIONALES para entrenar\n",
        "# Si tu entrenamiento anterior fue de 50 épocas y quieres entrenar 50 más, pon epochs = 50 aquí.\n",
        "epochs_to_add = 50 # Puedes ajustar el número de épocas adicionales\n",
        "\n",
        "# Define el número de épocas de paciencia para Early Stopping\n",
        "# El entrenamiento se detendrá si el rendimiento en validación no mejora durante este número de épocas.\n",
        "patience_epochs = 50 # <--- Ajusta este valor según necesites (ej: 50, 100)\n",
        "\n",
        "# El tamaño de imagen debe ser el mismo que usaste en el entrenamiento anterior\n",
        "img_size = 640 # Asegúrate de que sea el mismo valor que antes\n",
        "batch_size_add = 16 # <--- Define el tamaño del batch aquí (ej: 16, 32, 64)\n",
        "\n",
        "# La carpeta de proyecto y nombre del run deben ser los mismos para que retome el entrenamiento\n",
        "output_project_dir = '/content/drive/My Drive/YoloV8/testing'\n",
        "run_name = 'data' # Asegúrate de que sea el mismo nombre que el run anterior\n",
        "\n",
        "# Construye el comando para retomar el entrenamiento\n",
        "# Usamos el argumento 'resume' y especificamos la ruta al archivo last.pt\n",
        "# Encerrar las rutas con espacios en comillas dobles\n",
        "train_command_resume = f\"yolo train data=\\\"{yaml_path}\\\" model=\\\"{last_model_path}\\\" epochs={epochs_to_add} imgsz={img_size} project=\\\"{output_project_dir}\\\" name=\\\"{run_name}\\\" resume patience={patience_epochs}\\\" batch={batch_size_add}\" # Añadimos project, name y batch\n",
        "\n",
        "print(f\"Generated resume training command: {train_command_resume}\")\n",
        "\n",
        "# Para ejecutar este comando, puedes copiarlo y pegarlo en una celda de terminal de Colab\n",
        "# O puedes usar el magic command ! para ejecutarlo directamente\n",
        "\n",
        "# !!! DESCOMENTA LA SIGUIENTE LÍNEA PARA EJECUTAR EL COMANDO DE ENTRENAMIENTO !!!\n",
        "# !{train_command_resume}\n",
        "\n",
        "print(\"\\nCopy the command above and run it in a new cell (either code or terminal) to resume training.\")\n",
        "print(\"Alternatively, uncomment the line `!{train_command_resume}` above and run this cell.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "058ef332"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define la ruta a tu archivo dataset.yaml\n",
        "# ASEGÚRATE DE QUE ESTA RUTA SEA CORRECTA\n",
        "yaml_path = '/content/drive/My Drive/YoloV8/dataset.yaml' # <-- ASEGÚRATE DE QUE ESTA RUTA SEA CORRECTA\n",
        "\n",
        "# Define la ruta al modelo .pt con el que quieres empezar el NUEVO entrenamiento.\n",
        "# Puede ser tu best.pt de un entrenamiento anterior, o cualquier otro modelo .pt.\n",
        "# ASEGÚRATE DE QUE ESTA RUTA SEA CORRECTA\n",
        "starting_model_path = '/content/drive/My Drive/YoloV8/testing/data2/weights/best.pt' # <-- CAMBIA ESTO A LA RUTA DE TU MODELO .PT\n",
        "\n",
        "# Define los parámetros de entrenamiento para este NUEVO entrenamiento\n",
        "epochs = 100 # Puedes ajustar el número de épocas\n",
        "img_size = 640 # Puedes ajustar el tamaño de la imagen (debe coincidir con el modelo si no es un modelo base)\n",
        "batch_size = 16 # <--- Define el tamaño del batch aquí (ej: 16, 32, 64)\n",
        "\n",
        "# Define el directorio de salida para los resultados de este NUEVO entrenamiento\n",
        "# Es recomendable usar un nombre de carpeta diferente para no sobrescribir los resultados anteriores.\n",
        "output_project_dir = '/content/drive/My Drive/YoloV8/testing'\n",
        "run_name = 'data_retrain' # <-- CAMBIA ESTO A UN NOMBRE ÚNICO PARA ESTE ENTRENAMIENTO\n",
        "\n",
        "# Construye el comando para el NUEVO entrenamiento\n",
        "# Apuntamos al modelo .pt que queremos usar como punto de partida\n",
        "train_command_retrain = f\"yolo train data=\\\"{yaml_path}\\\" model=\\\"{starting_model_path}\\\" epochs={epochs} imgsz={img_size} project=\\\"{output_project_dir}\\\" name=\\\"{run_name}\\\" batch={batch_size}\"\n",
        "\n",
        "print(f\"Generated retraining command: {train_command_retrain}\")\n",
        "\n",
        "# Para ejecutar este comando, puedes copiarlo y pegarlo en una celda de terminal de Colab\n",
        "# O puedes usar el magic command ! para ejecutarlo directamente\n",
        "\n",
        "# !!! DESCOMENTA LA SIGUIENTE LÍNEA PARA EJECUTAR EL COMANDO DE ENTRENAMIENTO !!!\n",
        "# !{train_command_retrain}\n",
        "\n",
        "print(\"\\nCopy the command above and run it in a new cell (either code or terminal) to start the new training.\")\n",
        "print(\"Alternatively, uncomment the line `!{train_command_retrain}` above and run this cell.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4626f055"
      },
      "source": [
        "# Instructions for transferring the model to your Raspberry Pi:\n",
        "\n",
        "# Method 1: Using scp (Secure Copy Protocol) from your computer to Raspberry Pi\n",
        "# Open a terminal on your computer and run:\n",
        "# scp /path/to/your/local/best.pt pi@<raspberry_pi_ip_address>:/path/on/raspberry/pi/\n",
        "\n",
        "# Replace:\n",
        "# - /path/to/your/local/best.pt with the actual path to your best.pt file on your computer.\n",
        "# - <raspberry_pi_ip_address> with the IP address of your Raspberry Pi.\n",
        "# - /path/on/raspberry/pi/ with the desired directory path on your Raspberry Pi.\n",
        "# You will be prompted to enter the password for the 'pi' user on your Raspberry Pi.\n",
        "\n",
        "# Method 2: Using a USB drive\n",
        "# 1. Copy the best.pt file from your computer to a USB drive.\n",
        "# 2. Insert the USB drive into your Raspberry Pi.\n",
        "# 3. Open a terminal on your Raspberry Pi.\n",
        "# 4. The USB drive should be automatically mounted under /media/pi/<drive_name> or /mnt/<drive_name>.\n",
        "#    You can use the 'ls /media/pi' or 'ls /mnt' command to find the drive name.\n",
        "# 5. Copy the file from the USB drive to your desired location on the Raspberry Pi:\n",
        "#    cp /media/pi/<drive_name>/best.pt /path/on/raspberry/pi/\n",
        "\n",
        "# Method 3: Using cloud storage (like Google Drive, Dropbox, etc.)\n",
        "# 1. Upload the best.pt file to your preferred cloud storage service from your computer.\n",
        "# 2. On your Raspberry Pi, install the cloud storage client (if available) or use the web browser to download the file.\n",
        "#    For Google Drive, you might need to use tools like 'gdown' or the rclone command-line utility to download files from the terminal.\n",
        "\n",
        "# Remember the location where you save the best.pt file on your Raspberry Pi, as you will need this path later in your Python script."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9045e614"
      },
      "source": [
        "# Instructions for installing libraries on your Raspberry Pi terminal:\n",
        "\n",
        "# 1. Access the terminal on your Raspberry Pi.\n",
        "\n",
        "# 2. Update your system's package list and upgrade installed packages:\n",
        "# sudo apt update\n",
        "# sudo apt upgrade -y\n",
        "\n",
        "# 3. Install pip for Python 3 if it's not already installed:\n",
        "# sudo apt install python3-pip -y\n",
        "\n",
        "# 4. Install the ultralytics library using pip:\n",
        "# pip install ultralytics\n",
        "\n",
        "# 5. Install OpenCV Python bindings:\n",
        "# pip install opencv-python\n",
        "\n",
        "# 6. Install RPi.GPIO for controlling GPIO pins:\n",
        "# pip install rpi.gpio\n",
        "\n",
        "# 7. Install a compatible version of PyTorch or TensorFlow Lite.\n",
        "# This step is highly dependent on your specific Raspberry Pi model,\n",
        "# the operating system version, and the Python version.\n",
        "\n",
        "# Option 1: Attempt to install PyTorch (might require specific wheels or compiling)\n",
        "# Search for pre-compiled wheels for PyTorch on Raspberry Pi (ARM architecture)\n",
        "# Example (replace <version> and <python_version> with appropriate values):\n",
        "# pip install torch==<version>+cpu torchvision==<version>+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# or look for community-built wheels.\n",
        "\n",
        "# Option 2: Install TensorFlow Lite (often easier on Raspberry Pi)\n",
        "# Follow the official TensorFlow Lite installation guide for Raspberry Pi:\n",
        "# https://www.tensorflow.org/lite/guide/python#install_tensorflow_lite_for_python_on_raspberry_pi\n",
        "# This usually involves:\n",
        "# pip install tflite-runtime\n",
        "\n",
        "# 8. Verify the installations by trying to import the libraries in a Python 3 session on your Raspberry Pi:\n",
        "# python3\n",
        "# >>> import ultralytics\n",
        "# >>> import cv2\n",
        "# >>> import RPi.GPIO as GPIO # This will likely only work on a Raspberry Pi\n",
        "# >>> import torch # if you installed PyTorch\n",
        "# >>> import tensorflow.lite as tflite # if you installed TensorFlow Lite\n",
        "# >>> exit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ba40c85"
      },
      "source": [
        "# Instructions to be executed on your Raspberry Pi terminal:\n",
        "\n",
        "# Step 1: Verify camera detection\n",
        "# Open a terminal on your Raspberry Pi and run the following command to list video devices:\n",
        "# ls /dev/video*\n",
        "\n",
        "# Expected Output: You should see output similar to /dev/video0, /dev/video1, etc.\n",
        "# If you don't see any /dev/video* devices, try the following:\n",
        "# - Ensure the camera is securely connected to a USB port.\n",
        "# - Reboot your Raspberry Pi with the camera connected: sudo reboot\n",
        "# - Consult the camera's documentation (HT-SUA33GC-T1V-C) for specific driver installation or configuration steps if needed.\n",
        "\n",
        "# Step 2: Test the camera using fswebcam (optional but recommended)\n",
        "# If the camera is detected, you can test it by capturing an image.\n",
        "# First, install fswebcam if you don't have it:\n",
        "# sudo apt update\n",
        "# sudo apt install fswebcam -y\n",
        "\n",
        "# Then, capture a test image:\n",
        "# fswebcam test_image.jpg\n",
        "\n",
        "# Expected Output: A file named 'test_image.jpg' should be created in the directory where you ran the command.\n",
        "# You can view this image to confirm the camera is working.\n",
        "\n",
        "# If you encounter issues with fswebcam, you can also try testing with OpenCV in Python\n",
        "# (assuming you have OpenCV installed, which might be required for the next steps of the project).\n",
        "# Example Python code (run in a Python environment on your Pi):\n",
        "# import cv2\n",
        "# cap = cv2.VideoCapture(0) # 0 is typically the default camera\n",
        "# if not cap.isOpened():\n",
        "#     print(\"Error: Could not open camera.\")\n",
        "# else:\n",
        "#     ret, frame = cap.read()\n",
        "#     if ret:\n",
        "#         cv2.imwrite(\"test_opencv_image.jpg\", frame)\n",
        "#         print(\"Captured test_opencv_image.jpg\")\n",
        "#     else:\n",
        "#         print(\"Error: Could not read frame from camera.\")\n",
        "# cap.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11332d54"
      },
      "source": [
        "import cv2\n",
        "import time\n",
        "\n",
        "# Camera index (usually 0 for the default camera)\n",
        "# If you have multiple cameras, you might need to change this.\n",
        "# Use 'ls /dev/video*' in your Raspberry Pi terminal to list available video devices.\n",
        "camera_index = 0\n",
        "\n",
        "# Initialize video capture\n",
        "cap = cv2.VideoCapture(camera_index)\n",
        "\n",
        "# Check if the camera opened successfully\n",
        "if not cap.isOpened():\n",
        "    print(f\"Error: Could not open camera with index {camera_index}.\")\n",
        "    print(\"Please check if the camera is connected and the index is correct.\")\n",
        "else:\n",
        "    print(f\"Camera with index {camera_index} opened successfully.\")\n",
        "\n",
        "    # Loop to continuously read frames\n",
        "    while True:\n",
        "        # Read a frame from the camera\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # If the frame was not read successfully, break the loop\n",
        "        if not ret:\n",
        "            print(\"Error: Could not read frame.\")\n",
        "            break\n",
        "\n",
        "        # --- Optional: Display the frame (can be slow on RPi without optimization) ---\n",
        "        # cv2.imshow('Camera Feed', frame)\n",
        "\n",
        "        # --- Add a small delay to reduce CPU usage (optional) ---\n",
        "        # time.sleep(0.01) # Adjust delay as needed\n",
        "\n",
        "        # Break the loop if the 'q' key is pressed\n",
        "        # This part will only work if cv2.imshow is enabled and a window is active\n",
        "        # If not displaying, you might need a different mechanism to stop the script (e.g., Ctrl+C)\n",
        "        # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        #     break\n",
        "\n",
        "        # For a headless environment or to just test capture without display,\n",
        "        # you can remove the cv2.imshow and waitKey lines and rely on Ctrl+C to stop.\n",
        "        # Or add a simple counter to capture a few frames and then exit.\n",
        "        # For this subtask, we'll assume it runs until manually interrupted (e.g., Ctrl+C).\n",
        "        # In a real application, you would process the 'frame' here.\n",
        "\n",
        "        # Example: Print frame shape to confirm capture is working\n",
        "        print(f\"Captured frame shape: {frame.shape}\")\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()\n",
        "\n",
        "    # Destroy any OpenCV windows (if cv2.imshow was used)\n",
        "    # cv2.destroyAllWindows()\n",
        "\n",
        "    print(\"Video capture stopped.\")\n",
        "\n",
        "# Instructions on how to run this script on your Raspberry Pi:\n",
        "# 1. Save the code as a Python file (e.g., capture_video.py).\n",
        "# 2. Open a terminal on your Raspberry Pi.\n",
        "# 3. Navigate to the directory where you saved the file.\n",
        "# 4. Run the script using: python3 capture_video.py\n",
        "# 5. If you enabled cv2.imshow, press 'q' to exit the loop.\n",
        "# 6. If you did not enable cv2.imshow, press Ctrl+C in the terminal to stop the script."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "759e0774"
      },
      "source": [
        "# This code snippet demonstrates loading the YOLOv8 model and class names.\n",
        "# Note: The full script for real-time detection and logic is provided in later sections.\n",
        "\n",
        "import cv2\n",
        "import time\n",
        "from ultralytics import YOLO # Import the YOLO class\n",
        "import numpy as np\n",
        "import tflite_runtime.interpreter as tflite # Import for TFLite models\n",
        "\n",
        "# Define the path to your trained best.pt or best.tflite model file on your Raspberry Pi\n",
        "# !!! IMPORTANT: Make sure this path is correct on your Raspberry Pi !!!\n",
        "# Choose one based on the model format you are using:\n",
        "# best_model_path = '/path/to/your/best.pt' # For .pt model\n",
        "best_model_path_tflite = '/path/to/your/best.tflite' # For .tflite model <--- CHANGE THIS TO YOUR ACTUAL PATH\n",
        "\n",
        "# --- Define the path to your classes.txt file ---\n",
        "# This file should contain the names of your classes, one per line, in the correct order.\n",
        "classes_file_path = '/path/to/your/classes.txt' # <--- CHANGE THIS TO YOUR ACTUAL PATH\n",
        "\n",
        "# Load the class names from classes.txt\n",
        "try:\n",
        "    with open(classes_file_path, 'r') as f:\n",
        "        class_names = [line.strip() for line in f.readlines()]\n",
        "    print(f\"Class names loaded successfully from: {classes_file_path}\")\n",
        "    print(f\"Detected classes: {class_names}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading classes file {classes_file_path}: {e}\")\n",
        "    print(\"Please ensure the classes.txt file exists and the path is correct.\")\n",
        "    # exit() # Uncomment to exit if classes cannot be loaded\n",
        "\n",
        "# Load the YOLOv8 model (.pt or .tflite)\n",
        "try:\n",
        "    # If using a .pt model:\n",
        "    # model = YOLO(best_model_path)\n",
        "    # print(f\"YOLOv8 model loaded successfully from: {best_model_path}\")\n",
        "\n",
        "    # If using a .tflite model:\n",
        "    interpreter = tflite.Interpreter(model_path=best_model_path_tflite)\n",
        "    interpreter.allocate_tensors()\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    input_shape = input_details[0]['shape']\n",
        "    input_height = input_shape[1]\n",
        "    input_width = input_shape[2]\n",
        "    floating_model = input_details[0]['dtype'] == np.float32\n",
        "\n",
        "    print(f\"TFLite model loaded successfully from: {best_model_path_tflite}\")\n",
        "    print(f\"Model expects input size: {input_width}x{input_height}\")\n",
        "    print(f\"Model expects data type: {input_details[0]['dtype']}\")\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading the model: {e}\")\n",
        "    print(f\"Please verify that the model path is correct and the model file exists on your Raspberry Pi.\")\n",
        "    # exit() # Uncomment to exit if the model cannot be loaded\n",
        "\n",
        "# Note: The camera initialization and main loop follow this section in the complete script."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d7278b6"
      },
      "source": [
        "# This code snippet shows the detection processing and ROI filtering part within the main loop.\n",
        "# It assumes the model and camera are already initialized.\n",
        "\n",
        "# Inside the main video capture loop (while True:):\n",
        "\n",
        "# Read a frame from the camera\n",
        "ret, frame = cap.read()\n",
        "if not ret:\n",
        "    print(\"Error: Could not read frame. Exiting capture loop.\")\n",
        "    # break # Uncomment to break loop on error\n",
        "\n",
        "# --- Pre-process the frame for the TFLite model ---\n",
        "# If using a .pt model, you might not need explicit pre-processing here;\n",
        "# the model.predict() function handles it.\n",
        "# If using a .tflite model:\n",
        "input_frame = cv2.resize(frame, (input_width, input_height))\n",
        "if floating_model:\n",
        "    input_data = (np.float32(input_frame) - 127.5) / 127.5\n",
        "else:\n",
        "    input_data = np.uint8(input_frame)\n",
        "input_data = np.expand_dims(input_data, axis=0)\n",
        "\n",
        "# --- Run Inference ---\n",
        "# If using a .pt model:\n",
        "# results = model(frame, verbose=False) # Run inference\n",
        "\n",
        "# If using a .tflite model:\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "interpreter.invoke()\n",
        "output_boxes = interpreter.get_tensor(output_details[0]['index'])[0] # [ymin, xmin, ymax, xmax] normalized\n",
        "output_classes = interpreter.get_tensor(output_details[1]['index'])[0] # Class IDs\n",
        "output_scores = interpreter.get_tensor(output_details[2]['index'])[0] # Confidence scores\n",
        "\n",
        "\n",
        "# --- Process Detection Results ---\n",
        "\n",
        "valid_ball_detections = []\n",
        "linea_meta_detections = []\n",
        "travesano_detections = []\n",
        "\n",
        "frame_height, frame_width, _ = frame.shape\n",
        "\n",
        "# Define confidence threshold\n",
        "confidence_threshold = 0.5 # <--- Adjust as needed\n",
        "\n",
        "# Define class IDs (Make sure these match your classes.txt and dataset.yaml)\n",
        "valid_ball_class_ids = [0] # Example: Assuming 'balon' is class 0\n",
        "linea_meta_class_id = 1 # Example: Assuming 'linea_meta' is class 1\n",
        "travesano_class_id = 2 # Example: Assuming 'travesaño' is class 2\n",
        "\n",
        "# Define Goal ROI [x1, y1, x2, y2] (in pixel coordinates of the original frame size)\n",
        "# !!! IMPORTANT: Determine these coordinates experimentally for your setup !!!\n",
        "goal_roi_coords = [100, 200, 500, 400] # [x_min, y_min, x_max, y_max]\n",
        "\n",
        "\n",
        "# Iterate through detections\n",
        "# If using a .pt model:\n",
        "# if results and results[0].boxes:\n",
        "#     detections = results[0].boxes # Access the Boxes object\n",
        "\n",
        "#     for det in detections:\n",
        "#         bbox = det.xyxy[0].tolist() # [x1, y1, x2, y2] in pixel coordinates\n",
        "#         conf = det.conf[0].item()\n",
        "#         cls = int(det.cls[0].item())\n",
        "\n",
        "#         # Check confidence and if center is within ROI\n",
        "#         bbox_center_x = (bbox[0] + bbox[2]) / 2\n",
        "#         bbox_center_y = (bbox[1] + bbox[3]) / 2\n",
        "#         is_inside_roi = (bbox_center_x >= goal_roi_coords[0] and\n",
        "#                          bbox_center_x <= goal_roi_coords[2] and\n",
        "#                          bbox_center_y >= goal_roi_coords[1] and\n",
        "#                          bbox_center_y <= goal_roi_coords[3])\n",
        "\n",
        "#         if conf >= confidence_threshold and is_inside_roi:\n",
        "#             if cls in valid_ball_class_ids:\n",
        "#                 valid_ball_detections.append({'bbox': bbox, 'conf': conf, 'class_id': cls})\n",
        "#             elif cls == linea_meta_class_id:\n",
        "#                 linea_meta_detections.append({'bbox': bbox, 'conf': conf, 'class_id': cls})\n",
        "#             elif cls == travesano_class_id:\n",
        "#                 travesano_detections.append({'bbox': bbox, 'conf': conf, 'class_id': cls})\n",
        "\n",
        "# If using a .tflite model:\n",
        "for i in range(output_scores.shape[0]):\n",
        "    score = output_scores[i]\n",
        "    class_id = int(output_classes[i])\n",
        "    bbox_normalized = output_boxes[i]\n",
        "\n",
        "    if score >= confidence_threshold:\n",
        "        # Denormalize bbox coordinates\n",
        "        x1 = int(bbox_normalized[1] * frame_width)\n",
        "        y1 = int(bbox_normalized[0] * frame_height)\n",
        "        x2 = int(bbox_normalized[3] * frame_width)\n",
        "        y2 = int(bbox_normalized[2] * frame_height)\n",
        "\n",
        "        # Ensure coordinates are within frame bounds\n",
        "        x1 = max(0, x1)\n",
        "        y1 = max(0, y1)\n",
        "        x2 = min(frame_width - 1, x2)\n",
        "        y2 = min(frame_height - 1, y2)\n",
        "\n",
        "        bbox_pixel = [x1, y1, x2, y2]\n",
        "\n",
        "        # Check if the center of the bounding box is inside the ROI\n",
        "        bbox_center_x = (bbox_pixel[0] + bbox_pixel[2]) / 2\n",
        "        bbox_center_y = (bbox_pixel[1] + bbox_pixel[3]) / 2\n",
        "\n",
        "        is_inside_roi = (bbox_center_x >= goal_roi_coords[0] and\n",
        "                         bbox_center_x <= goal_roi_coords[2] and\n",
        "                         bbox_center_y >= goal_roi_coords[1] and\n",
        "                         bbox_center_y <= goal_roi_coords[3])\n",
        "\n",
        "        # Identify class and store if inside ROI\n",
        "        if is_inside_roi:\n",
        "             if class_id in valid_ball_class_ids:\n",
        "                 valid_ball_detections.append({'bbox': bbox_pixel, 'conf': score, 'class_id': class_id})\n",
        "             elif class_id == linea_meta_class_id:\n",
        "                 linea_meta_detections.append({'bbox': bbox_pixel, 'conf': score, 'class_id': class_id})\n",
        "             elif class_id == travesano_class_id:\n",
        "                 travesano_detections.append({'bbox': bbox_pixel, 'conf': score, 'class_id': class_id})\n",
        "\n",
        "\n",
        "# At this point, valid_ball_detections, linea_meta_detections, and travesano_detections\n",
        "# contain the filtered detections within the ROI."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9027517b"
      },
      "source": [
        "# This code snippet shows the goal logic implementation within the main loop.\n",
        "# It assumes the detection processing has already been done for the current frame.\n",
        "\n",
        "# Inside the main video capture loop (while True:):\n",
        "\n",
        "# --- Goal Logic Implementation ---\n",
        "current_frame_goal_detected = False\n",
        "\n",
        "# Need at least one detection of each RELEVANTE type to consider a goal\n",
        "# (Balón dentro de ROI, Línea de Meta dentro de ROI, Travesaño dentro de ROI)\n",
        "# We assume detections have already been filtered by ROI if that logic was applied earlier.\n",
        "if valid_ball_detections and linea_meta_detections and travesano_detections:\n",
        "    # For simplicity, take the first detection of each type\n",
        "    ball_det = valid_ball_detections[0]\n",
        "    linea_meta_det = linea_meta_detections[0]\n",
        "    travesano_det = travesano_detections[0]\n",
        "\n",
        "    ball_bbox = ball_det['bbox']\n",
        "    linea_meta_bbox = linea_meta_det['bbox']\n",
        "    travesano_bbox = travesano_det['bbox']\n",
        "\n",
        "    # Calculate points/lines for comparison\n",
        "    ball_center_x = (ball_bbox[0] + ball_bbox[2]) / 2\n",
        "    ball_center_y = (ball_bbox[1] + ball_bbox[3]) / 2\n",
        "    ball_bottom_center_y = ball_bbox[3] # Using the bottom edge of the ball for horizontal line crossing\n",
        "\n",
        "    # Define goal line orientation and crossing direction\n",
        "    # !!! IMPORTANT: Adjust these based on your setup and goal line orientation !!!\n",
        "    # Example: If the goal line is horizontal and crossing is when the ball goes below:\n",
        "    goal_line_orientation = 'horizontal' # 'horizontal' or 'vertical'\n",
        "    crossing_direction = 'below' # For horizontal: 'above' or 'below'; For vertical: 'left' or 'right'\n",
        "\n",
        "    # Calculate the relevant coordinate for the goal line based on orientation\n",
        "    if goal_line_orientation == 'horizontal':\n",
        "        line_y_pos = (linea_meta_bbox[1] + linea_meta_bbox[3]) / 2 # Center y of the line\n",
        "        # Or you can use the top or bottom edge of the line's bbox depending on how you annotated it\n",
        "    elif goal_line_orientation == 'vertical':\n",
        "        line_x_pos = (linea_meta_bbox[0] + linea_meta_bbox[2]) / 2 # Center x of the line\n",
        "        # Or you can use the left or right edge of the line's bbox\n",
        "\n",
        "    # Calculate the relevant coordinate for the crossbar (bottom edge)\n",
        "    travesano_bottom_y = travesano_bbox[3] # Using the bottom edge of the crossbar's bbox\n",
        "\n",
        "\n",
        "    # --- Check Conditions ---\n",
        "\n",
        "    # 1. Is the ball's center within the Goal ROI? (Already filtered if that logic was used before)\n",
        "    # If not filtered before, uncomment this part:\n",
        "    # is_inside_roi = (ball_center_x >= goal_roi_coords[0] and\n",
        "    #                  ball_center_x <= goal_roi_coords[2] and\n",
        "    #                  ball_center_y >= goal_roi_coords[1] and\n",
        "    #                  ball_center_y <= goal_roi_coords[3])\n",
        "\n",
        "\n",
        "    # 2. Has the ball crossed the goal line?\n",
        "    has_crossed_line = False\n",
        "    if goal_line_orientation == 'horizontal':\n",
        "        if crossing_direction == 'below' and ball_bottom_center_y > line_y_pos: # Check bottom of ball vs line y\n",
        "            has_crossed_line = True\n",
        "        elif crossing_direction == 'above' and ball_bbox[1] < line_y_pos: # Check top of ball vs line y\n",
        "             has_crossed_line = True\n",
        "    elif goal_line_orientation == 'vertical':\n",
        "        if crossing_direction == 'right' and ball_center_x > line_x_pos: # Check center x of ball vs line x\n",
        "             has_crossed_line = True\n",
        "        elif crossing_direction == 'left' and ball_center_x < line_x_pos: # Check center x of ball vs line x\n",
        "             has_crossed_line = True\n",
        "\n",
        "    # 3. Is the ball below the crossbar?\n",
        "    is_below_crossbar = (ball_bbox[1] < travesano_bottom_y) # Check top of ball vs bottom of crossbar\n",
        "\n",
        "    # --- Final Goal Condition ---\n",
        "    # If filtering detections by ROI was done before, the condition is only:\n",
        "    current_frame_goal_detected = has_crossed_line and is_below_crossbar\n",
        "    # If not filtering by ROI before, use:\n",
        "    # current_frame_goal_detected = is_inside_roi and has_crossed_line and is_below_crossbar\n",
        "\n",
        "\n",
        "    # For debugging, print state of conditions:\n",
        "    # print(f\"Frame: {int(time.time())}, ROI: {is_inside_roi}, Crossed Line: {has_crossed_line}, Below Crossbar: {is_below_crossbar}\")\n",
        "    # if current_frame_goal_detected:\n",
        "    #     print(\"Potential Goal Detected!\")\n",
        "\n",
        "# Note: The goal action activation and loop control follow this section."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73a2a2cb"
      },
      "source": [
        "# This code snippet shows the goal action function and its call within the main loop.\n",
        "\n",
        "import RPi.GPIO as GPIO # Import for controlling GPIO pins (on Raspberry Pi)\n",
        "import time # Import for time.sleep\n",
        "\n",
        "# !!! IMPORTANT: Define the GPIO pin number for your LED !!!\n",
        "led_pin = 17 # <--- CHANGE THIS NUMBER TO THE ACTUAL GPIO PIN YOU ARE USING FOR THE LED !!!\n",
        "\n",
        "# --- Configure RPi.GPIO at the beginning of the script (once) ---\n",
        "try:\n",
        "    GPIO.setmode(GPIO.BCM) # Use BCM numbering\n",
        "    GPIO.setup(led_pin, GPIO.OUT) # Configure the pin as an output\n",
        "    GPIO.output(led_pin, GPIO.LOW) # Ensure the LED is off initially\n",
        "    print(f\"RPi.GPIO configured for pin {led_pin}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error configuring RPi.GPIO: {e}\")\n",
        "    print(\"Please ensure you are running on a Raspberry Pi and have the necessary permissions.\")\n",
        "    # Consider how to handle this error if GPIO configuration is critical.\n",
        "\n",
        "\n",
        "# Placeholder function for goal action\n",
        "def activate_goal_action():\n",
        "    \"\"\"\n",
        "    Function to perform the action when a goal is detected.\n",
        "    This version turns on an LED for a short time.\n",
        "    \"\"\"\n",
        "    print(\"¡¡¡ GOL !!! Activando LED...\")\n",
        "    try:\n",
        "        # Turn on the LED\n",
        "        GPIO.output(led_pin, GPIO.HIGH)\n",
        "        # Keep the LED on for 1 second (adjust if needed)\n",
        "        time.sleep(1)\n",
        "        # Turn off the LED\n",
        "        GPIO.output(led_pin, GPIO.LOW)\n",
        "        print(\"LED deactivated.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error controlling the LED: {e}\")\n",
        "        # Consider how to handle this error if the LED action is critical.\n",
        "\n",
        "\n",
        "# --- Inside the main video capture loop (while True:) after goal logic ---\n",
        "\n",
        "# Variable to track goal state (to trigger action only once per event)\n",
        "# Initialize this variable before the main loop: goal_detected_state = False\n",
        "\n",
        "# Trigger the action if a goal is detected in the current frame AND\n",
        "# the state was NOT goal_detected_state in the previous frame.\n",
        "# This ensures the action is triggered only on the rising edge of the detection.\n",
        "if current_frame_goal_detected and not goal_detected_state:\n",
        "     activate_goal_action() # Call the function that turns on the LED\n",
        "     goal_detected_state = True # Update state to detected\n",
        "elif not current_frame_goal_detected and goal_detected_state:\n",
        "     # If goal was detected but is no longer, reset the state\n",
        "     goal_detected_state = False\n",
        "\n",
        "# --- Add GPIO cleanup at the end of the script (after the main loop) ---\n",
        "# Ensure this runs when the script exits (e.g., using a try...finally block)\n",
        "try:\n",
        "    GPIO.cleanup() # Clean up GPIO settings\n",
        "    print(\"GPIO cleanup performed.\")\n",
        "except Exception as e:\n",
        "     print(f\"Error during GPIO cleanup: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d388b6dd"
      },
      "source": [
        "# Instructions for evaluating performance (FPS) on your Raspberry Pi:\n",
        "\n",
        "# 1. Run the script with your current model (.pt or .tflite) on your Raspberry Pi.\n",
        "# 2. Add code to measure and print the frames per second (FPS).\n",
        "\n",
        "# Example of how to measure FPS within your main loop:\n",
        "\n",
        "# Initialize variables before the main loop:\n",
        "# start_time = time.time()\n",
        "# frame_count = 0\n",
        "\n",
        "# Inside the main video capture loop (while True:):\n",
        "\n",
        "    # After processing the frame and detections:\n",
        "\n",
        "    # Increment frame count\n",
        "    # frame_count += 1\n",
        "\n",
        "    # Calculate elapsed time\n",
        "    # elapsed_time = time.time() - start_time\n",
        "\n",
        "    # Calculate FPS and print periodically (e.g., every 100 frames)\n",
        "    # if frame_count % 100 == 0:\n",
        "    #     fps = frame_count / elapsed_time\n",
        "    #     print(f\"Current FPS: {fps:.2f}\")\n",
        "        # You might want to reset start_time and frame_count here\n",
        "        # start_time = time.time()\n",
        "        # frame_count = 0\n",
        "\n",
        "# Run the script and observe the printed FPS values to understand the current performance."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79c14d9e"
      },
      "source": [
        "# Adjusting the confidence threshold can impact performance.\n",
        "# A higher threshold means fewer detections are processed, which can speed things up,\n",
        "# but might also miss some objects.\n",
        "\n",
        "# In your script, locate the line where confidence_threshold is defined:\n",
        "# confidence_threshold = 0.5 # <--- Adjust as needed\n",
        "\n",
        "# Try increasing this value (e.g., to 0.6, 0.7, or higher) and re-run the script\n",
        "# on your Raspberry Pi to see if it improves performance without significantly\n",
        "# impacting the accuracy of your ball/line/crossbar detection."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "942a0f23"
      },
      "source": [
        "# Task\n",
        "Modifica el script de detección de objetos para que funcione con dos cámaras conectadas a una Raspberry Pi 4, detecte un balón en tiempo real en ambas cámaras y determine si hay gol basándose en las detecciones de ambas cámaras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee49acaa"
      },
      "source": [
        "## Configurar el entorno en la raspberry pi\n",
        "\n",
        "### Subtask:\n",
        "Instala las librerías necesarias en la Raspberry Pi para ejecutar el modelo TFLite, capturar video de la cámara y controlar el pin GPIO para la acción de gol.\n",
        "\n",
        "Esto incluye `opencv-python`, `tflite-runtime` y `rpi.gpio`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bcbb9cb"
      },
      "source": [
        "## Modificar el script para inicializar y capturar video de dos cámaras\n",
        "\n",
        "### Subtask:\n",
        "Actualizar el script para abrir ambas cámaras y leer frames de cada una en el bucle principal.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2db18c91"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires modifying the script to handle two cameras. I will update the script to initialize two camera objects, read frames from both, and include error handling for the frame reading process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a768ecef"
      },
      "source": [
        "# AGREGA ESTA LÍNEA COMO LA PRIMERA LÍNEA DE TU ARCHIVO:\n",
        "#!/home/pi/myenv/bin/python\n",
        "# Asegúrate de que la ruta '/home/pi/myenv/bin/python' sea la ruta CORRECTA al intérprete de Python en tu entorno virtual.\n",
        "# Si tu venv se llama diferente o está en otra carpeta, cambia esta ruta.\n",
        "\n",
        "\n",
        "import cv2\n",
        "import time\n",
        "\n",
        "# --- CONFIGURACIONES INICIALES ---\n",
        "\n",
        "# Define el índice de la cámara. Usualmente 0 para la cámara principal conectada.\n",
        "# Si tienes múltiples cámaras o una cámara CSI, el índice podría ser diferente.\n",
        "# Usa 'ls /dev/video*' en tu terminal para confirmar los índices disponibles.\n",
        "camera_index = 0 # <--- CAMBIA ESTO AL ÍNDICE DE TU CÁMARA REAL\n",
        "\n",
        "# Define la resolución deseada (opcional, a veces ayuda con rendimiento/compatibilidad)\n",
        "# Puedes probar con la resolución por defecto primero.\n",
        "# frame_width = 640\n",
        "# frame_height = 480\n",
        "\n",
        "\n",
        "# --- INICIALIZAR CAPTURA DE VIDEO ---\n",
        "\n",
        "print(f\"Intentando abrir cámara con índice {camera_index}...\")\n",
        "cap = cv2.VideoCapture(camera_index)\n",
        "\n",
        "# --- VERIFICAR SI LA CÁMARA SE ABRIÓ CORRECTAMENTE ---\n",
        "\n",
        "if not cap.isOpened():\n",
        "    print(f\"Error: No se pudo abrir la cámara con índice {camera_index}.\")\n",
        "    print(\"Por favor, verifica lo siguiente:\")\n",
        "    print(\"- Asegúrate de que la cámara esté correctamente conectada.\")\n",
        "    print(\"- Verifica el índice de la cámara usando 'ls /dev/video*'.\")\n",
        "    print(\"- Asegúrate de tener los permisos necesarios para acceder a la cámara (puedes necesitar ejecutar con 'sudo').\")\n",
        "    print(\"- Asegúrate de que OpenCV está correctamente instalado en tu entorno virtual.\")\n",
        "    # Aquí podrías añadir un 'exit()' para que el script se detenga si la cámara no abre.\n",
        "    # Para este ejemplo, continuaremos imprimiendo un mensaje de error.\n",
        "else:\n",
        "    print(f\"Cámara con índice {camera_index} abierta exitosamente.\")\n",
        "\n",
        "    # --- Opcional: Configurar resolución si la definiste arriba ---\n",
        "    # cap.set(cv2.CAP_PROP_FRAME_WIDTH, frame_width)\n",
        "    # cap.set(cv2.CAP_PROP_FRAME_HEIGHT, frame_height)\n",
        "    # print(f\"Resolución configurada a {cap.get(cv2.CAP_PROP_FRAME_WIDTH)}x{cap.get(cv2.CAP_PROP_FRAME_HEIGHT)}\")\n",
        "\n",
        "\n",
        "# --- BUCLE PRINCIPAL DE CAPTURA (AUN NO CON DETECCION) ---\n",
        "# Este bucle solo leerá frames y opcionalmente los mostrará.\n",
        "\n",
        "# try:\n",
        "#     while True:\n",
        "#         # Lee un frame de la cámara\n",
        "#         ret, frame = cap.read()\n",
        "\n",
        "#         # Si el frame no fue leído correctamente, rompe el bucle\n",
        "#         if not ret:\n",
        "#             print(\"Error: No se pudo leer frame. Saliendo del bucle de captura.\")\n",
        "#             break\n",
        "\n",
        "#         # --- Aquí es donde procesarías el 'frame' ---\n",
        "#         # Por ahora, solo imprimiremos la forma del frame para confirmar que se lee.\n",
        "#         print(f\"Frame capturado. Forma: {frame.shape}\")\n",
        "\n",
        "#         # --- Opcional: Mostrar el frame (solo si tienes pantalla conectada a la Pi) ---\n",
        "#         # cv2.imshow('Camera Feed', frame)\n",
        "\n",
        "#         # --- Opcional: Esperar una tecla (necesario si usas cv2.imshow) ---\n",
        "#         # if cv2.waitKey(1) & 0xFF == ord('q'): # Presiona 'q' para salir\n",
        "#         #     break\n",
        "\n",
        "#         # Si no usas cv2.imshow, el bucle correrá rápido. Puedes añadir un pequeño delay si es necesario.\n",
        "#         # time.sleep(0.01) # Pequeño delay para reducir carga de CPU\n",
        "\n",
        "# except KeyboardInterrupt:\n",
        "#     # Permite salir del bucle con Ctrl+C en la terminal\n",
        "#     print(\"\\nInterrupción por teclado. Deteniendo la captura.\")\n",
        "\n",
        "# finally:\n",
        "#     # --- LIMPIEZA FINAL ---\n",
        "#     print(\"Realizando limpieza...\")\n",
        "#     # Libera el objeto de captura de video\n",
        "#     if 'cap' in locals() and cap.isOpened():\n",
        "#         cap.release()\n",
        "#         print(\"Captura de video liberada.\")\n",
        "\n",
        "#     # Destruye cualquier ventana de OpenCV (si se usó cv2.imshow)\n",
        "#     # cv2.destroyAllWindows()\n",
        "\n",
        "#     print(\"Script finalizado.\")\n",
        "\n",
        "\n",
        "# --- PRÓXIMO PASO: Integraremos la carga del modelo TFLite y la detección en el bucle. ---"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f83ea88",
        "outputId": "b81351f7-e139-4264-e14a-fcfc52ad84fd"
      },
      "source": [
        "# Read the content of the 2 camaras SDK.txt file\n",
        "with open('/content/2 camaras SDK.txt', 'r') as f:\n",
        "    sdk_code = f.read()\n",
        "\n",
        "# Print the content\n",
        "print(sdk_code)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#!/usr/bin/env python3\n",
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "multi_cam_2x30.py\n",
            "Abre y graba 2 cámaras MindVision (mvsdk) simultáneamente a 30 FPS.\n",
            "- Ajusta SDK_PATH si mvsdk.py está en otra ruta.\n",
            "- Usa un hub USB3 alimentado si conectas 2 cámaras en la Raspberry Pi.\n",
            "\"\"\"\n",
            "\n",
            "import sys\n",
            "import os\n",
            "import threading\n",
            "import queue\n",
            "import time\n",
            "import numpy as np\n",
            "import cv2\n",
            "\n",
            "# Ruta donde está mvsdk.py (ajusta si está en otro lugar)\n",
            "SDK_PATH = \"/home/pi/Downloads/h/demo/python_demo\"\n",
            "if SDK_PATH not in sys.path:\n",
            "    sys.path.insert(0, SDK_PATH)\n",
            "\n",
            "import mvsdk  # requiere mvsdk.py en SDK_PATH o instalado globalmente\n",
            "\n",
            "class CameraWorker(threading.Thread):\n",
            "    def __init__(self, devinfo, name, out_queue, stop_event, save_filename=None, exposure_ms=30, target_fps=30):\n",
            "        super().__init__(daemon=True)\n",
            "        self.devinfo = devinfo\n",
            "        self.name = name\n",
            "        self.out_q = out_queue\n",
            "        self.stop_event = stop_event\n",
            "        self.save_filename = save_filename\n",
            "        self.exposure_ms = exposure_ms\n",
            "        self.target_fps = float(target_fps) if target_fps > 0 else 30.0\n",
            "        self.frame_interval = 1.0 / self.target_fps\n",
            "\n",
            "        self.hCamera = None\n",
            "        self.pFrameBuffer = None\n",
            "        self.cap = None\n",
            "        self.mono = False\n",
            "        self.writer = None\n",
            "        self.width = None\n",
            "        self.height = None\n",
            "\n",
            "    def open_camera(self):\n",
            "        self.hCamera = mvsdk.CameraInit(self.devinfo, -1, -1)\n",
            "        self.cap = mvsdk.CameraGetCapability(self.hCamera)\n",
            "        self.mono = (self.cap.sIspCapacity.bMonoSensor != 0)\n",
            "\n",
            "        if self.mono:\n",
            "            mvsdk.CameraSetIspOutFormat(self.hCamera, mvsdk.CAMERA_MEDIA_TYPE_MONO8)\n",
            "        else:\n",
            "            # preferimos BGR8 para compatibilidad directa con OpenCV\n",
            "            mvsdk.CameraSetIspOutFormat(self.hCamera, mvsdk.CAMERA_MEDIA_TYPE_BGR8)\n",
            "\n",
            "        mvsdk.CameraSetTriggerMode(self.hCamera, 0)  # continuous\n",
            "        try:\n",
            "            mvsdk.CameraSetAeState(self.hCamera, 0)\n",
            "            mvsdk.CameraSetExposureTime(self.hCamera, int(self.exposure_ms * 1000))\n",
            "        except Exception:\n",
            "            pass\n",
            "\n",
            "        mvsdk.CameraPlay(self.hCamera)\n",
            "\n",
            "        FrameBufferSize = (self.cap.sResolutionRange.iWidthMax *\n",
            "                           self.cap.sResolutionRange.iHeightMax *\n",
            "                           (1 if self.mono else 3))\n",
            "        self.pFrameBuffer = mvsdk.CameraAlignMalloc(FrameBufferSize, 16)\n",
            "\n",
            "    def run(self):\n",
            "        try:\n",
            "            self.open_camera()\n",
            "        except Exception as e:\n",
            "            print(f\"[{self.name}] Error al abrir cámara: {e}\")\n",
            "            return\n",
            "\n",
            "        last_push = 0.0\n",
            "        first_frame = True\n",
            "\n",
            "        while not self.stop_event.is_set():\n",
            "            try:\n",
            "                pRawData, FrameHead = mvsdk.CameraGetImageBuffer(self.hCamera, 300)\n",
            "                mvsdk.CameraImageProcess(self.hCamera, pRawData, self.pFrameBuffer, FrameHead)\n",
            "                mvsdk.CameraReleaseImageBuffer(self.hCamera, pRawData)\n",
            "\n",
            "                frame_data = (mvsdk.c_ubyte * FrameHead.uBytes).from_address(self.pFrameBuffer)\n",
            "                arr = np.frombuffer(frame_data, dtype=np.uint8)\n",
            "\n",
            "                if FrameHead.uiMediaType == mvsdk.CAMERA_MEDIA_TYPE_MONO8:\n",
            "                    frame = arr.reshape((FrameHead.iHeight, FrameHead.iWidth))\n",
            "                    # convert to 3-channel for display/writer later\n",
            "                    frame_color = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)\n",
            "                else:\n",
            "                    frame = arr.reshape((FrameHead.iHeight, FrameHead.iWidth, 3))\n",
            "                    frame_color = frame  # already BGR\n",
            "\n",
            "                now = time.time()\n",
            "                if first_frame:\n",
            "                    self.height, self.width = frame_color.shape[:2]\n",
            "                    if self.save_filename:\n",
            "                        fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # puedes cambiar codec si lo deseas\n",
            "                        # VideoWriter expects color frames; for mono we convert above\n",
            "                        self.writer = cv2.VideoWriter(self.save_filename, fourcc, float(self.target_fps),\n",
            "                                                      (self.width, self.height), True)\n",
            "                    first_frame = False\n",
            "\n",
            "                # Throttling: push / write only si toca según target_fps\n",
            "                if (now - last_push) >= self.frame_interval:\n",
            "                    last_push = now\n",
            "                    # mantener cola pequeña: si está llena descarta el más viejo\n",
            "                    try:\n",
            "                        if self.out_q.qsize() > 2:\n",
            "                            try:\n",
            "                                _ = self.out_q.get_nowait()\n",
            "                            except queue.Empty:\n",
            "                                pass\n",
            "                        self.out_q.put_nowait((self.name, frame_color, now))\n",
            "                    except queue.Full:\n",
            "                        pass\n",
            "\n",
            "                    if self.writer is not None:\n",
            "                        self.writer.write(frame_color)\n",
            "\n",
            "                # pequeña pausa para ceder CPU (ajustable)\n",
            "                time.sleep(0.001)\n",
            "\n",
            "            except mvsdk.CameraException as e:\n",
            "                # ignora timeouts normales\n",
            "                if e.error_code != mvsdk.CAMERA_STATUS_TIME_OUT:\n",
            "                    print(f\"[{self.name}] CameraGetImageBuffer failed({e.error_code}): {e.message}\")\n",
            "                continue\n",
            "            except Exception as e:\n",
            "                print(f\"[{self.name}] Error inesperado en captura: {e}\")\n",
            "                break\n",
            "\n",
            "        # Cleanup\n",
            "        if self.writer:\n",
            "            self.writer.release()\n",
            "        try:\n",
            "            mvsdk.CameraUnInit(self.hCamera)\n",
            "        except Exception:\n",
            "            pass\n",
            "        try:\n",
            "            mvsdk.CameraAlignFree(self.pFrameBuffer)\n",
            "        except Exception:\n",
            "            pass\n",
            "        print(f\"[{self.name}] terminado\")\n",
            "\n",
            "def main():\n",
            "    devs = mvsdk.CameraEnumerateDevice()\n",
            "    if not devs:\n",
            "        print(\"No se encontraron cámaras (mvsdk.CameraEnumerateDevice()).\")\n",
            "        return\n",
            "\n",
            "    # decide cuántas cámaras abrir (2 por defecto; si hay 1, abrirá solo 1)\n",
            "    want = 2\n",
            "    devices = devs[:want]\n",
            "    print(\"Dispositivos encontrados:\", [d.GetFriendlyName() for d in devices])\n",
            "\n",
            "    stop_event = threading.Event()\n",
            "    queues = []\n",
            "    workers = []\n",
            "\n",
            "    for i, dev in enumerate(devices):\n",
            "        q = queue.Queue(maxsize=4)\n",
            "        name = f\"cam{i}\"\n",
            "        out_file = f\"cam{i}_out.mp4\"\n",
            "        w = CameraWorker(dev, name, q, stop_event, save_filename=out_file, exposure_ms=30, target_fps=30)\n",
            "        w.start()\n",
            "        queues.append(q)\n",
            "        workers.append(w)\n",
            "        time.sleep(0.2)  # pequeño delay al iniciar cámaras\n",
            "\n",
            "    # Ventanas para mostrar cada cámara\n",
            "    window_names = [f\"Camera {i}\" for i in range(len(workers))]\n",
            "    for wn in window_names:\n",
            "        cv2.namedWindow(wn, cv2.WINDOW_AUTOSIZE)\n",
            "\n",
            "    last_frames = {w.name: None for w in workers}\n",
            "\n",
            "    try:\n",
            "        print(\"Presiona 'q' o cierra una ventana para salir.\")\n",
            "        while True:\n",
            "            # Recolecta frames de las colas\n",
            "            for q in queues:\n",
            "                try:\n",
            "                    name, frame, ts = q.get(timeout=0.05)\n",
            "                    last_frames[name] = frame\n",
            "                except queue.Empty:\n",
            "                    pass\n",
            "\n",
            "            # Mostrar\n",
            "            for idx, w in enumerate(workers):\n",
            "                name = w.name\n",
            "                frame = last_frames.get(name)\n",
            "                if frame is not None:\n",
            "                    # opcional: redimensiona para pantalla\n",
            "                    disp = cv2.resize(frame, (640, 480), interpolation=cv2.INTER_LINEAR)\n",
            "                    cv2.imshow(window_names[idx], disp)\n",
            "                else:\n",
            "                    # imagen negra si aún no hay frame\n",
            "                    blank = np.zeros((480, 640, 3), dtype=np.uint8)\n",
            "                    cv2.imshow(window_names[idx], blank)\n",
            "\n",
            "            key = cv2.waitKey(1) & 0xFF\n",
            "            if key == ord('q'):\n",
            "                print(\"Salida por tecla q.\")\n",
            "                break\n",
            "\n",
            "            # detectar si se cerró alguna ventana con la X\n",
            "            closed = False\n",
            "            for wn in window_names:\n",
            "                if cv2.getWindowProperty(wn, cv2.WND_PROP_VISIBLE) < 1:\n",
            "                    closed = True\n",
            "                    break\n",
            "            if closed:\n",
            "                print(\"Ventana cerrada por el usuario — saliendo.\")\n",
            "                break\n",
            "\n",
            "    finally:\n",
            "        # señalizar paro y esperar hilos\n",
            "        stop_event.set()\n",
            "        for w in workers:\n",
            "            w.join(timeout=3)\n",
            "        cv2.destroyAllWindows()\n",
            "        print(\"Todo terminado.\")\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jp9UUr1qgJGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d631f4d8"
      },
      "source": [
        "# Task\n",
        "Adaptar el script de detección de objetos proporcionado (`/content/2 camaras SDK.txt`) para utilizar un modelo YOLOv8 TFLite entrenado, integrar la inferencia en un sistema de doble cámara, implementar la lógica de detección de gol basada en las detecciones de ambas cámaras, y configurar el script para que se ejecute automáticamente al inicio en una Raspberry Pi 4 utilizando un entorno virtual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5280ec4d"
      },
      "source": [
        "## Adaptar el script base para la detección\n",
        "\n",
        "### Subtask:\n",
        "Modificar el script proporcionado (`2 camaras SDK.txt`) para cargar el modelo YOLOv8 TFLite y los nombres de las clases al inicio.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0f177bd"
      },
      "source": [
        "**Reasoning**:\n",
        "I will modify the provided script (`2 camaras SDK.txt`) to include the necessary imports, define file paths for the TFLite model and class names, and add code to load the class names and the TFLite model with error handling, as requested by the subtask. I will incorporate steps 1 through 8 from the instructions into this single code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "7f7bca75",
        "outputId": "9fe6a21c-2e71-44c4-a5fc-5846c236949b"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "multi_cam_2x30.py\n",
        "Abre y graba 2 cámaras MindVision (mvsdk) simultáneamente a 30 FPS.\n",
        "- Ajusta SDK_PATH si mvsdk.py está en otra ruta.\n",
        "- Usa un hub USB3 alimentado si conectas 2 cámaras en la Raspberry Pi.\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import threading\n",
        "import queue\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "# Importa la librería tflite_runtime para cargar y ejecutar el modelo TFLite\n",
        "# Asegúrate de haberla instalado en tu Raspberry Pi: pip install tflite-runtime\n",
        "import tflite_runtime.interpreter as tflite\n",
        "\n",
        "# Ruta donde está mvsdk.py (ajusta si está en otro lugar)\n",
        "SDK_PATH = \"/home/pi/Downloads/h/demo/python_demo\"\n",
        "if SDK_PATH not in sys.path:\n",
        "    sys.path.insert(0, SDK_PATH)\n",
        "\n",
        "# Importa la librería RPi.GPIO para controlar pines GPIO (solo en Raspberry Pi)\n",
        "# Asegúrate de haberla instalado en tu Raspberry Pi: pip install rpi.gpio\n",
        "# Solo importa si no es None para permitir pruebas en otros sistemas\n",
        "try:\n",
        "    import RPi.GPIO as GPIO\n",
        "    GPIO_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"Advertencia: RPi.GPIO no encontrado. Las funciones de GPIO no estarán disponibles.\")\n",
        "    GPIO_AVAILABLE = False\n",
        "\n",
        "\n",
        "import mvsdk  # requiere mvsdk.py en SDK_PATH o instalado globalmente\n",
        "\n",
        "class CameraWorker(threading.Thread):\n",
        "    def __init__(self, devinfo, name, out_queue, stop_event, save_filename=None, exposure_ms=30, target_fps=30):\n",
        "        super().__init__(daemon=True)\n",
        "        self.devinfo = devinfo\n",
        "        self.name = name\n",
        "        self.out_q = out_queue\n",
        "        self.stop_event = stop_event\n",
        "        self.save_filename = save_filename\n",
        "        self.exposure_ms = exposure_ms\n",
        "        self.target_fps = float(target_fps) if target_fps > 0 else 30.0\n",
        "        self.frame_interval = 1.0 / self.target_fps\n",
        "\n",
        "        self.hCamera = None\n",
        "        self.pFrameBuffer = None\n",
        "        self.cap = None\n",
        "        self.mono = False\n",
        "        self.writer = None\n",
        "        self.width = None\n",
        "        self.height = None\n",
        "\n",
        "    def open_camera(self):\n",
        "        self.hCamera = mvsdk.CameraInit(self.devinfo, -1, -1)\n",
        "        self.cap = mvsdk.CameraGetCapability(self.hCamera)\n",
        "        self.mono = (self.cap.sIspCapacity.bMonoSensor != 0)\n",
        "\n",
        "        if self.mono:\n",
        "            mvsdk.CameraSetIspOutFormat(self.hCamera, mvsdk.CAMERA_MEDIA_TYPE_MONO8)\n",
        "        else:\n",
        "            # preferimos BGR8 para compatibilidad directa con OpenCV\n",
        "            mvsdk.CameraSetIspOutFormat(self.hCamera, mvsdk.CAMERA_MEDIA_TYPE_BGR8)\n",
        "\n",
        "        mvsdk.CameraSetTriggerMode(self.hCamera, 0)  # continuous\n",
        "        try:\n",
        "            mvsdk.CameraSetAeState(self.hCamera, 0)\n",
        "            mvsdk.CameraSetExposureTime(self.hCamera, int(self.exposure_ms * 1000))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        mvsdk.CameraPlay(self.hCamera)\n",
        "\n",
        "        FrameBufferSize = (self.cap.sResolutionRange.iWidthMax *\n",
        "                           self.cap.sResolutionRange.iHeightMax *\n",
        "                           (1 if self.mono else 3))\n",
        "        self.pFrameBuffer = mvsdk.CameraAlignMalloc(FrameBufferSize, 16)\n",
        "\n",
        "    def run(self):\n",
        "        try:\n",
        "            self.open_camera()\n",
        "        except Exception as e:\n",
        "            print(f\"[{self.name}] Error al abrir cámara: {e}\")\n",
        "            return\n",
        "\n",
        "        last_push = 0.0\n",
        "        first_frame = True\n",
        "\n",
        "        while not self.stop_event.is_set():\n",
        "            try:\n",
        "                pRawData, FrameHead = mvsdk.CameraGetImageBuffer(self.hCamera, 300)\n",
        "                mvsdk.CameraImageProcess(self.hCamera, pRawData, self.pFrameBuffer, FrameHead)\n",
        "                mvsdk.CameraReleaseImageBuffer(self.hCamera, pRawData)\n",
        "\n",
        "                frame_data = (mvsdk.c_ubyte * FrameHead.uBytes).from_address(self.pFrameBuffer)\n",
        "                arr = np.frombuffer(frame_data, dtype=np.uint8)\n",
        "\n",
        "                if FrameHead.uiMediaType == mvsdk.CAMERA_MEDIA_TYPE_MONO8:\n",
        "                    frame = arr.reshape((FrameHead.iHeight, FrameHead.iWidth))\n",
        "                    # convert to 3-channel for display/writer later\n",
        "                    frame_color = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)\n",
        "                else:\n",
        "                    frame = arr.reshape((FrameHead.iHeight, FrameHead.iWidth, 3))\n",
        "                    frame_color = frame  # already BGR\n",
        "\n",
        "                now = time.time()\n",
        "                if first_frame:\n",
        "                    self.height, self.width = frame_color.shape[:2]\n",
        "                    if self.save_filename:\n",
        "                        fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # puedes cambiar codec si lo deseas\n",
        "                        # VideoWriter expects color frames; for mono we convert above\n",
        "                        self.writer = cv2.VideoWriter(self.save_filename, fourcc, float(self.target_fps),\n",
        "                                                      (self.width, self.height), True)\n",
        "                    first_frame = False\n",
        "\n",
        "                # Throttling: push / write only si toca según target_fps\n",
        "                if (now - last_push) >= self.frame_interval:\n",
        "                    last_push = now\n",
        "                    # mantener cola pequeña: si está llena descarta el más viejo\n",
        "                    try:\n",
        "                        if self.out_q.qsize() > 2:\n",
        "                            try:\n",
        "                                _ = self.out_q.get_nowait()\n",
        "                            except queue.Empty:\n",
        "                                pass\n",
        "                        self.out_q.put_nowait((self.name, frame_color, now))\n",
        "                    except queue.Full:\n",
        "                        pass\n",
        "\n",
        "                    if self.writer is not None:\n",
        "                        self.writer.write(frame_color)\n",
        "\n",
        "                # pequeña pausa para ceder CPU (ajustable)\n",
        "                time.sleep(0.001)\n",
        "\n",
        "            except mvsdk.CameraException as e:\n",
        "                # ignora timeouts normales\n",
        "                if e.error_code != mvsdk.CAMERA_STATUS_TIME_OUT:\n",
        "                    print(f\"[{self.name}] CameraGetImageBuffer failed({e.error_code}): {e.message}\")\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                print(f\"[{self.name}] Error inesperado en captura: {e}\")\n",
        "                break\n",
        "\n",
        "        # Cleanup\n",
        "        if self.writer:\n",
        "            self.writer.release()\n",
        "        try:\n",
        "            mvsdk.CameraUnInit(self.hCamera)\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            mvsdk.CameraAlignFree(self.pFrameBuffer)\n",
        "        except Exception:\n",
        "            pass\n",
        "        print(f\"[{self.name}] terminado\")\n",
        "\n",
        "# --- CONFIGURACIONES PARA EL MODELO TFLITE Y CLASES ---\n",
        "\n",
        "# --- Define la ruta a tu archivo de modelo .tflite ---\n",
        "# !!! IMPORTANTE: Reemplaza esta ruta con la ruta real donde guardaste\n",
        "# el archivo best.tflite (o como le hayas llamado) en tu Raspberry Pi. !!!\n",
        "# Por ejemplo: '/home/pi/robot_vision/best.tflite'\n",
        "best_model_path_tflite = '/path/to/your/best.tflite' # <--- CAMBIA ESTO A TU RUTA REAL\n",
        "\n",
        "# --- Define la ruta a tu archivo classes.txt ---\n",
        "# Necesitas un archivo de texto con los nombres de tus clases, uno por línea,\n",
        "# en el mismo orden que usaste para entrenar ('balon', 'linea_meta', 'travesaño', etc.).\n",
        "# Por ejemplo: '/home/pi/robot_vision/classes.txt'\n",
        "classes_file_path = '/path/to/your/classes.txt' # <--- CAMBIA ESTO A LA RUTA REAL\n",
        "\n",
        "# --- Carga los nombres de las clases desde el archivo classes.txt ---\n",
        "class_names = []\n",
        "try:\n",
        "    with open(classes_file_path, 'r') as f:\n",
        "        class_names = [line.strip() for line in f.readlines()]\n",
        "    print(f\"Nombres de clases cargados exitosamente desde: {classes_file_path}\")\n",
        "    print(f\"Clases detectadas: {class_names}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar el archivo de clases {classes_file_path}: {e}\")\n",
        "    print(\"Asegúrate de que el archivo classes.txt exista y la ruta sea correcta en tu Raspberry Pi.\")\n",
        "    sys.exit(1) # Sal del script si no se pueden cargar las clases.\n",
        "\n",
        "\n",
        "# --- Carga el modelo TensorFlow Lite (.tflite) ---\n",
        "try:\n",
        "    # Carga el modelo .tflite y obtiene el intérprete.\n",
        "    interpreter = tflite.Interpreter(model_path=best_model_path_tflite)\n",
        "\n",
        "    # Asigna los tensores (input y output). Esto configura el modelo para la inferencia.\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    # Obtiene información sobre los tensores de entrada y salida del modelo.\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    # Obtiene el tamaño de entrada esperado por el modelo (altura, ancho)\n",
        "    # input_details[0]['shape'] da la forma del tensor de entrada, usualmente [1, height, width, 3]\n",
        "    input_shape = input_details[0]['shape']\n",
        "    input_height = input_shape[1]\n",
        "    input_width = input_shape[2]\n",
        "\n",
        "    # Determina el tipo de datos de entrada esperado por el modelo (uint8 o float32)\n",
        "    floating_model = input_details[0]['dtype'] == np.float32\n",
        "\n",
        "    print(f\"Modelo TFLite cargado exitosamente desde: {best_model_path_tflite}\")\n",
        "    print(f\"Tamaño de entrada esperado por el modelo: {input_width}x{input_height}\")\n",
        "    print(f\"El modelo espera datos de tipo: {input_details[0]['dtype']}\")\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar el modelo TFLite: {e}\")\n",
        "    print(f\"Por favor, verifica que la ruta '{best_model_path_tflite}' sea correcta y que el archivo best.tflite exista en tu Raspberry Pi.\")\n",
        "    print(\"Asegúrate también de haber instalado 'tflite-runtime' (`pip install tflite-runtime`).\")\n",
        "    sys.exit(1) # Sal del script si no se pudo cargar el modelo TFLite.\n",
        "\n",
        "# --- Fin de la carga del modelo y clases ---\n",
        "\n",
        "\n",
        "# --- INICIALIZACIÓN DE CÁMARAS Y OTROS COMPONENTES ---\n",
        "\n",
        "def main():\n",
        "    devs = mvsdk.CameraEnumerateDevice()\n",
        "    if not devs:\n",
        "        print(\"No se encontraron cámaras (mvsdk.CameraEnumerateDevice()).\")\n",
        "        return\n",
        "\n",
        "    # decide cuántas cámaras abrir (2 por defecto; si hay 1, abrirá solo 1)\n",
        "    want = 2\n",
        "    devices = devs[:want]\n",
        "    print(\"Dispositivos encontrados:\", [d.GetFriendlyName() for d in devices])\n",
        "\n",
        "    stop_event = threading.Event()\n",
        "    queues = []\n",
        "    workers = []\n",
        "\n",
        "    for i, dev in enumerate(devices):\n",
        "        q = queue.Queue(maxsize=4)\n",
        "        name = f\"cam{i}\"\n",
        "        out_file = f\"cam{i}_out.mp4\"\n",
        "        w = CameraWorker(dev, name, q, stop_event, save_filename=out_file, exposure_ms=30, target_fps=30)\n",
        "        w.start()\n",
        "        queues.append(q)\n",
        "        workers.append(w)\n",
        "        time.sleep(0.2)  # pequeño delay al iniciar cámaras\n",
        "\n",
        "    # Ventanas para mostrar cada cámara\n",
        "    window_names = [f\"Camera {i}\" for i in range(len(workers))]\n",
        "    for wn in window_names:\n",
        "        cv2.namedWindow(wn, cv2.WINDOW_AUTOSIZE)\n",
        "\n",
        "    last_frames = {w.name: None for w in workers}\n",
        "\n",
        "    try:\n",
        "        print(\"Presiona 'q' o cierra una ventana para salir.\")\n",
        "        while True:\n",
        "            # Recolecta frames de las colas\n",
        "            for q in queues:\n",
        "                try:\n",
        "                    name, frame, ts = q.get(timeout=0.05)\n",
        "                    last_frames[name] = frame\n",
        "                except queue.Empty:\n",
        "                    pass\n",
        "\n",
        "            # Mostrar\n",
        "            for idx, w in enumerate(workers):\n",
        "                name = w.name\n",
        "                frame = last_frames.get(name)\n",
        "                if frame is not None:\n",
        "                    # opcional: redimensiona para pantalla\n",
        "                    disp = cv2.resize(frame, (640, 480), interpolation=cv2.INTER_LINEAR)\n",
        "                    cv2.imshow(window_names[idx], disp)\n",
        "                else:\n",
        "                    # imagen negra si aún no hay frame\n",
        "                    blank = np.zeros((480, 640, 3), dtype=np.uint8)\n",
        "                    cv2.imshow(window_names[idx], blank)\n",
        "\n",
        "            key = cv2.waitKey(1) & 0xFF\n",
        "            if key == ord('q'):\n",
        "                print(\"Salida por tecla q.\")\n",
        "                break\n",
        "\n",
        "            # detectar si se cerró alguna ventana con la X\n",
        "            closed = False\n",
        "            for wn in window_names:\n",
        "                if cv2.getWindowProperty(wn, cv2.WND_PROP_VISIBLE) < 1:\n",
        "                    closed = True\n",
        "                    break\n",
        "            if closed:\n",
        "                print(\"Ventana cerrada por el usuario — saliendo.\")\n",
        "                break\n",
        "\n",
        "    finally:\n",
        "        # señalizar paro y esperar hilos\n",
        "        stop_event.set()\n",
        "        for w in workers:\n",
        "            w.join(timeout=3)\n",
        "        cv2.destroyAllWindows()\n",
        "        print(\"Todo terminado.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tflite_runtime'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-515805138.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Importa la librería tflite_runtime para cargar y ejecutar el modelo TFLite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Asegúrate de haberla instalado en tu Raspberry Pi: pip install tflite-runtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtflite_runtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpreter\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtflite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Ruta donde está mvsdk.py (ajusta si está en otro lugar)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tflite_runtime'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac2f0fc9"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the `tflite_runtime` library is not installed in the current environment. Although the script is intended for a Raspberry Pi where this library should be installed, I need to install it in this environment to allow the code block to execute and verify its syntax and structure before providing it to the user.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/home/pi/myenv/bin/python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "multi_cam_2x30.py\n",
        "Abre y graba 2 cámaras MindVision (mvsdk) simultáneamente a 30 FPS.\n",
        "- Ajusta SDK_PATH si mvsdk.py está en otra ruta.\n",
        "- Usa un hub USB3 alimentado si conectas 2 cámaras en la Raspberry Pi.\n",
        "\n",
        "Adaptado para detección con modelo .pt (Ultralytics YOLOv8) y lógica de gol.\n",
        "NOTA: Este enfoque puede ser MUY LENTO en la CPU de la Raspberry Pi 4.\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import threading\n",
        "import queue\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "# Importa la librería ultralytics para cargar el modelo .pt\n",
        "# Asegúrate de haberla instalado en tu Raspberry Pi: pip install ultralytics\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Ruta donde está mvsdk.py (ajusta si está en otro lugar)\n",
        "SDK_PATH = \"/home/pi/Downloads/h/demo/python_demo\"\n",
        "if SDK_PATH not in sys.path:\n",
        "    sys.path.insert(0, SDK_PATH)\n",
        "\n",
        "# Importa la librería RPi.GPIO para controlar pines GPIO (solo en Raspberry Pi)\n",
        "# Asegúrate de haberla instalado en tu Raspberry Pi: pip install rpi.gpio\n",
        "# Solo importa si no es None para permitir pruebas en otros sistemas\n",
        "try:\n",
        "    import RPi.GPIO as GPIO\n",
        "    GPIO_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"Advertencia: RPi.GPIO no encontrado. Las funciones de GPIO no estarán disponibles.\")\n",
        "    GPIO_AVAILABLE = False\n",
        "\n",
        "\n",
        "import mvsdk  # requiere mvsdk.py en SDK_PATH o instalado globalmente\n",
        "\n",
        "# --- CONFIGURACIONES GLOBALES PARA EL MODELO .PT Y CLASES ---\n",
        "\n",
        "# --- Define la ruta a tu archivo de modelo .pt ---\n",
        "# !!! IMPORTANTE: Reemplaza esta ruta con la ruta real donde guardaste\n",
        "# el archivo best.pt en tu Raspberry Pi. !!!\n",
        "# Por ejemplo: '/home/pi/robot_vision/best.pt'\n",
        "best_model_path_pt = '/home/pi/best.pt' # <--- CAMBIA ESTO A TU RUTA REAL\n",
        "\n",
        "# --- Define la ruta a tu archivo classes.txt ---\n",
        "# Necesitas un archivo de texto con los nombres de tus clases, uno por línea,\n",
        "# en el mismo orden que usaste para entrenar ('balon', 'linea_meta', 'travesaño', etc.).\n",
        "# Por ejemplo: '/home/pi/robot_vision/classes.txt'\n",
        "classes_file_path = '/home/pi/classes.txt' # <--- CAMBIA ESTO A LA RUTA REAL\n",
        "\n",
        "# Define un umbral de confianza para las detecciones.\n",
        "# Las detecciones con confianza por debajo de este umbral serán ignoradas.\n",
        "confidence_threshold = 0.5 # <--- Ajusta según necesites (ej: 0.6, 0.7)\n",
        "\n",
        "# !!! IMPORTANTE: Define los IDs de clase para tus clases ('balon', 'linea_meta', 'travesaño') !!!\n",
        "# Estos IDs deben coincidir con el orden de tus clases en classes.txt y tu dataset.yaml.\n",
        "# Si 'balon' es la primera clase, su ID es 0. Si 'línea_meta' es la segunda, su ID es 1, etc.\n",
        "valid_ball_class_ids = [0] # <--- CAMBIA ESTO A LA LISTA DE IDS DE CLASE PARA TUS BALONES VALIDOS (ej: [0, 1, 2] si tienes varios tipos de balones)\n",
        "linea_meta_class_id = 1 # <--- CAMBIA ESTO AL ID DE CLASE PARA 'línea_de_meta'\n",
        "travesano_class_id = 2 # <--- CAMBIA ESTO AL ID DE CLASE PARA 'travesaño'\n",
        "\n",
        "# !!! IMPORTANTE: Define las coordenadas para la Región de Interés (ROI) del arco [x1, y1, x2, y2] !!!\n",
        "# Estas coordenadas definen un rectángulo en píxeles del frame de la cámara que abarca el área del arco.\n",
        "# Necesitarás determinarlas experimentalmente mirando la transmisión de tu cámara y tomando nota de las coordenadas.\n",
        "# Ejemplo: [x_min, y_min, x_max, y_max]\n",
        "goal_roi_coords = [100, 200, 500, 400] # <--- AJUSTA ESTAS COORDENADAS A TU SETUP\n",
        "print(f\"Región de Interés (ROI) del arco definida como: {goal_roi_coords}\")\n",
        "\n",
        "# Define la orientación de la línea de meta y la dirección en la que se considera gol.\n",
        "# !!! IMPORTANTE: Ajusta estos basados en la orientación real de tu línea de meta en la imagen\n",
        "# y la dirección en la que la pelota debe cruzar para ser gol. !!!\n",
        "# Ejemplo: Si la línea de meta es horizontal y un gol es cuando la pelota pasa por debajo de ella:\n",
        "goal_line_orientation = 'horizontal' # 'horizontal' o 'vertical'\n",
        "crossing_direction = 'below' # Para línea horizontal: 'above' o 'below'. Para línea vertical: 'left' o 'right'.\n",
        "\n",
        "# !!! IMPORTANTE: Define el pin GPIO que usarás para tu acción (ej: un LED) !!!\n",
        "led_pin = 17 # <--- CAMBIA ESTE NÚMERO AL PIN GPIO REAL QUE USAS (Numeración BCM) !!!\n",
        "\n",
        "\n",
        "# --- INICIALIZACIÓN GLOBAL DEL MODELO .PT Y CLASES ---\n",
        "\n",
        "# --- Carga los nombres de las clases desde el archivo classes.txt ---\n",
        "class_names = []\n",
        "try:\n",
        "    with open(classes_file_path, 'r') as f:\n",
        "        class_names = [line.strip() for line in f.readlines()]\n",
        "    print(f\"Nombres de clases cargados exitosamente desde: {classes_file_path}\")\n",
        "    print(f\"Clases detectadas: {class_names}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar el archivo de clases {classes_file_path}: {e}\")\n",
        "    print(\"Asegúrate de que el archivo classes.txt exista y la ruta sea correcta en tu Raspberry Pi.\")\n",
        "    if GPIO_AVAILABLE:\n",
        "        GPIO.cleanup() # Limpia GPIO antes de salir si estaba disponible\n",
        "    sys.exit(1) # Sal del script si no se pueden cargar las clases.\n",
        "\n",
        "\n",
        "# --- Carga el modelo YOLOv8 (.pt) ---\n",
        "try:\n",
        "    model = YOLO(best_model_path_pt)\n",
        "    print(f\"Modelo YOLOv8 (.pt) cargado exitosamente desde: {best_model_path_pt}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar el modelo .pt: {e}\")\n",
        "    print(f\"Por favor, verifica que la ruta '{best_model_path_pt}' sea correcta y que el archivo best.pt exista en tu Raspberry Pi.\")\n",
        "    print(\"Asegúrate también de haber instalado 'ultralytics' (`pip install ultralytics`) y una versión compatible de PyTorch.\")\n",
        "    if GPIO_AVAILABLE:\n",
        "        GPIO.cleanup() # Limpia GPIO antes de salir si estaba disponible\n",
        "    sys.exit(1) # Sal del script si no se pudo cargar el modelo.\n",
        "\n",
        "# --- Fin de la carga global del modelo .pt ---\n",
        "\n",
        "\n",
        "class CameraWorker(threading.Thread):\n",
        "    def __init__(self, devinfo, name, out_queue, stop_event, save_filename=None, exposure_ms=30, target_fps=30):\n",
        "        super().__init__(daemon=True)\n",
        "        self.devinfo = devinfo\n",
        "        self.name = name\n",
        "        # La cola ahora enviará frames ANOTADOS o frames + detecciones\n",
        "        self.out_q = out_queue\n",
        "        self.stop_event = stop_event\n",
        "        self.save_filename = save_filename\n",
        "        self.exposure_ms = exposure_ms\n",
        "        self.target_fps = float(target_fps) if target_fps > 0 else 30.0\n",
        "        self.frame_interval = 1.0 / self.target_fps\n",
        "\n",
        "        self.hCamera = None\n",
        "        self.pFrameBuffer = None\n",
        "        self.cap = None\n",
        "        self.mono = False\n",
        "        self.writer = None\n",
        "        self.width = None\n",
        "        self.height = None\n",
        "\n",
        "        # Referencia global al modelo YOLOv8 .pt\n",
        "        self.model = model\n",
        "\n",
        "        # Configuraciones de detección globales\n",
        "        self.confidence_threshold = confidence_threshold\n",
        "        self.valid_ball_class_ids = valid_ball_class_ids\n",
        "        self.linea_meta_class_id = linea_meta_class_id\n",
        "        self.travesano_class_id = travesano_class_id\n",
        "        self.class_names = class_names # Para visualización\n",
        "\n",
        "        # Configuraciones de ROI\n",
        "        self.goal_roi_coords = goal_roi_coords\n",
        "\n",
        "\n",
        "    def open_camera(self):\n",
        "        self.hCamera = mvsdk.CameraInit(self.devinfo, -1, -1)\n",
        "        self.cap = mvsdk.CameraGetCapability(self.hCamera)\n",
        "        self.mono = (self.cap.sIspCapacity.bMonoSensor != 0)\n",
        "\n",
        "        if self.mono:\n",
        "            mvsdk.CameraSetIspOutFormat(self.hCamera, mvsdk.CAMERA_MEDIA_TYPE_MONO8)\n",
        "        else:\n",
        "            # preferimos BGR8 para compatibilidad directa con OpenCV\n",
        "            mvsdk.CameraSetIspOutFormat(self.hCamera, mvsdk.CAMERA_MEDIA_TYPE_BGR8)\n",
        "\n",
        "        mvsdk.CameraSetTriggerMode(self.hCamera, 0)  # continuous\n",
        "        try:\n",
        "            mvsdk.CameraSetAeState(self.hCamera, 0)\n",
        "            mvsdk.CameraSetExposureTime(self.hCamera, int(self.exposure_ms * 1000))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        mvsdk.CameraPlay(self.hCamera)\n",
        "\n",
        "        FrameBufferSize = (self.cap.sResolutionRange.iWidthMax *\n",
        "                           self.cap.sResolutionRange.iHeightMax *\n",
        "                           (1 if self.mono else 3))\n",
        "        self.pFrameBuffer = mvsdk.CameraAlignMalloc(FrameBufferSize, 16)\n",
        "\n",
        "    def run(self):\n",
        "        try:\n",
        "            self.open_camera()\n",
        "        except Exception as e:\n",
        "            print(f\"[{self.name}] Error al abrir cámara: {e}\")\n",
        "            # Señaliza al hilo principal que esta cámara falló si es necesario\n",
        "            # self.out_q.put_nowait((self.name, \"ERROR\", time.time()))\n",
        "            return\n",
        "\n",
        "        last_push = 0.0\n",
        "        first_frame = True\n",
        "\n",
        "        while not self.stop_event.is_set():\n",
        "            try:\n",
        "                pRawData, FrameHead = mvsdk.CameraGetImageBuffer(self.hCamera, 300)\n",
        "                mvsdk.CameraImageProcess(self.hCamera, pRawData, self.pFrameBuffer, FrameHead)\n",
        "                mvsdk.CameraReleaseImageBuffer(self.hCamera, pRawData)\n",
        "\n",
        "                frame_data = (mvsdk.c_ubyte * FrameHead.uBytes).from_address(self.pFrameBuffer)\n",
        "                arr = np.frombuffer(frame_data, dtype=np.uint8)\n",
        "\n",
        "                if FrameHead.uiMediaType == mvsdk.CAMERA_MEDIA_TYPE_MONO8:\n",
        "                    frame = arr.reshape((FrameHead.iHeight, FrameHead.iWidth))\n",
        "                    # convert to 3-channel for display/writer later\n",
        "                    frame_color = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)\n",
        "                else:\n",
        "                    frame_color = arr.reshape((FrameHead.iHeight, FrameHead.iWidth, 3))\n",
        "                    # frame_color = frame  # already BGR\n",
        "\n",
        "                now = time.time()\n",
        "                if first_frame:\n",
        "                    self.height, self.width = frame_color.shape[:2]\n",
        "                    if self.save_filename:\n",
        "                        fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # puedes cambiar codec si lo deseas\n",
        "                        # VideoWriter expects color frames; for mono we convert above\n",
        "                        self.writer = cv2.VideoWriter(self.save_filename, fourcc, float(self.target_fps),\n",
        "                                                      (self.width, self.height), True)\n",
        "                    first_frame = False\n",
        "\n",
        "                # --- INFERENCIA YOLOv8 (.pt) EN EL FRAME CAPTURADO ---\n",
        "                # El método predict de Ultralytics se encarga del pre-procesamiento\n",
        "                results = self.model.predict(frame_color, conf=self.confidence_threshold, verbose=False) # Ejecuta la inferencia\n",
        "\n",
        "                # --- PROCESAMIENTO DE LOS RESULTADOS DE DETECCIÓN ---\n",
        "                current_frame_detections = [] # Lista para guardar detecciones procesadas de este frame\n",
        "\n",
        "                frame_height, frame_width, _ = frame_color.shape # Usar dimensiones del frame original\n",
        "\n",
        "                if results and results[0].boxes: # Asegura que hay resultados y detecciones\n",
        "                    detections = results[0].boxes # Accede al objeto Boxes de la primera imagen en el batch (solo 1)\n",
        "\n",
        "                    for det in detections:\n",
        "                        # Obtiene bounding box en formato [x1, y1, x2, y2] en píxeles\n",
        "                        bbox_pixel = det.xyxy[0].tolist() # Convertir tensor a lista\n",
        "\n",
        "                        # Obtiene confianza y clase\n",
        "                        score = det.conf[0].item() # Convertir tensor a escalar float\n",
        "                        class_id = int(det.cls[0].item()) # Convertir tensor a escalar int\n",
        "\n",
        "                        # El umbral de confianza ya se aplicó en model.predict, pero verificamos de nuevo por si acaso\n",
        "                        if score >= self.confidence_threshold:\n",
        "\n",
        "                            # Verifica si el centro de la bounding box está dentro de la ROI del arco\n",
        "                            bbox_center_x = (bbox_pixel[0] + bbox_pixel[2]) / 2\n",
        "                            bbox_center_y = (bbox_pixel[1] + bbox_pixel[3]) / 2\n",
        "\n",
        "                            is_inside_roi = (bbox_center_x >= self.goal_roi_coords[0] and\n",
        "                                             bbox_center_x <= self.goal_roi_coords[2] and\n",
        "                                             bbox_center_y >= self.goal_roi_coords[1] and\n",
        "                                             bbox_center_y <= self.goal_roi_coords[3])\n",
        "\n",
        "                            # Almacena la detección si está dentro de la ROI y es una clase relevante\n",
        "                            if is_inside_roi:\n",
        "                                # Verifica si la clase detectada es una de las que nos interesan\n",
        "                                if (class_id in self.valid_ball_class_ids or\n",
        "                                    class_id == self.linea_meta_class_id or\n",
        "                                    class_id == self.travesano_class_id):\n",
        "\n",
        "                                    current_frame_detections.append({\n",
        "                                        'bbox': bbox_pixel,\n",
        "                                        'conf': float(score), # Convertir a float estándar\n",
        "                                        'class_id': int(class_id), # Convertir a int estándar\n",
        "                                        'class_name': self.class_names[class_id] if class_id < len(self.class_names) else 'unknown'\n",
        "                                    })\n",
        "\n",
        "                # --- FIN DEL PROCESAMIENTO DE DETECCIÓN ---\n",
        "\n",
        "\n",
        "                # Throttling: push / write only si toca según target_fps\n",
        "                if (now - last_push) >= self.frame_interval:\n",
        "                    last_push = now\n",
        "                    # Enviar el frame original Y las detecciones a la cola principal\n",
        "                    # La cola principal (en main) se encargará de la lógica de gol y visualización\n",
        "                    try:\n",
        "                        if self.out_q.qsize() > 2: # Mantener cola pequeña\n",
        "                            try:\n",
        "                                _ = self.out_q.get_nowait()\n",
        "                            except queue.Empty:\n",
        "                                pass\n",
        "                        # Enviamos una tupla: (nombre_camara, frame_color, lista_de_detecciones_procesadas)\n",
        "                        self.out_q.put_nowait((self.name, frame_color, current_frame_detecciones))\n",
        "                    except queue.Full:\n",
        "                        pass # Cola llena, descarta el frame/detecciones\n",
        "\n",
        "                    # Opcional: Escribir frame original o frame anotado al archivo de video\n",
        "                    # if self.writer is not None:\n",
        "                    #     # Puedes dibujar las detecciones en el frame antes de escribirlo si quieres\n",
        "                    #     annotated_frame = frame_color.copy()\n",
        "                    #     # ... dibujar detecciones en annotated_frame ...\n",
        "                    #     self.writer.write(annotated_frame)\n",
        "                    if self.writer is not None:\n",
        "                         self.writer.write(frame_color) # Escribe el frame original sin anotaciones\n",
        "\n",
        "\n",
        "                # pequeña pausa para ceder CPU (ajustable)\n",
        "                time.sleep(0.001)\n",
        "\n",
        "            except mvsdk.CameraException as e:\n",
        "                # ignora timeouts normales\n",
        "                if e.error_code != mvsdk.CAMERA_STATUS_TIME_OUT:\n",
        "                    print(f\"[{self.name}] CameraGetImageBuffer failed({e.error_code}): {e.message}\")\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                print(f\"[{self.name}] Error inesperado en captura o inferencia: {e}\")\n",
        "                # Considerar cómo manejar errores graves en un hilo.\n",
        "                # Podrías señalar al hilo principal que este worker ha fallado.\n",
        "                # self.out_q.put_nowait((self.name, \"FATAL_ERROR\", time.time()))\n",
        "                break\n",
        "\n",
        "        # Cleanup\n",
        "        if self.writer:\n",
        "            self.writer.release()\n",
        "        try:\n",
        "            mvsdk.CameraUnInit(self.hCamera)\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            mvsdk.CameraAlignFree(self.pFrameBuffer)\n",
        "        except Exception:\n",
        "            pass\n",
        "        print(f\"[{self.name}] terminado\")\n",
        "\n",
        "# --- Configuración de RPi.GPIO para la acción de gol (Ej: LED) ---\n",
        "# Se configura una vez al inicio del script principal\n",
        "if GPIO_AVAILABLE:\n",
        "    try:\n",
        "        # El warning \"This channel is already in use\" puede aparecer si el script se ejecutó antes y no se limpió GPIO.\n",
        "        # Puedes añadir GPIO.setwarnings(False) ANTES de setmode para suprimirlo si estás seguro de que no es un problema real.\n",
        "        # GPIO.setwarnings(False) # Descomenta esta línea para suprimir el warning\n",
        "        GPIO.setmode(GPIO.BCM) # Usa numeración BCM\n",
        "        GPIO.setup(led_pin, GPIO.OUT) # Configura el pin como salida\n",
        "        GPIO.output(led_pin, GPIO.LOW) # Asegúrate de que la acción esté inactiva al inicio (ej: LED apagado)\n",
        "        print(f\"RPi.GPIO configurado para el pin {led_pin}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error configurando RPi.GPIO: {e}\")\n",
        "        print(\"Asegúrate de estar ejecutando en una Raspberry Pi, tener la librería rpi.gpio instalada y los permisos adecuados (puede requerir 'sudo').\")\n",
        "        # Si GPIO falla aquí, el script podría continuar sin la acción de gol, o podrías decidir salir.\n",
        "        GPIO_AVAILABLE = False # Desactiva el flag si falla la configuración inicial\n",
        "\n",
        "\n",
        "# Variable para seguir el estado del gol (para activar la acción solo una vez por evento)\n",
        "goal_detected_state = False\n",
        "\n",
        "# --- Función para la acción de gol ---\n",
        "def activate_goal_action():\n",
        "    \"\"\"\n",
        "    Función que realiza la acción cuando se detecta un gol.\n",
        "    En este ejemplo, enciende un LED por un corto tiempo usando RPi.GPIO.\n",
        "    !!! REEMPLAZA ESTO CON TU ACCIÓN ESPECÍFICA DE ROBOT !!!\n",
        "    \"\"\"\n",
        "    if GPIO_AVAILABLE:\n",
        "        print(\"¡¡¡ GOL DETECTADO !!! Activando acción...\")\n",
        "        try:\n",
        "            # --- Ejemplo: Controlar un LED ---\n",
        "            GPIO.output(led_pin, GPIO.HIGH) # Enciende el LED\n",
        "            time.sleep(1) # Mantén encendido por 1 segundo (ajusta el tiempo)\n",
        "            GPIO.output(led_pin, GPIO.LOW) # Apaga el LED\n",
        "            print(\"Acción de gol completada (LED apagado).\")\n",
        "\n",
        "            # --- Otros ejemplos de acciones: ---\n",
        "            # - Enviar un comando a un motor\n",
        "            # - Reproducir un sonido\n",
        "            # - Enviar un mensaje por red\n",
        "            # etc.\n",
        "            # Elimina o comenta el código del LED y añade tu propia lógica aquí.\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error durante la ejecución de la acción de gol en GPIO: {e}\")\n",
        "            # Maneja errores si tu acción de hardware falla.\n",
        "    else:\n",
        "        print(\"¡¡¡ GOL DETECTADO !!! (GPIO no disponible o no configurado)\")\n",
        "        # Acción alternativa si GPIO no está disponible (ej: solo imprimir)\n",
        "\n",
        "\n",
        "# --- INICIALIZACIÓN DE CÁMARAS Y OTROS COMPONENTES ---\n",
        "\n",
        "def main():\n",
        "    global goal_detected_state # Acceder a la variable global\n",
        "\n",
        "    devs = mvsdk.CameraEnumerateDevice()\n",
        "    if not devs:\n",
        "        print(\"No se encontraron cámaras (mvsdk.CameraEnumerateDevice()).\")\n",
        "        # Limpieza de GPIO si no se encontraron cámaras y salió antes de finally\n",
        "        if GPIO_AVAILABLE:\n",
        "             try:\n",
        "                 GPIO.cleanup()\n",
        "                 print(\"Limpieza de GPIO realizada.\")\n",
        "             except Exception as e:\n",
        "                 print(f\"Error durante la limpieza de GPIO: {e}\")\n",
        "        return\n",
        "\n",
        "    # decide cuántas cámaras abrir (2 por defecto; si hay 1, abrirá solo 1)\n",
        "    want = 2\n",
        "    devices = devs[:want]\n",
        "    print(\"Dispositivos encontrados:\", [d.GetFriendlyName() for d in devices])\n",
        "\n",
        "    if len(devices) == 0:\n",
        "        print(\"No se encontraron cámaras para abrir.\")\n",
        "        # Limpieza de GPIO si no se encontraron cámaras para abrir\n",
        "        if GPIO_AVAILABLE:\n",
        "             try:\n",
        "                 GPIO.cleanup()\n",
        "                 print(\"Limpieza de GPIO realizada.\")\n",
        "             except Exception as e:\n",
        "                 print(f\"Error durante la limpieza de GPIO: {e}\")\n",
        "        return\n",
        "\n",
        "\n",
        "    stop_event = threading.Event()\n",
        "    queues = []\n",
        "    workers = []\n",
        "    # Diccionario para almacenar las últimas detecciones de cada cámara\n",
        "    last_detections = {f\"cam{i}\": [] for i in range(len(devices))}\n",
        "    # Diccionario para almacenar los últimos frames de cada cámara (opcional para visualización)\n",
        "    last_frames = {f\"cam{i}\": None for i in range(len(devices))}\n",
        "\n",
        "\n",
        "    for i, dev in enumerate(devices):\n",
        "        # La cola ahora recibe (nombre_camara, frame, lista_detecciones)\n",
        "        q = queue.Queue(maxsize=4)\n",
        "        name = f\"cam{i}\"\n",
        "        # Puedes deshabilitar la grabación si no la necesitas para ahorrar recursos\n",
        "        # out_file = f\"cam{i}_out.mp4\"\n",
        "        out_file = None # No grabar por defecto para ahorrar CPU/disco\n",
        "        w = CameraWorker(dev, name, q, stop_event, save_filename=out_file, exposure_ms=30, target_fps=30)\n",
        "        w.start()\n",
        "        queues.append(q)\n",
        "        workers.append(w)\n",
        "        time.sleep(0.2)  # pequeño delay al iniciar cámaras\n",
        "\n",
        "    # Ventanas para mostrar cada cámara (opcional, requiere pantalla)\n",
        "    window_names = [f\"Camera {i}\" for i in range(len(workers))]\n",
        "    display_windows = False # <--- CAMBIA A True si quieres mostrar las ventanas en una pantalla conectada\n",
        "    if display_windows:\n",
        "        for wn in window_names:\n",
        "            cv2.namedWindow(wn, cv2.WINDOW_AUTOSIZE)\n",
        "\n",
        "\n",
        "    # --- BUCLE PRINCIPAL PARA PROCESAR DETECCIONES DE AMBAS CÁMARAS ---\n",
        "    print(\"Iniciando bucle principal de procesamiento de detecciones...\")\n",
        "    try:\n",
        "        print(\"Presiona 'q' o cierra una ventana (si se muestra) para salir.\")\n",
        "        while True:\n",
        "            # Recolecta frames y detecciones de las colas de los workers\n",
        "            # Usamos un timeout pequeño para no bloquear indefinidamente si una cola está vacía\n",
        "            processed_frames_count = 0\n",
        "            for q in queues:\n",
        "                try:\n",
        "                    # Esperamos recibir (nombre_camara, frame, lista_detecciones)\n",
        "                    # Usamos un timeout mayor para dar tiempo a los workers a procesar\n",
        "                    name, frame, detections = q.get(timeout=0.1) # Timeout 0.1 segundos\n",
        "                    last_detections[name] = detections # Almacena las detecciones de esta cámara\n",
        "                    last_frames[name] = frame # Almacena el frame (opcional para visualización)\n",
        "                    processed_frames_count += 1\n",
        "                    # print(f\"Recibidas {len(detections)} detecciones de {name}\") # Depuración\n",
        "\n",
        "                except queue.Empty:\n",
        "                    pass # Si la cola está vacía, simplemente continuamos\n",
        "\n",
        "\n",
        "            # --- Lógica de Detección de \"Gol\" con Detecciones Combinadas ---\n",
        "            # Ejecutamos la lógica de gol si hemos recibido frames/detecciones de AL MENOS UNA cámara\n",
        "            # en esta iteración del bucle principal. Esto evita que el bucle principal se bloquee\n",
        "            # esperando frames de una cámara que podría haber fallado.\n",
        "            # Sin embargo, la lógica interna de gol aún requerirá detecciones de las cámaras relevantes.\n",
        "            # Un enfoque más robusto podría verificar si las últimas detecciones recibidas son recientes.\n",
        "\n",
        "            # Combinar detecciones de todas las cámaras que han reportado en esta iteración o tienen últimas detecciones almacenadas\n",
        "            all_ball_detections = []\n",
        "            all_linea_meta_detecciones = []\n",
        "            all_travesano_detections = []\n",
        "\n",
        "            # Itera sobre las últimas detecciones que tenemos de CADA cámara\n",
        "            for cam_name in last_detections:\n",
        "                 for det in last_detections[cam_name]:\n",
        "                      # Verifica si la detección está dentro de la ROI.\n",
        "                      # Aunque ya filtramos por ROI en el worker, esta verificación adicional\n",
        "                      # puede ser útil si la lógica de ROI se modifica o para mayor seguridad.\n",
        "                      # Para ser coherentes con el filtrado del worker, podemos omitir la verificación de ROI aquí\n",
        "                      # y confiar en que las detecciones en last_detections ya pasaron el filtro de ROI.\n",
        "                      # O, si la ROI es diferente por cámara, la lógica aquí sería más compleja.\n",
        "                      # Asumiendo una ROI GLOBAL aplicada en el worker:\n",
        "                      if det['class_id'] in valid_ball_class_ids:\n",
        "                            all_ball_detections.append(det)\n",
        "                      elif det['class_id'] == linea_meta_class_id:\n",
        "                            all_linea_meta_detecciones.append(det)\n",
        "                      elif det['class_id'] == travesano_class_id:\n",
        "                            all_travesano_detections.append(det)\n",
        "\n",
        "\n",
        "            # Procedemos con la lógica de gol solo si hay al menos una detección de cada objeto relevante\n",
        "            # (balón, línea de meta, travesaño) en el conjunto combinado de detecciones de todas las cámaras.\n",
        "            current_frame_goal_detected = False # Flag para el frame actual\n",
        "\n",
        "            if all_ball_detections and all_linea_meta_detecciones and all_travesano_detections:\n",
        "                 # Para simplicidad, tomamos la primera detección encontrada de cada tipo en la lista combinada.\n",
        "                 # En un sistema más robusto, podrías querer usar la detección con mayor confianza,\n",
        "                 # o considerar la triangulación si las cámaras están calibradas y la geometría lo permite.\n",
        "                 ball_det = all_ball_detections[0]\n",
        "                 linea_meta_det = all_linea_meta_detecciones[0]\n",
        "                 travesano_det = all_travesano_detections[0]\n",
        "\n",
        "                 ball_bbox = ball_det['bbox']\n",
        "                 linea_meta_bbox = linea_meta_det['bbox']\n",
        "                 travesano_bbox = travesano_det['bbox']\n",
        "\n",
        "                 # Calcula puntos/líneas relevantes para la comparación\n",
        "                 # Usaremos la coordenada Y del borde inferior del balón para cruces horizontales.\n",
        "                 ball_bottom_y = ball_bbox[3]\n",
        "                 # Usaremos la coordenada X del centro del balón para cruces verticales.\n",
        "                 ball_center_x = (ball_bbox[0] + ball_bbox[2]) / 2\n",
        "\n",
        "                 # Calcula la coordenada relevante de la línea de meta\n",
        "                 if goal_line_orientation == 'horizontal':\n",
        "                      # Usamos la coordenada Y del centro de la bounding box de la línea de meta como referencia.\n",
        "                      line_y_pos = (linea_meta_bbox[1] + linea_meta_bbox[3]) / 2\n",
        "                 elif goal_line_orientation == 'vertical':\n",
        "                      # Usamos la coordenada X del centro de la bounding box de la línea de meta como referencia.\n",
        "                      line_x_pos = (linea_meta_bbox[0] + linea_meta_bbox[2]) / 2\n",
        "                 else:\n",
        "                      # Si la orientación no es válida, no podemos determinar el cruce\n",
        "                      # Esto debería ser manejado al inicio o con una advertencia clara\n",
        "                      pass # Mantener current_frame_goal_detected en False\n",
        "\n",
        "\n",
        "                 # Calcula la coordenada Y del borde inferior del travesaño\n",
        "                 travesano_bottom_y = travesano_bbox[3]\n",
        "\n",
        "\n",
        "                 # --- Verifica las condiciones para el gol ---\n",
        "\n",
        "                 # 1. ¿Ha cruzado la pelota la línea de meta en la dirección definida?\n",
        "                 has_crossed_line = False # Reset for current check\n",
        "                 if goal_line_orientation == 'horizontal':\n",
        "                      if crossing_direction == 'below' and ball_bottom_y > line_y_pos:\n",
        "                           has_crossed_line = True\n",
        "                      elif crossing_direction == 'above' and ball_bbox[1] < line_y_pos: # Usamos el borde superior del balón para cruce 'above'\n",
        "                           has_crossed_line = True\n",
        "                 elif goal_line_orientation == 'vertical':\n",
        "                      if crossing_direction == 'right' and ball_center_x > line_x_pos:\n",
        "                           has_crossed_line = True\n",
        "                      elif crossing_direction == 'left' and ball_center_x < line_x_pos:\n",
        "                           has_crossed_line = True\n",
        "                 # else: has_crossed_line remains False if orientation/direction invalid\n",
        "\n",
        "\n",
        "                 # 2. ¿Está la pelota por debajo del travesaño?\n",
        "                 is_below_crossbar = (ball_bottom_y < travesano_bottom_y) # Usamos el borde inferior del balón vs el borde inferior del travesaño\n",
        "\n",
        "\n",
        "                 # --- Condición Final de Gol ---\n",
        "                 # Se considera gol si se cumplen:\n",
        "                 # - Hay al menos una detección de balón, línea de meta y travesaño (ya verificado por las listas no vacías)\n",
        "                 # - El balón ha cruzado la línea de meta en la dirección correcta.\n",
        "                 # - El balón está por debajo del travesaño.\n",
        "                 current_frame_goal_detected = has_crossed_line and is_below_crossbar\n",
        "\n",
        "                 # --- Opcional: Imprimir estado de las condiciones para depuración ---\n",
        "                 # print(f\"Lógica Gol - Cruzó Línea: {has_crossed_line}, Debajo Travesaño: {is_below_crossbar}, Gol Frame Actual: {current_frame_goal_detected}\")\n",
        "\n",
        "\n",
        "            # --- Activar Acción de Gol ---\n",
        "            # Activamos la acción si se detecta gol en el frame actual Y NO se detectó gol en el estado anterior.\n",
        "            # Esto es una detección de \"borde ascendente\" para evitar múltiples activaciones continuas.\n",
        "            if current_frame_goal_detected and not goal_detected_state:\n",
        "                 activate_goal_action() # Llama a la función que realiza la acción (ej: enciende LED)\n",
        "                 goal_detected_state = True # Actualiza el estado a 'gol detectado'\n",
        "            elif not current_frame_goal_detected and goal_detected_state:\n",
        "                 # Si antes se detectó gol, pero ahora no, resetea el estado.\n",
        "                 goal_detected_state = False\n",
        "\n",
        "\n",
        "            # --- Opcional: Visualización del frame con detecciones (si display_windows es True) ---\n",
        "            # La visualización se hace en el hilo principal con los últimos frames y detecciones recibidas.\n",
        "            if display_windows:\n",
        "                for cam_name, frame in last_frames.items():\n",
        "                     if frame is not None:\n",
        "                          # Dibuja las detecciones en el frame antes de mostrarlo\n",
        "                          annotated_frame = frame.copy()\n",
        "                          detections_for_cam = last_detections.get(cam_name, [])\n",
        "\n",
        "                          # Dibuja la ROI del arco en cada frame\n",
        "                          cv2.rectangle(annotated_frame, (goal_roi_coords[0], goal_roi_coords[1]), (goal_roi_coords[2], goal_roi_coords[3]), (255, 0, 0), 2) # Azul, grosor 2\n",
        "\n",
        "                          for det in detections_for_cam:\n",
        "                               bbox_pixel = det['bbox']\n",
        "                               class_id = det['class_id']\n",
        "                               score = det['conf']\n",
        "                               class_name = det['class_name']\n",
        "\n",
        "                               # Dibuja el rectángulo de la bounding box\n",
        "                               color = (0, 255, 0) # Verde por defecto\n",
        "                               if class_id in valid_ball_class_ids:\n",
        "                                   color = (0, 0, 255) # Rojo para balones\n",
        "                               elif class_id == linea_meta_class_id:\n",
        "                                   color = (255, 255, 0) # Cyan para línea de meta\n",
        "                               elif class_id == travesano_class_id:\n",
        "                                   color = (255, 0, 255) # Magenta para travesaño\n",
        "\n",
        "                               cv2.rectangle(annotated_frame, (bbox_pixel[0], bbox_pixel[1]), (bbox_pixel[2], bbox_pixel[3]), color, 2)\n",
        "\n",
        "                               # Dibuja la etiqueta (nombre de clase y confianza)\n",
        "                               label = f\"{class_name}: {score:.2f}\"\n",
        "                               cv2.putText(annotated_frame, label, (bbox_pixel[0], bbox_pixel[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "\n",
        "                          # opcional: redimensiona para pantalla\n",
        "                          disp = cv2.resize(annotated_frame, (640, 480), interpolation=cv2.INTER_LINEAR)\n",
        "                          # Encuentra el nombre de la ventana correcto para esta cámara\n",
        "                          window_idx = int(cam_name.replace(\"cam\", \"\"))\n",
        "                          cv2.imshow(window_names[window_idx], disp)\n",
        "\n",
        "\n",
        "                key = cv2.waitKey(1) & 0xFF\n",
        "                if key == ord('q'):\n",
        "                    print(\"Salida por tecla q.\")\n",
        "                    stop_event.set() # Señaliza a los workers que se detengan\n",
        "                    break\n",
        "\n",
        "                # detectar si se cerró alguna ventana con la X\n",
        "                closed = False\n",
        "                for wn in window_names:\n",
        "                    if cv2.getWindowProperty(wn, cv2.WND_PROP_VISIBLE) < 1:\n",
        "                        closed = True\n",
        "                        break\n",
        "                if closed:\n",
        "                    print(\"Ventana cerrada por el usuario — saliendo.\")\n",
        "                    stop_event.set() # Señaliza a los workers que se detengan\n",
        "                    break\n",
        "\n",
        "            # Pequeña pausa para evitar consumir todo el CPU en el bucle principal\n",
        "            time.sleep(0.005) # Ajusta según sea necesario\n",
        "\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        # Maneja la interrupción por teclado (Ctrl+C)\n",
        "        print(\"\\nInterrupción por teclado. Deteniendo el script.\")\n",
        "        stop_event.set() # Señaliza a los workers que se detengan\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nOcurrió un error inesperado durante la ejecución del bucle principal: {e}\")\n",
        "        stop_event.set() # Señaliza a los workers que se detengan\n",
        "\n",
        "\n",
        "    finally:\n",
        "        # --- LIMPIEZA FINAL ---\n",
        "\n",
        "        print(\"Realizando limpieza...\")\n",
        "        # Esperar a que los hilos workers terminen\n",
        "        for w in workers:\n",
        "            w.join(timeout=3) # Espera un máximo de 3 segundos por cada hilo\n",
        "\n",
        "        # Destruye cualquier ventana de OpenCV (si se usó cv2.imshow)\n",
        "        if display_windows:\n",
        "             cv2.destroyAllWindows()\n",
        "             print(\"Ventanas de OpenCV destruidas.\")\n",
        "\n",
        "        # --- Limpieza de GPIO ---\n",
        "        if GPIO_AVAILABLE:\n",
        "            try:\n",
        "                GPIO.cleanup() # Limpia la configuración de los pines GPIO\n",
        "                print(\"Limpieza de GPIO realizada.\")\n",
        "            except Exception as e:\n",
        "                 print(f\"Error durante la limpieza de GPIO: {e}\")\n",
        "\n",
        "        print(\"Script de detección de gol finalizado.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "xeXnm078rjih"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}